[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estadística II: modelos",
    "section": "",
    "text": "Prefacio\nEste “Quarto Book” contiene el material preparado por el profesor para la asignatura Estadística II de \\(3^{er}\\) curso del Grado en Matemáticas de la Universidad de Castilla-La Mancha. Es la tercera asignatura de “Estadística” en el Grado, tras Elementos de Probabilidad y Estadística, y Estadística I, por lo que presupone ciertos conocimientos de probabilidad y estadística, así como de manejo del software R.\nEsta asignatura sirve para introducir al estudiante en el manejo de modelos (estadísticos) de distinto tipo. Concretamente:\nLos distintos modelos citados darían lugar, por sí solos, a asignaturas completas. Por lo que, para encajarlos en una asignatura cuatrimestral, se tomará la libertad de considerar distinto nivel de detalle con cada uno de los modelos/temas. Además se va a buscar un adecuado balance entre la teoría y el uso práctico de estos modelos o técnicas, haciendo uso del software estadístico R.\nEl material está basado en la experiencia del profesor, y en los libros de la bibliografía, entre los que se destacan:\nUn “gran” libro no sólo por su contenido (más de 50 temas), sino por su enfoque teórico/práctico. Cada tema conjuga teoría con un caso práctico ilustrativo, en aproximadamente 20 páginas.\nPor otro lado, en este material se podrían haber incluido otros modelos (por ejemplo, regresión ordinal, modelos GAM, …), técnicas (curvas ROC, árboles, …), enfoques (machine learning, diseño óptimo de experimentos… ) etc. que supondrían una asignatura mucho más completa, pero que sería imposible de abarcar en un cuatrimestre. Cabe señalar que algunos de ellos aparecen en la bibliografía citada anteriormente, pudiendo el estudiante/lector acudir a ella para ampliar sus conocimientos.\nObviamente, el material no está libre de errores o erratas. Por lo que, si se detectan, se agradece que sean comunicadas al profesor.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#agradecimientos",
    "href": "index.html#agradecimientos",
    "title": "Estadística II: modelos",
    "section": "Agradecimientos",
    "text": "Agradecimientos\nAunque el material es fruto de la experiencia del profesor, me gustaría agradecer a mi compañero y amigo Dr. D. Licesio J. Rodríguez-Aragón (UCLM) sus inestimables materiales y comentarios, que han sido de grandísima ayuda para la elaboración de este libro.\nTambién a los compañeros del Área de Estadística del Departamento de Matemáticas de la UCLM, en especial a Dr. D. Mariano Amo Salas, por sus comentarios y sugerencias de todo el material, pero especialmente en la parte del análisis de supervivencia; a Dr. D. Sergio Pozuelo Campos, y Dra. Da. Irene García Camacha por sus comentarios y apoyo en esta docencia en el grado en Matemáticas.\nPor último, agradezco a los autores de los libros y artículos citados, así como a los autores de los paquetes de R utilizados, su trabajo y dedicación. Especialmente a mi compañero de grupo de investigación Dr. D. Juan Manuel Rodríguez Díaz (USAL) por sus materiales sobre modelos lineales y diseño de experimentos, que han sido de gran ayuda para la elaboración de este libro.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#software-r",
    "href": "index.html#software-r",
    "title": "Estadística II: modelos",
    "section": "Software: R",
    "text": "Software: R\nLa parte práctica de la asignatura se realizará con la ayuda del software/lenguaje de programación libre R (https://www.r-project.org/), utilizando RStudio como interfaz.\n\n\n\n\n\n\nEs recomendable tener la última versión tanto de R como de RStudio:\n\nDescarga de R: https://cloud.r-project.org/.\nDescarga de RStudio: https://posit.co/downloads/.\n\n\n\n\nManuales (básicos):\n\nR Para principiantes, E. Paradis.\nIntroducción a R\n\nAlgunos más en https://cran.r-project.org/other-docs.html\nBlog: \n\nR Bloggers",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "Cap1-LM.html",
    "href": "Cap1-LM.html",
    "title": "1  Modelos lineales",
    "section": "",
    "text": "1.1 Modelo estadístico de regresión\nEn este tema se estudiarán los denominados modelos lineales. El caso paradigmático es la regresión lineal simple, caso particular del modelo de regresión lineal múltiple. Los modelos de diseño de experimentos (que estudiaremos en el Capítulo 2) también son modelos lineales. En ambos modelos, la denominada variable respuesta debe ser cuantitativa/numérica continua, a diferencia del modelo lineal generalizado (que estudiaremos en el Capítulo 3).\nUna buena referencia para este tema es el libro “Regresión y diseño de experimentos”, Peña (2002), concretamente la segunda parte, capítulos 5 a 10. Otro libro de referencia con un buen contenido matemático y práctico es “Linear Models with R”, Faraway (2004), capítulos 1 a 7. Otra referencia, con un enfoque más aplicado, es el capítulo 15 “Modelización lineal”, Casero-Alonso y Durbán (2024), https://cdr-book.github.io/cap-lm.html del libro “Fundamentos de Ciencia de Datos con R”, Fernández-Avilés y Montero (2024), que denominaremos en el texto como CDR.\nParafraseando a G.E.P. Box …Todos los modelos son falsos. Pero algunos son útiles. Es imposible describir la realidad de forma exacta mediante un modelo, pero puede ser útil utilizar un modelo aproximado, basado en los “datos”, que permita entender y explicar el fenómeno o experimento de interés. En matemáticas un modelo es una relación matemática, no necesariamente algebraica, que permite entender el fenómeno. En estadística, a los modelos matemáticos se les añade un término de error, para incluir lo que la parte estructual (algebraica) no es capaz de explicar Este término de error se desea aleatorio, estocástico, y tiene el papel de “cajón de sastre”.\nEl modelo lineal de regresión lineal simple permite modelizar (de forma aproximada) el comportamiento de una variable cuantitativa, denominada respuesta o dependiente, denotada por \\(Y\\), mediante una función lineal de otra variable cuantitativa, denominada explicativa o predictora, \\(X\\), que se supone está correlacionada con ella (correlación no implica causalidad). La forma habitual de expresar el modelo es: \\[ Y = \\beta_0 + \\beta_1 X + \\epsilon,\\]\ndonde \\(Y = (y_1, y_2, \\ldots, y_n)^\\top\\) es el vector de las \\(n\\) observaciones de la variable respuesta, \\(X = (x_1, x_2, \\ldots, x_n)^\\top\\) es el vector de las \\(n\\) observaciones de la variable explicativa, \\(\\beta_i \\ (i=0 \\ ó \\ 1)\\) son los coeficientes o parámetros del modelo, y \\(\\epsilon=(\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n)\\) es el vector de errores aleatorio que convierte el modelo determinista (\\(\\beta_0 + \\beta_1 X\\)) en modelo estocástico. Los supuestos más habituales sobre el error son:\nLa generalización a varias variables explicativas es inmediata: \\[ Y = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_k X_k + \\epsilon.\\] A este modelo se le denomina modelo lineal de regresión lineal múltiple.\nUtilizando notación matricial: \\[Y = X \\beta + \\epsilon ;\\]\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1k} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2k} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{nk}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_n \\\\\n\\end{bmatrix}.\\] donde \\(Y\\) vuelve a ser el vector de \\(n\\) respuestas, \\(X\\) es ahora la matriz \\(n\\times(k+1)\\) de variables explicativas, que contiene una columna de unos para incluir en el modelo el parámetro \\(\\beta_0\\) que no depende de las variables explicativas, \\(\\beta\\) el vector de \\(k+1\\) parámetros del modelo y \\(\\epsilon\\) vuelve a ser el vector de los términos de error aleatorios, \\(\\epsilon \\sim N(0, \\sigma^2I)\\).\nPregunta\nEn el modelo de regresión ¿tienen que ser todas las variables cuantitativas?\nRespuesta corta: No.\nDependiendo del tipo de variables (\\(X\\) e \\(Y\\)) se tendrán distintos modelos…\n¿Si hay más de una variable \\(Y\\)? Se habla de regresión múltiple multivariante, que se podrá ver en otra asignatura del grado.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#modelo-estadístico-de-regresión",
    "href": "Cap1-LM.html#modelo-estadístico-de-regresión",
    "title": "1  Modelos lineales",
    "section": "",
    "text": "tener media cero,\nvarianza constante y\nseguir una distribución normal.\n\n\n\nEn este contexto, cuando se habla de modelo lineal hay que distinguir entre modelo lineal en las variables y modelo lineal en los parámetros. Así:\n- \\(Y=\\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\) es un modelo lineal en las variables y en los parámetros.\n- \\(Y=\\beta_1 X_1 + \\beta_2 X_1^2  + \\epsilon\\) es un modelo lineal en los parámetros, pero no en la variable.\n- \\(Y = \\beta_1X_1^{\\beta_2}  + \\epsilon\\) es un modelo no lineal tanto en los parámetros como en la variable.\n\n\n\n\nLos modelos que se puedan expresar en forma matricial son lineales.\n\n\n\n\n1.1.1 Objetivos de la regresión\n\nDescribir la estructura general entre la(s) variable(s) explicativa(s) y la respuesta, estimando y evaluando su efecto.\nEste proceso suele ser iterativo, hasta seleccionar la(s) variable(s) del mejor modelo posible.\nPredecir observaciones futuras.\n\nAmbos objetivos pueden ser muy distintos!!! Bajo el prisma del “machine learning” suele relajarse la descripción estructural y evaluación de los efectos dando toda la importancia a la mejor predicción posible.\n\n\n1.1.2 Supuestos del modelo de regresión\nLos supuestos en los que se basa el modelo de regresión expuesto son:\n\nNormalidad: no solo la variable \\(Y\\) debe seguir una distribución normal, sino los valores de \\(Y\\) para cada valor de \\(X_i\\) (distribución condicionada) también deben seguir una distribución normal.\nLinealidad: relación lineal entre cada una de las variables explicativas y la respuesta.\nHomocedasticidad (homogeneidad de varianzas): la varianza de la variable \\(Y\\) para cada valor de \\(X_i\\) debe ser homogénea.\nIndependencia: cada observación de la variable \\(Y\\) debe ser independiente de las demás.\n\nLos supuestos anteriores se basan en las hipótesis de los errores:\n\nesperanza nula: \\(E[\\epsilon]=0\\),\nvarianza constante: \\(Var[\\epsilon]=\\sigma^2I\\),\ndistribución normal \\(\\epsilon \\sim N(0, \\sigma^2 I)\\)\nindependencia entre errores: \\(E[\\epsilon_i\\epsilon_j]=0\\)\n\nSe puede encontrar mas información sobre las también denominadas hipótesis básicas en Peña (2002) (capítulo 5, apartados 5.2.1 Hipótesis básicas y 5.2.2 Comentarios a las hipótesis) y en Faraway (2004)…\nResiduos\nLos residuos, \\(u_i\\), son los errores observados para los datos y el modelo escogido (realizaciones de la variable aleatoria error \\(\\epsilon\\)). Recogen toda la información que la estructura del modelo no ha sido capaz de asimilar. Es muy importante el estudio de los residuos para comprobar las hipótesis y supuestos anteriores y dar con ello validez al uso del modelo, como veremos en la Sección 1.2.12 a nivel teórico y en los casos prácticos (?sec-pract).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#lm",
    "href": "Cap1-LM.html#lm",
    "title": "1  Modelos lineales",
    "section": "1.2 Modelo lineal de regresión simple",
    "text": "1.2 Modelo lineal de regresión simple\nEs habitual comenzar el estudio de los modelos lineales con el caso más sencillo de regresión lineal simple para profundizar en los detalles de cada uno de los pasos del proceso de modelización, estimación, validación y predicción. Posteriormente se señalarán los aspectos que cambian al pasar a la regresión lineal múltiple, que es el caso más habitual en la práctica.\nPartiendo de los datos muestrales recogidos sobre el fenómeno de interés. ¿Cómo obtener el modelo de regresión lineal simple?\nREVISAR EL ORDEN\n\nEstructura: se deben identificar la variable respuesta (la que se pretende explicar) y la variable explicativa. Esta identificación define una forma estructural, que en este caso sencillo podría tener 2 formas:\n\n\n\\(E(Y)=\\beta_0 + \\beta_1 X\\), la habitual (incluso diría “por defecto”) o\n\\(E(Y)= \\beta_1 X\\) (proporcionalidad directa, sin término independiente).\n\n\nEstimación: con los valores muestrales recogidos \\((x_i, y_i)\\) se obtendrán estimaciones \\(\\hat{\\beta}_i\\) de los parámetros \\(\\beta_i\\) del modelo preestablecido.\nValidación: mediante el análisis de residuos se comprobarán las hipótesis que validan el uso del modelo.\nInterpretación/Inferencia: validado el modelo se interpretarán los coeficientes, valorando previamente su significación y su utilidad práctica. También se dará cuenta de la bondad del modelo para explicar la variable respuesta, mediante algún indicador.\nLlegados a este punto puede verse conveniente cambiar el modelo (en este caso considerar otra variable explicativa, o aplicar alguna transformación) para intentar obtener un modelo que explique mejor la respuesta, con mayor bondad, por lo que se volvería al paso 1.\nPredicción: se obtendrán las predicciones necesarias u oportunas a partir del modelo considerado, teniendo cuidado con la extrapolación (y en algunos casos, también con la interpolación).\n\n\n1.2.1 Estimación de los parámetros del modelo\nEn el caso más habitual de regresión simple: \\[Y = \\beta_0 + \\beta_1X + \\epsilon,\\] se estimarán los dos parámetros del modelo: \\(\\beta_0\\), la ordenada en el origen y \\(\\beta_1\\), la pendiente de la recta. Intuitivamente, se buscan las estimaciones \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que “expliquen” de la mejor manera posible la relación entre las dos variables, aquellas que generen las mejores predicciones posibles: \\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X\\]\n\n\n1.2.2 Método de mínimos cuadrados\nEl método por excelencia para obtener tales estimaciones es el método de mínimos cuadrados (1805 Gauss y Legendre). Consiste en minimizar los cuadrados de los denominados residuos, diferencia entre los valores observados y la predicción: \\(u_i = y_i - \\hat{y}_i = y_i - \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\).\nEs decir, \\[\\min_{\\hat{\\beta}_0, \\hat{\\beta}_1} \\sum u_i^2 =\n\\min_{\\hat{\\beta}_0, \\hat{\\beta}_1} \\sum(y_i-\\hat{y_i})^2 =\n\\min_{\\hat{\\beta}_0, \\hat{\\beta}_1} \\sum(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2.\\]\nDerivando, e igualando a cero se tiene un sistema lineal de 2 ecuaciones con 2 incógnitas, denominado ecuaciones normales: \\[\\left.\\begin{align*}\n\\dfrac{\\partial \\sum u_i^2}{\\partial \\beta_0} &= 0 \\Rightarrow \\sum_{i} u_i = 0 \\\\\n\\dfrac{\\partial \\sum u_i^2}{\\partial \\beta_1} &= 0 \\Rightarrow \\sum_{i} u_i x_i = 0\n\\end{align*}\n\\right\\rbrace\\]\ncuya solución es:\n\\[\\begin{align*}\n\\hat{\\beta}_1^{MC} &= \\rho \\frac{S_Y}{S_X}, \\\\\n\\hat{\\beta}_0^{MC} &= \\bar{y} - \\hat{\\beta}_1^{MC} \\bar{x},\n\\end{align*} \\] donde \\(\\rho\\) es el coeficiente de correlación lineal de Pearson y \\(S_Y\\) y \\(S_X\\) son las desviaciones típicas muestrales de las variables \\(Y\\) y \\(X\\).\nCorrelación\nEn la regresión lineal simple se espera que ambas variables estén correlacionadas, así, el modelo tendrá sentido práctico. El coeficiente de correlación lineal de Pearson mide la fuerza de la relación lineal entre las dos variables. Se puede expresar en función de las medias y las desviaciones típicas de las variables:\n\\[\\rho=\\frac{1}{n-1}\\sum(\\frac{x_i-\\bar{x}}{s_x})(\\frac{y_i-\\bar{y}}{s_y})=\\frac{1}{n-1}\\sum z_x \\cdot z_y=\\frac{s_{xy}}{s_x s_y}=\\frac{SP_{xy}}{\\sqrt{SC_x\\cdot SC_y}}\\]\nLos valores de este coeficiente se extienden de \\(-1\\) a \\(1\\), indicando ausencia de correlación cuanto más cercano a \\(0\\).\n\nHay que destacar el ejemplo ilustrativo del cuarteto de Anscombe, en el que 4 conjuntos de datos presentan el mismo valor del coeficiente de correlación lineal, pero su interpretación es muy distinta en cada uno de los 4 casos.\n\n\n\n1.2.3 Interpretación geométrica\nLos estimadores de mínimos cuadrados (MC) tienen una interpretación geométrica sencilla y gráficamente elocuente. Son aquellos que hacen que la recta de regresión simple minimice las distancias verticales (residuos) de toda la nube de puntos (Véase la Figura 5.7 de Peña (2002), apartado 5.4.4).\nAhora bien, también se puede representar el problema de MC en el espacio de las \\(n\\) observaciones (en lugar del espacio de 2 variables), lo que proporciona una interpretación geométrica interesante (véase Faraway (2004) apartado 2.3). La proyección ortogonal del vector \\(\\mathbf{Y}^\\top=(y_1, y_2, \\ldots, y_n)\\) sobre el plano definido por los vectores \\(\\mathbf{1}\\) y \\(\\mathbf{X}\\) genera el vector de residuos \\(\\mathbf{u}\\) perpendicular a ellos y a todos los vectores del plano. Además, aplicando el teorema de Pitágoras al triangulo rectángulo resultante de la proyección (Figuras 5.11 de Faraway (2004) y … de Peña (2002)) proporciona una forma alternativa de obtener la fórmula de la suma de cuadrados que veremos más adelante.\n\n\n1.2.4 Método de máxima verosimilitud\nTambién se pueden estimar los parámetros del modelo mediante el método de máxima verosimilitud (MV), que consiste en maximizar la función de verosimilitud \\(L(\\beta)\\). Suponiendo normalidad: \\[L(\\beta) = \\log (2\\pi)^{-n/2} \\sigma^{-n} \\exp\\left[-\\frac{\\sum_{i=1}^{n} u_i^2}{2\\sigma^2}\\right]\\]\nBajo dicho supuesto de normalidad, los estimadores MC coinciden con los MV. Por lo que los estimadores que minimizan la suma de cuadrados de los residuos, también maximizan la probabilidad de los datos observados.\nEn Peña (2002), apartado 5.4.1, se puede ver el detalle de la obtención de la estimación de los parámetros del modelo de regresión simple utilizando el método de máxima verosimilitud.\n\n\n1.2.5 Estimación de la varianza del modelo\n(5.4.3 Peña: varianza residual)\nEn 5.4.1 se obtiene también el estimador por MV de \\(\\sigma^2\\) que resulta ser la varianza de los residuos: \\[\\hat{\\sigma}^2=\\dfrac{\\sum e_i^2 }{n}.\\] En 5.4.3 se habla de la varianza residual, argumentando que los \\(n\\) residuos no son independientes, dadas las dos ecuaciones de restricción entre los residuos. Así los grados de libertad en este caso son \\(n-2\\) y por ello se define la varianza residual como \\[\\hat{S}_R^2=\\dfrac{\\sum e_i^2 }{n-2},\\] que …(propiedades en 5.5.4)\n\n\n1.2.6 Propiedades de los estimadores\n(5.5 de Peña, véase Tabla 5.3 página 264, aunque no aclara mucho.\nLa que sí que parece interesante la figura 5.15 que tiene 3 muestras al azar de la población y sus estadísticos asociados)\nPropiedades sobre \\(\\beta_0\\), \\(\\beta_1\\) y \\(\\sigma^2\\).\n(JUAN) 1. La ecuación de la recta de regresión también se suele expresar en la forma punto-pendiente: \\[(y - \\bar{y}) = \\hat{\\beta}_1 (x - \\bar{x}) \\] 1. Del punto anterior se deduce inmediatamente que el punto \\((\\bar{x}, \\bar{y})\\) pertenece a la recta estimada. 1. Los puntos con valores de \\(x\\) más alejados a la media tienen más influencia sobre la estimación de \\(\\beta_1\\), más cuanto menos puntos sean. 1. $_1 (_1, ) $ 1. $_0 (_0, (1 + ) ) $ 1. \\(Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = -\\dfrac{\\bar{x}\\sigma^2}{nS_x^2}\\)\nPor lo tanto, cuando \\(\\bar{x}&gt;0\\), \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) estarán negativamente correlados.\n\n\n1.2.7 Contrastes de hipótesis\nEn una regresión lineal simple, realizar un contraste de hipótesis sobre el parámetro \\(\\beta_1\\) asociado a la variable explicativa, es equivalente al contraste de hipótesis sobre el coeficiente de correlación lineal de Pearson entre dicha variable y la respuesta, \\(\\rho\\): \\[\\left. \\begin{array}{ll}\nH_0: \\beta_1 = 0 \\\\\nH_1: \\beta_1 \\neq 0\n\\end{array} \\right\\rbrace\n\\qquad \\equiv  \\qquad\n\\left. \\begin{array}{ll}\nH_0: \\rho = 0 \\\\\nH_1: \\rho \\neq 0\n\\end{array} \\right\\rbrace\\]\nPodemos determinar la significación del coeficiente \\(\\beta_1\\) (de una correlación). El p-valor nos dará la fuerza de dicha significación. Para parejas de variables normales e incorreladas se cumple que: \\[\\rho\\sqrt{\\frac{n-2}{1-\\rho^2}} \\sim t_{n-2}.\\] Esto también se cumple de forma aproximada si las variables son no normales y si los tamaños de muestra son “grandes” (o no demasiado pequeños).\n(Apartado 5.6 de Peña, Inferencia parámetros) Habla de que la pdf de los estimadores obtenida en 5.5 se utiliza para construir estadísticos “t” que proporcionen IC y HT sobre los parámetros del modelo de regresión.\n(5.6.1 Fundamentos) Interesante!!! … Proporciona la expresión general (5.20) del IC para cualquier parámetro beta de centralización a partir de su estimador (si sigue una distribución normal)… Y en la tabla 5.4 aparecen los estadísticos (“pivote”) para IC y para HT, que son distintos para beta0 y beta1 !!!\n(5.6.2 El contraste de regresión)\nAunque “presentado” en 5.6.1 aquí lo relacionan con el ANOVA… Se trata de comparar la variabilidad/varianza residual (para cada \\(x_i\\) fijo) con la variabilidad/varianza de \\(y\\)… Si ambas son próximas concluiremos que \\(\\beta_1\\) puede ser 0!! Mientras que si la residual es mucho que menor que la de \\(y\\) concluiremos que \\(\\beta_1\\) es significativamente distinto de 0.\nEn la expresión (5.23) descompone las desviaciones de los datos (notación matricial) en dos componentes ortogonales. Por el teorema de Pitágoras se llega a la descomposición de la variabilidad (que aquí vemos en el siguiente apartado). Hace referencia a que ya salen estos resultados en el capítulo 2 (DoE) Introduce VT, VNE y VE como “variación” Total, no explicada y explicada… y en el apéndice 5B demuestra la distribución que siguen. (Interpretación página 271) Dice que es importante resaltar el supuesto de linealidad… Concluyendo que “aceptar H0” (recta horizontal) no implica que \\(x\\) e \\(y\\) sean independientes y lo ilustra con la figura 5.17 (típica de relación cuadrática entre x e y… -NO son independientes-, con recta de regresión estimada horizontal, x e y son linealmente independientes!!!)\n(Último párrafo pagina 272) “Estrictamente sólo podemos concluir a partir del contraste de regresión que las variable son independientes cuando tienen conjuntamente una distribución normal, ya que entre variables conjuntamente normales sólo son posibles relaciones lineales”.\n(Peña *5.7.2 Inferencias acerca del coeficiente de correlación (lineal),\n“La distribución en el muestreo de \\(r\\) es complicada. Para CH o IC… transformación de Fisher…” )\n\n\n1.2.8 Suma de Cuadrados en la regresión\nLos residuos, distancias verticales entre cada uno de los puntos, \\((x_i,y_i)\\), y la recta de regresión, \\(\\hat{y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\), expresan el error aleatorio del modelo. ¿Hasta qué punto es más importante el efecto de la variable \\(X\\) sobre la variable \\(Y\\) que el error de los residuos?\nLa recta de regresión siempre pasa por el centro de los datos \\((\\bar{X},\\bar{Y})\\), ese punto se conoce como centroide. Es el centro de gravedad de la nube de puntos.\nSi las variables \\(X\\) e \\(Y\\) no estuviesen relacionadas, no aportaría información sobre \\(Y\\) conocer los valores de \\(X\\). La mejor predicción que podríamos hacer sería predecir \\(Y\\) con su media, \\(\\bar{Y}\\), sin tener en cuenta el valor de \\(X\\). Este modelo, el mas sencillo, es el que vamos a intentar falsar.\n\\[\\begin{align*}\nH_0\\!:&\\ \\beta_1 = 0 \\\\\nH_1\\!:&\\ \\beta_1 \\neq 0\n\\end{align*}\\]\nEl estudio de la variabilidad (información) de la variable respuesta nos aportará evidencias que nos permitan rechazar \\(H_0\\) (falsarla).\n\\[SC_{total}=SC_y=\\sum (y_i-\\bar{y})^2\\]\n\\(SC_{total}\\) es el numerador de la habitual varianza muestral. Se puede calcular multiplicando dicha varianza por los grados de libertad \\(n-1\\).\nMatemáticamente se puede descomponerse en \\(SC_{regresion}\\) y \\(SC_{residual\\).\n\\[SC_{total}=SC_{regresion}+SC_{residual}\\] \\[\\sum (y-\\bar{y})^2={\\sum (\\hat{y}-\\bar{y})^2}+\\sum (y-\\hat{y})^2\\]\nEstudiamos los grados de libertad (gl) al calcular cada uno de los términos. Para \\(SC_{total}\\) “gastamos” \\(1\\) gl al dar la media, \\(\\bar{y}\\), por lo que tiene \\(n-1\\) gl. Los gl de los residuos son \\(n-2\\), necesitamos \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) para calcular \\(SC_{residuos}\\).\nCon lo que nos queda \\(1\\) gl para la suma de cuadrados de la regresión \\(SC_{regresion}\\). “Gastamos” \\(1\\) gl con el parámetro extra que hemos estimado \\(\\hat{\\beta}_1\\), la pendiente.\nPor último, se promedia cada suma de cuadrados por sus respectivos grados de libertad (SCM: Suma de Cuadrados Media), en definitiva, calculamos varianzas.\nNunca seremos capaces de realizar predicciones perfectas, todos los modelos son falsos, pero estamos interesados en comparar el Efecto de \\(X\\) sobre \\(Y\\) con el Error Aleatorio (residual):\n\\[\\frac{\\text{Efecto de X sobre Y}}{\\text{Error Aleatorio}}=\\frac{\\text{Varianza de la Regresión}}{\\text{Varianza Error}}=\\frac{SCM_{regresion}}{SCM_{error}}=F\\]\nEsto se conoce en estadística como un Análisis de la Varianza,ANOVA.\nNuestro modelo sencillo que intentamos falsar es \\(H_0:\\beta_1=0\\). Para que podamos falsar dicha hipótesis la Varianza de la Regresión debe ser mayor que la Varianza del Error, cuanto más grande mejor.\nComparamos entonces el estadístico F obtenido de dividir las dos varianzas con una distribución F con los grados de libertad correspondientes, \\(1\\) en el numerador y \\(n-2\\) en el denominador. Calculando el p-valor correspondiente se podrá rechazar (o no) la hipótesis nula.\nEn Peña (2002), Apéndice 5B, se dan detalles sobre la “Deducción de las distribuciones de sumas de cuadrados”.\n\n\n1.2.9 Bondad de ajuste: Coeficiente de Determinación, \\(R^2\\)\n(Faraway 2.7 “regresión simple”,\nPeña 5.7 El coef de corr en regresión,\n5.7.1 Coeficiente de determinación y coeficiente de correlación lineal)\nPor la descomposición mencionada anteriormente, \\(SC_{total}=SC_{regresion}+SC_{residual}\\), la proporción de variabilidad explicada por la regresión, respecto al total es un indicador de la bondad de la regresión:\n\\[\\frac{SC_{regresion}}{SC_{total}} = R^2\\]\nEsto es lo que se conoce como coeficiente de determinación, \\(R^2\\). Como proporción, puede tomar valores entre \\(0\\) y \\(1\\), evitando así la dependencia de las unidades de medida, y suele expresarse en porcentaje.\nLa siguiente Figura permite ilustrar el concepto de bondad de ajuste.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nEn la gráfica de la izquierda se tiene un buen ajuste de la recta de regresión, que produce un \\(R^2\\) elevado. Mientras que en la gráfica de la izquierda el \\(R^2\\) es próximo a cero, indicando que el conocimiento de los valores de \\(X\\) aporta casi nula información sobre los valores de variable \\(Y\\). Para más énfasis en la imagen de la izquierda se muestra la variabilidad total de \\(Y\\) frente a la variabilidad que tiene \\(Y\\) para casi cualquier valor de \\(X\\).\nObtención del coeficiente\nEn el caso de la regresión lineal simple coincide con el cuadrado del coeficiente de correlación, \\(\\rho\\) (véase Peña (2002), apartado 5.7.1, donde se dan detalles de cómo llegar al coeficiente de correlación a partir del de determinación, o a partir de la varianza residual).\n\n\n1.2.10 Error estándar de la pendiente y la ordenada en el origen\n(véase Apéndice 5A Peña: Deducción de los IC para los parámetros)\nAdemás del test F (ANOVA), es importante la varianza residual, \\(SC_{residuos}/n-2\\). La varianza residual cumple un papel importante para calcular el error estándar de la pendiente, \\(\\beta_1\\):\n\\[EE_b=\\sqrt{\\frac{SC_{residuos}/n-2}{SC_x}}\\]\nCon este error estándar podemos calcular un intervalo de confianza para la pendiente.\n\\[\\beta_1\\pm t_{\\alpha/2,n-2}\\cdot EE_{\\beta_1}\\]\nTambién la varianza residual es importante para calcular el error estándar de la ordenada en el origen, \\(\\beta_0\\):\n\\[EE_{\\beta_0}=\\sqrt{\\frac{SC_{residuos}/n-2 \\cdot \\sum x_i^2}{n \\cdot SC_x}}\\]\nPudiendo de la misma manera calcular intervalos de confianza para la ordenada en el origen.\nLo más interesante de esto es poder calcular el error estándar de una predicción y poder obtener intervalos de confianza para las predicciones.\n\\[EE_{\\hat{y}}=\\sqrt{\\frac{SC_{residuos}}{n-2}(\\frac{1}{n}+\\frac{(x_i-\\bar{x})^2}{SC_x})}\\]\n\\[(\\beta_0+\\beta_1\\cdot x_i) \\pm t_{\\alpha/2,n-2}\\cdot EE_{\\hat{y}}\\]\nCon R podremos obtener intervalos de confianza para predicciones del modelo. Otras herramientas son las bandas de confianza que algunos paquetes pueden dar a nuestros gráficos de regresión. Estas son bandas de confianza para la respuesta media, \\(\\bar{Y}\\) para cada valor individual de la \\(X\\). Esto significa que tenemos una confianza, habitualmente del 95%, en que la verdadera recta de regresión cae en la región marcada.\nLos intervalos de confianza para las predicciones son más amplios que para los intervalos de confianza de la recta.\n\n\n1.2.11 Interpretación de la recta de regresión\n(véase 5.9 Peña, tb cap 15 de FDCR)\nUna vez estimada la recta de regresión, si es significativa estadísticamente hablando, se puede pasar a interpretar los coeficientes/parámetros, si tienen un sentido práctico (y el análisis de residuos que veremos más adelante no invalida los supuestos en los que se basa).\nLa interpretación generalmente más importante es la del coeficiente \\(\\beta_1\\) dado que recoge el efecto sobre la variable \\(y\\) de la variación de una unidad de la variable explicativa \\(x\\). Su interpretación debe hacerse acorde a las unidades en la que esté recogida la variable (no es lo mismo que, si es una medida de temperatura, se haya medido en ºC que en ºK, o si es de tiempo que se mida en segundos o en días). Así el impacto sobre la respuesta del cambio de \\(1\\) ºC o de \\(1\\) s será de una magnitud muy distinta al cambio de 100ºC o \\(1\\) hora. Es más, el valor del coeficiente \\(\\beta_1\\) se podría aumentar o disminuir haciendo cambios de escala en las variables \\(y\\) y \\(x\\). Por último, ¿y qué pasa si en lugar de tener sólo una variable tenemos más variables? Veremos en el apartado siguiente de regresión múltiple, el cambio que supone, en la interpretación de los coeficientes, el tener varias variables explicativas.\nLa interpretación del parámetro \\(\\beta_0\\) es el del valor medio en ausencia del valor de la \\(x\\), que en ciertas ocasiones no pertenece al rango de variación de la variable \\(x\\) o puede no tener sentido (piense por ejemplo que la variable \\(x\\) recoge la edad de los individuos, ¿tiene sentido la media a \\(0\\) años?).\nSi la recta no fuese significativa, no significa que no haya relación entre \\(x\\) e \\(y\\), quizá la relación es no lineal (como veremos en el siguiente subapartado) o quizá el rango escogido para la \\(x\\) no es el idóneo para observar su influencia sobre la \\(y\\) (quizá es demasiado estrecho). Pero, como señala Peña, para encontrar relaciones causales hay que acudir a datos experimentales (como veremos en un tema posterior), porque en un experimento se puede intentar controlar los valores de la variable \\(x\\) que se cree que influyen sobre la \\(y\\) y aleatorizar el resto de variables para “repartir” su impacto sobre la respuesta. Mientras que si los datos son observacionales sólo se puede deducir covariación, pero no causalidad, como que haya más criminalidad en las ciudades con más policías. Aquí la causa es una tercera variable, el tamaño de la ciudad. Reducir el número de policías no causaría una reducción de la criminalidad!!\nAdemás se puede ver Peña Apéndice 5C: modelo con regresor aleatorio\n\n\n1.2.12 Diagnosis\n(Peña Capítulo 6 Diagnosis (y predicción))\n(Sección 3.3.3 del libro “Statlearning”. )\nEl diagnóstico de los residuos permite comprobar las hipótesis sobre las que se sustenta el modelo de regresión lineal, a saber:\n\nlinealidad\nhomocedasticidad\nnormalidad\nindependencia\n\nCon la ayuda de R será bastante sencillo obtener los denominados gráficos de diagnóstico. Pero ¿qué se pretende observar en dichos gráficos? En líneas generales, se pretende observar aleatoriedad en los residuos, que la “nube de puntos” que generen no se aprecie ningún patrón sistemático. Salvo en el gráfico Q-Q en el que se mira la normalidad de los residuos, que se desea que se ajusten a la diagonal que aparece en el gráfico. Concretamente:\n\nlinealidad: a partir del gráfico de residuos fente a valores ajustados con el modelo para cada valor observado, se deseará observar aleatoriedad, valores dispersos entorno al 0 (verticalmente), sin tendencias ni otros patrones marcados. ## POR AQUÍ (JUAN)\nhomocedasticidad: a partir de un gráfico de residuos studentizados (en valor absoluto?) frente a valores observados de la variable \\(x\\). De nuevo se desea observar aleatoriedad, ausencia de patrones. Si para unos valores de \\(s\\) se aprecia mucha más dispersión que para otros, los datos podrían presentar heterocedasticidad\nnormalidad: se visualizará mediante el gráfico Q-Q (de cuantiles teóricos, frente a cuantiles observados de los residuos), cuando más alineados a la diagonal que aparece, más cercanos a la normalidad teórica estarán los datos. Se incumple la normalidad generalmente por las colas, con puntos que se alejan ostensiblemente de la recta “guía”.\nindependencia: se visualizará mediante el gráfico…\n\n(Faraway CAP 4)\nA diferencia de la clasificación anterior, Faraway (hablando de regresión múltiple) divide los problemas potenciales de la regresión obtenida en 3 categorías:\n\nerror: se asume que los errores son independientes, tienen varianza constante y son normalmente distribuidos.\nmodelo: se asume que la parte estructural del modelo es correcta, es decir, que relación entre las variables es lineal.\nobservaciones inusuales: en ocasiones unas pocas observaciones no se ajustan al modelo, o cambian/influyen demasiado en el modelo.\n\nAdemás indica que las técnicas gráficas de diagnóstico son más flexibles, pero mucho más difíciles de interpretar, que las de contrastes que son sencillas pero directas, no requieren de intuición, pero no permiten una visualización general del problema.\nLa diagnosis de los residuos puede dar pistas para solucionar los problemas de la regresión, lo que implica que la modelización es un proceso iterativo, con cierto aire artesanal.\n(Faraway 4.1 Checking error assumptions)\nDeja clara la diferencia entre error y residuo… “the errors may have equal variance and be uncorrelated, the residuals do not.”\n\n\n1.2.13 Homocedasticidad\nLos residuos deben tener varianza constante, homocedasticidad, con respecto a los valores de la variable \\(x\\), en el caso de regresión simple. El gráfico de residuos apropiado para apreciarlo es el de residuos frente a valores ajustados (que es también válido para el caso de regresión múltiple).\nEn el siguiente gráfico se pueden ver ejemplos simulados de distintas situaciones, que se interpretan de una forma clara. Pero la realidad supera la ficción… Se necesita cierta experiencia con casos reales para no cometer equivocaciones al interpretar gráficos de residuos.\n\n\n\n\n\n\n\n\n\nLo deseable en este gráfico es encontrar una nube de puntos dispersos (y simétricos, mirando verticalmente) alrededor de 0, sin ningún patrón aparente (como en el primer gráfico). Si se observa un patrón, como la forma cónica (como en el segundo gráfico) o una tendencia (como la no lineal que aparece en el tercer gráfico), puede indicar problemas de heterocedasticidad (varianza no constante) o no linealidad en el modelo. Estas dos últimas situaciones dan pistas de las posibles acciones a tomar sobre los datos, como transformar la variable \\(x\\) o \\(y\\) para conseguir homocedasticidad, o incluir algún término no lineal en la variable \\(x\\) (manteniendo el modelo lineal en los parámetros). Por su parte, el primer gráfico permitiría validar gráficamente el supuesto de homocedasticidad.\n(Faraway habla de plotear los residuos frente a cada uno de los \\(x_i\\) -regresión múltiple- tanto los \\(x_i\\) incluidos en el modelo, como los no incluidos, mirando en estos últimos si existe alguna relación que indique la necesidad de incluirlo en el modelo)\n(Faraway tb se explaya con el plot del valor absoluto de los residuos frente a los valores ajustados, me suena que es uno de los gráficos de diagnóstico de R. Dice que está diseñado especialmente para comprobar la varianza no constante, “doblando” la mitad inferior sobre la superior para aumentar la resolución para detectar varianza no constante. Pero que el primer plot indicado es necesario para comprobar la nolinealidad.\nTambién calcula una regresión (lm) del valor absoluto de los residuos frente a los fitted, observando un p-valor &gt; 0.05, por lo que se puede asumir varianza constante, pero habla de que no es totalmente correcto)\n(Faraway) Existen test formales para detectar heterocedasticidad, como hacer una regresión sobre los residuos, pero dependen de la hipótesis alternativa especificada (la hipótesis nula es clara: varianza constante). Así, el contraste puede detectar bien un tipo específico de heterocedasticidad, pero no tener potencia suficiente para otros.\nLos contrastes formales pueden parecer más precisos, pero los gráficos de residuos son más versátiles: permiten detectar problemas, revelar estructuras ocultas, que no se vislumbraban con los contrastes formales. Sin embargo, como se ha mencionado, a veces los gráficos pueden ser ambiguos, pero al menos permiten verificar que no hay grandes desviaciones de los supuestos del modelo.\nPor todo ello, predomina un enfoque gráfico para el diagnóstico, acudiendo a los contrastes formales como complemento para confirmar lo observado en los gráficos. (fin)\n\n\n1.2.14 Normalidad\nLos residuos deben seguir una distribución normal. El gráfico apropiado para evaluarlo es el denominado Q-Q plot (gráfico Cuantil-Cuantil): un diagrama de dispersión de los cuantiles de los residuos frente a los cuantiles de la distribución normal (con la misma media y desviación típica de los residuos). La interpretación, como diagrama de dispersión, es bien sencilla, los residuos siguen una distribución normal cuanto más se alineen los puntos del diagrama sobre una linea recta (diagonal)). Desviaciones de la “linealidad” sugerirán otros casos, por ejemplo, es típico observar curvatura en forma de S, indicando que los residuos tiene colas más ligeras que la distribución normal, o curvatura en forma de U indicando que siguen otra distribución.\n\n\n\n\n\n\n\n\n\n\n\n1.2.15 Observaciones influyentes\n(Peña 6.3 Observaciones atípicas e influyentes\n6.3.1 puntos palanca ¡¡¡!!!\n6.3.2 Residuos estandarizados\n6.3.3 Observaciones influyentes 6.3.4 Tratamiento de los atípicos)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#modelo-lineal-de-regresión-múltiple",
    "href": "Cap1-LM.html#modelo-lineal-de-regresión-múltiple",
    "title": "1  Modelos lineales",
    "section": "1.3 Modelo lineal de regresión múltiple",
    "text": "1.3 Modelo lineal de regresión múltiple\nCuando disponemos de más de una variable explicativa o independiente (continuas), entonces nuestro modelo pasa a ser un modelo con múltiples variables. ¿Cuáles? ¿En qué relación?\nLa regresión múltiple tiene los siguientes retos:\n\nLa mayor parte de los estudios son observacionales, no experimentales.\nEs muy habitual tener demasiadas variables explicativas.\nEs muy habitual tener pocas observaciones, valores de la variable respuesta.\nNo disponer de todas las combinaciones posibles de las variables explicativas.\n\nDesde el punto de vista estadístico:\n\nCuidado con las variables explicativas correlacionadas o cordelaras (apartan información redundante).\n¿Qué variables incluir en el modelo?\n¿Hay curvatura en la respuesta? Modelo lineal de regresión no lineal.\nLas variables explicativas ¿interaccionan entre sí?\nTodo esto hace que el número de parámetros se dispare Vs pocas observaciones.\n\nRecordemos:\n\nTodos los modelos son falsos.\nAlgunos modelos son mejores que otros.\nEl modelo perfecto y exacto no existe.\nEn un modelo, la sencillez es un acierto.\n\nTipos de modelos:\n\nSaturado: Un parámetro para cada observación. Ajuste perfecto. Grados de libertad 0.\nMaximal: Contiene p variables y sus interacciones. Muchos de estos términos son despreciables. Grados de libertad \\(n-p-1\\).\nMinimal y Adecuado: Contiene las variables e interacciones significativas. Grados de libertad \\(n-p'-1\\).\nModelo ‘Nulo’:Único parámetro, \\(\\bar{y}\\). Grados de libertad \\(n-1\\).\n\nCorrelación\nEn la regresión múltiple, hay que tener claro que la correlación se desea entre cada variable explicativa y la respuesta, y no entre ellas. De haber correlación entre variables explicativas se presenta el problema denominado multicolinealidad. Para resolverlo se puede acudir a la selección de variables (Cap 2), teniendo en cuenta el factor de inflación de la varianza (VIF). Ahora bien, conviene recordar que correlación no implica causalidad (ejemplos: delitos vs policías, limones vs accidentes,…).\n\n1.3.1 Estimación MC\nEn 2.4 de Faraway se puede ver la obtención del estimador MC para los parámetros del modelo de regresión lineal múltiple: \\[\\hat{\\beta}=(X^\\top X)^{-1}X^\\top\\] siempre que \\(X^\\top X\\) sea invertible.\nY se habla de la matriz \\(H=X(X^\\top X)^{-1}X^\\top\\), denominada the hat-matrix, the orthogonal projection of y onto the space spanned by X.\nCon esta matriz, se pueden expresar:\n\nlos valores predichos o ajustados: \\(\\hat{Y}=HY=X\\hat{\\beta}\\)\nlos residuos: \\(\\hat{\\epsilon}=Y-X\\hat{\\beta}=(I-H)Y\\)\nla suma de cuadrados residual (RSS por sus siglas en inglés): \\(\\hat{\\epsilon}^\\top\\hat{\\epsilon}=Y^\\top((I-H)Y\\)\n\n(Traído de la INTERP GEO) En Faraway (2.3) se dice que el propósito conceptual del modelo es representar, de la mejor manera posible, la complejidad de la respuesta, dada en el espacio n-dimensional, en un espacio más pequeño, el k-dimensional de las variables. Si el modelo se ajusta bien, la estructura de los datos queda capturada en esas k dimensiones, dejando la variación aleatoria en los residuos que pertenecen a un espacio de dimensión n-k.\n\n\n1.3.2 Bondad de ajuste\nEn el caso de regresión lineal múltiple con el cuadrado del coeficiente de correlación múltiple. El \\(R^2\\) ajustado es una corrección para suavizar el comportamiento del \\(R^2\\) que aumenta al incluir en el modelo más variables explicativas.\n¿Fórmula del R2 ajustado?\n\n\n1.3.3 Teorema de Gauss-Markov\n(véase 2.6 de Faraway) Faltan detalles en las fórmulas MTM, recortar!!\nHay varias razones para usar el estimador MC de \\(\\beta\\).\n\nes el resultado de una proyección ortogonal sobre el espacio del modelo (interpretación geométrica).\nes también el estimador de máxima verosimilitud si los errores son independientes e idénticamente distribuidos siguiendo una normal.\nes el mejor estimador lineal insesgado (BLUE) según el teorema de Gauss-Markov.\n\nEl teorema de Gauss-Markov se basa en el concepto de función estimable. Una combinación lineal de los parámetros, \\(\\psi = c^\\top\\beta\\), es estimable si, y solo si, existe una combinación lineal \\(\\mathbf{a}^\\top \\mathbf{y}\\) tal que:\n\\[\n\\mathbb{E}[\\mathbf{a}^\\top \\mathbf{y}] = \\mathbf{c}^\\top \\boldsymbol{\\beta}\n\\]\nLas funciones estimables incluyen predicciones de observaciones futuras, lo que explica por qué vale la pena considerarlas. Si la matriz \\(\\mathbf{X}\\) tiene rango completo, entonces todas las combinaciones lineales son estimables.\nSupuestos del modelo\nSupongamos que \\(\\mathbb{E}[\\boldsymbol{\\varepsilon}] = 0\\) y \\(\\text{Var}(\\boldsymbol{\\varepsilon}) = \\sigma^2 \\mathbf{I}\\). Supongamos también que la parte estructural del modelo, \\(\\mathbb{E}[\\mathbf{y}] = \\mathbf{X} \\boldsymbol{\\beta}\\), es correcta. Sea \\(\\mathbf{c}^\\top \\boldsymbol{\\beta}\\) una función estimable; entonces el teorema de Gauss-Markov establece que, dentro de la clase de todos los estimadores lineales insesgados de \\(\\mathbf{c}^\\top \\boldsymbol{\\beta}\\), el estimador de mínimos cuadrados tiene la varianza mínima y es único.\nDemostración\nSupongamos que \\(\\mathbf{a}^\\top \\mathbf{y}\\) es un estimador insesgado de \\(\\mathbf{c}^\\top \\boldsymbol{\\beta}\\), lo que implica que:\n\\[\n\\mathbf{a}^\\top \\mathbf{X} = \\mathbf{c}^\\top\n\\]\nEsto implica que \\(\\mathbf{c}\\) está en el espacio imagen de \\(\\mathbf{X}^\\top\\), y por tanto también en el espacio imagen de \\(\\mathbf{X}^\\top \\mathbf{X}\\), lo que significa que existe un vector \\(\\boldsymbol{\\lambda}\\) tal que \\(\\mathbf{c} = \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\lambda}\\). Entonces:\n\\[\n\\text{Var}(\\mathbf{a}^\\top \\mathbf{y}) = \\sigma^2 \\mathbf{a}^\\top \\mathbf{a}\n\\]\ny\n\\[\n\\text{Var}(\\mathbf{c}^\\top \\hat{\\boldsymbol{\\beta}}) = \\sigma^2 \\mathbf{c}^\\top (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{c}\n\\]\nComo las varianzas no pueden ser negativas, se concluye que:\n\\[\n\\text{Var}(\\mathbf{a}^\\top \\mathbf{y}) \\geq \\text{Var}(\\mathbf{c}^\\top \\hat{\\boldsymbol{\\beta}})\n\\]\nEs decir, el estimador de mínimos cuadrados tiene varianza mínima. Para demostrar que es único, debe cumplirse que:\n\\[\n\\text{Var}(\\mathbf{a}^\\top \\mathbf{y} - \\mathbf{c}^\\top \\hat{\\boldsymbol{\\beta}}) = 0\n\\]\nlo que implica que \\(\\mathbf{a}^\\top - \\mathbf{c}^\\top (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top = 0\\), es decir, \\(\\mathbf{a}^\\top = \\mathbf{c}^\\top (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top\\), por lo tanto el estimador es único. Esto completa la demostración.\nConsideraciones adicionales\nEl teorema de Gauss-Markov muestra que el estimador de mínimos cuadrados es una buena elección, pero requiere que los errores sean no correlacionados y con varianza constante. Incluso si los errores son no normales pero se comportan bien, estimadores no lineales o sesgados pueden funcionar mejor. Por tanto, el teorema no obliga a usar mínimos cuadrados siempre, pero lo recomienda fuertemente salvo que haya una buena razón para no hacerlo.\nSituaciones donde considerar otros estimadores\n\nCuando los errores están correlacionados o tienen varianza desigual, se deben usar mínimos cuadrados generalizados (ver Sección 6.1).\nCuando la distribución de errores tiene colas largas, se pueden usar estimadores robustos, que típicamente no son lineales en \\(\\mathbf{y}\\) (ver Sección 6.4).\nCuando los predictores están altamente correlacionados (colinealidad), se pueden preferir estimadores sesgados como la regresión ridge (ver Capítulo 9).\n\n\n\n1.3.4 (Identificabilidad)\nLa identificabilidad es un concepto clave en modelos estadísticos, especialmente en regresión. Un modelo es identificable si los parámetros del modelo pueden ser estimados de manera única a partir de los datos observados. En otras palabras, para cada conjunto de datos, hay una única solución para los parámetros del modelo. Un modelo es no identificable si hay múltiples conjuntos de parámetros que pueden generar los mismos datos observados. Esto puede ocurrir por varias razones, como la presencia de variables redundantes o la falta de información suficiente en los datos para distinguir entre diferentes configuraciones de parámetros.\nEn el contexto de la regresión lineal, la identificabilidad se refiere a la capacidad de estimar los parámetros del modelo de manera única a partir de los datos. Si el modelo es identificable, entonces cada conjunto de datos conducirá a una única estimación de los parámetros \\(\\boldsymbol{\\beta}\\) .\nEl estimador de mínimos cuadrados es la solución de las ecuaciones normales:\n\\[\n\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}^\\top \\mathbf{y}\n\\]\ndonde \\(\\mathbf{X}\\) es una matriz de dimensión \\(n \\times p\\) . Si \\(\\mathbf{X}^\\top \\mathbf{X}\\) es singular y no puede invertirse, entonces habrá infinitas soluciones para las ecuaciones normales y el modelo será, al menos en parte, no identificable.\nLa no identificabilidad ocurre cuando \\(\\mathbf{X}\\) no tiene rango completo, es decir, cuando sus columnas son linealmente dependientes. En datos observacionales, esto suele deberse a algún descuido. Algunos ejemplos:\n\nSe mide el peso de una persona tanto en libras como en kilos, y ambas variables se incluyen en el modelo.\nPara cada individuo se registran los años de educación preuniversitaria, los años de educación universitaria y también el total de años de educación, y se incluyen las tres variables en el modelo.\nHay más variables que casos, es decir, \\(p &gt; n\\) . Cuando \\(p = n\\) , tal vez se puedan estimar todos los parámetros, pero no quedarán grados de libertad para estimar errores estándar ni realizar pruebas. A este modelo se le llama saturado. Cuando \\(p &gt; n\\) , se le llama supersaturado. Curiosamente, estos modelos se consideran en experimentos de cribado a gran escala usados en diseño y fabricación de productos, pero no hay esperanza de estimar de forma única todos los parámetros en tales modelos.\n\nEstos problemas pueden evitarse prestando atención. La identificabilidad es un tema más relevante en experimentos diseñados. Considera un experimento simple con dos grupos, donde las observaciones del tratamiento son \\(y_1, \\dots, y_n\\) y las del grupo control son \\(y_{n+1}, \\dots, y_{m+n}\\) . Supongamos que intentamos modelar la respuesta con una media general \\(\\mu\\) y efectos de grupo \\(\\tau_1\\) y \\(\\tau_2\\) :\nAunque \\(\\mathbf{X}\\) tiene tres columnas, su rango es solo 2: los parámetros \\((\\mu, \\tau_1, \\tau_2)\\) no son identificables y las ecuaciones normales tienen infinitas soluciones. Podemos resolver este problema imponiendo restricciones, como \\(\\mu = 0\\) o \\(\\tau_1 + \\tau_2 = 0\\) , por ejemplo.\nLos paquetes estadísticos manejan la no identificabilidad de distintas formas. En el caso de regresión anterior, algunos pueden devolver mensajes de error, y otros pueden ajustar el modelo porque los errores de redondeo eliminan la no identificabilidad exacta. En otros casos, se aplican restricciones, pero estas pueden ser diferentes de las que uno espera. Por defecto, R ajusta el modelo identificable más grande eliminando variables en orden inverso al que aparecen en la fórmula del modelo.\n\n\n1.3.5 Transformaciones\nFamilia de transformaciones Box-Cox para la variable respuesta ¿no?\nIndicaciones de Faraway (en 4.1): En la práctica, el gráfico de residuos frente a valores ajustados puede dar pistas sobre la transformación a realizar. Obviamente se pueden intentar varias transformaciones, porque, de nuevo, es un arte encontrar la transformación más adecuada, la intuición y la experiencia pueden ayudar. Un pequeño detalle/truco es tener en cuenta que transformaciones como la raíz cuadrada o el logaritmo no funcionarán si la variable respuesta \\(y\\) toma valores negativos, pero se puede subsanar considerando \\(y+\\delta\\) para evitarlo (aunque se pierde la interpretación directa de los resultados).\n\n\n1.3.6 Otros casos de regresión lineal\nExisten otros modelos de regresión que pueden ajustarse con la técnica de modelos lineales mediante transformaciones. Por ejemplo si hay una relación exponencial entre las variables \\(X\\) e \\(Y\\), se pueden tomar logaritmos en ambas variables linealizando así el modelo. O si la relación es potencial, considerar una potencia de la variable \\(X\\): un modelo lineal de regresión no lineal. El tomar un modelo más complejo sólo tiene sentido si produce resultados significativos a la hora de explicar la relación.\nAhora bien, hay que tener en cuenta que estas transformaciones pueden alterar la interpretación de los coeficientes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#sec-pract-airquality",
    "href": "Cap1-LM.html#sec-pract-airquality",
    "title": "1  Modelos lineales",
    "section": "1.4 Caso práctico: airquality",
    "text": "1.4 Caso práctico: airquality\nEl primero de los casos prácticos de modelización lineal que se va a tratar en profundidad se basa en los datos airquality que contienen 153 medidas (de 6 variables) de calidad del aire en Nueva York. Entre otros, se estudia en Casero-Alonso y Durbán (2024), concretamente en: https://cdr-book.github.io/cap-lm.html#Casos. Aquí se presentarán ejemplos ligeramente distintos y con algo más de profundidad.\n\n1.4.1 Exploración de los datos\nAntes de comenzar el proceso de modelización lineal, es muy recomendable explorar los datos. El conjunto de datos airquality está disponible en la distribución base de R:\n\n?airquality #Para obtener más información sobre las variables, unidades, etc. \n\n\n#Primeras filas del data frame\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nComo se observa hay varios valores faltantes (NA) entre los datos, lo que puede afectar a los resultados de la regresión. Su impacto debe estudiarse, pero sobrepasa el nivel de este curso.\nAdemás, al observar detenidamente los valores de la variable Day se puede inferir que los datos son temporales, lo que requiere un análisis específico (de Series Temporales, que también sobrepasa el alcance de este curso). El tener datos temporales hace que se incumpla, de partida, desde el plano teórico/conceptual, el supuesto de independencia de las observaciones para la modelización lineal. Este detalle implica que algunas hipótesis se pueden comprobar de antemano, nacen del planteamiento del problema, de la recogida de datos, etc. sin necesidad de llegar al análisis de residuos, en el que incluso podría no quedar reflejado el incumplimiento.\nAun con esta situación (valores faltantes, datos temporalmente dependientes) analizaremos diversos modelos lineales. Eso sí, no tiene sentido intentar explicar la influencia lineal de la variable Day en el resto de variables, por ejemplo, por el hecho de que los días \\(1\\), \\(2\\), etc. de meses distintos no son “homogéneos”, etc.\nProcedemos a obtener resúmenes numéricos y gráficos:\n\n## Resúmenes numéricos de las variables\nsummary(airquality)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\n## Estructura (formato) del data frame\nstr(airquality)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n## Resumen gráfico de relaciones pareadas\npairs(airquality, upper.panel = panel.smooth)\n\n\n\n\n\n\n\n\nDel resumen numérico se obtiene que hay 2 variables Ozone y Solar.R que contienen NAs, principalmente la primera con 37 valores no disponibles. Estos resúmenes, junto con la visualización de la estructura (y la consulta de las unidades en las que están medidas) permiten determinar que las variables Ozone, Solar.R, Wind y Temp se pueden considerar cuantitativas continuas (obsérvense sus rangos) a los efectos de modelización lineal, aunque sólo Wind esté definida como num (y el resto como int). Sobre Temp, viendo el rango de sus valores (de 56 a 97), no parece que estén en ºC. ¿Y cómo considerar a la variable Month? El tratamiento más adecuado es como variable cualitativa, dado que, aunque vemos valores numéricos, de 5 a 9, no puede interpretarse como una variable continua en la que tenga sentido incrementar 1 unidad. Además, otorgarle valores 5 a 9 es un convenio para tratarlas por ordenador de una manera más cómoda, pero realmente sus valores son mayo, junio… Queda así más claro que no tiene sentido aumentar 1 unidad, por ejemplo, cuando estamos en el mes 12.\nDe los diagramas de dispersión, al incluir el panel.smooth se pueden observar líneas de tendencias suavizadas de los datos. Se aprecia que casi ninguna de las relaciones entre las variables numéricas es lineal, sólo lo parece Wind frente a Temp. Los gráficos que involucran a la variable Month se aprecian distintos al resto, por los pocos valores de dicha variable, mientras que los que involucran a Day no reflejan lo mismo.\n\nAlternativas: Existen distintas funciones/paquetes más o menos sofisticados que realizan este análisis exploratorio de distintas maneras. El lector interesado puede explorar: - El paquete skimr y su función skim. -\n\n\nPor ejemplo, con las siguientes funciones se pueden obtener gráficos complementarios al gráfico obtenido anteriormente con pairs().\n\n\n#Es necesario tener los paquetes instalados previamente\nlibrary(corrplot) \ncorrplot(cor(airquality, use = \"pairwise\"))\nlibrary(GGally)\nggpairs(airquality)\n#Para que aparezca los diagramas de dispersión \"arriba\"\nggpairs(airquality,\n         upper = list(continuous = wrap(\"points\", alpha = 0.7)),  \n         lower = list(continuous = wrap(\"cor\", size = 4)),  \n         diag = list(continuous = wrap(\"densityDiag\")))  \n\n\n\n1.4.2 lm() simple\nPara ilustrar toda la teoría de las secciones anteriores se va a realizar una regresión lineal simple. La función lm() de R proporciona, a partir de los datos disponibles, la estimación de los parámetros del modelo que se especifique. También se pueden obtener, aplicando distintas funciones, la significación de dichas estimaciones, sus intervalos de confianza, predicciones, etc. Así como los gráficos de diagnóstico.\nModelización\nConsideramos para empezar un modelo de regresión simple, a diferencia del libro CDR. De entre los posibles modelos nos decantamos por intentar explicar la concentración de Ozono en función de la radiación solar: \\[Ozone = \\beta_0 + \\beta_1 Solar.R + \\epsilon\\]\n\nEl lector tiene aquí una buena tarea conceptual, la de ejercitarse en plantear modelizaciones lineales, que ¡tengan sentido práctico!\n¿Tiene sentido explicar/predecir Ozone en función de los valores de Solar.R observados? ¿Y al revés? ¿O explicar/predecir Day en función de Ozone? …\n\nEn R definiríamos así el modelo anterior:\n\nmodelo &lt;- Ozone ~ Solar.R\n\n\nLa sintaxis básica (regresión simple) es respuesta ~ explicativa.\nLa extensión a regresión múltiple es directa: resp ~ explica1 + explica2 indicaría un modelo con predictores explica1 y explica2, y así sucesivamente.\nEn la sección de regresión múltiple se indican algunos “trucos”.\n\n\nPregunta\n¿Cómo obtener la regresión de proporcionalidad directa con R?\n\nEstimación\nEn R pueden obtenerse de varias maneras utilizando la función lm():\n\n#Opción 1: aprovechando la definición anterior del modelo\n#lm(modelo, data = airquality) \n#Opción 2: directa -&gt; lectura más clara\nlm(Ozone ~ Solar.R, \n   data = airquality)\n\n\nCall:\nlm(formula = Ozone ~ Solar.R, data = airquality)\n\nCoefficients:\n(Intercept)      Solar.R  \n    18.5987       0.1272  \n\n\nAquí se pueden ver las estimaciones para los dos parámetros obtenidas a partir de los datos (omitiendo las observaciones con valores NA).\n\nLa función lm() aplicada a un modelo simple y ~ x (o múltiple, como veremos) requiere que los datos de las variables y y x estén en el Environment, o se especifique en data el conjunto de datos en el que están las dos variables.\n\nDiagrama de dispersión y recta estimada\nDado que se está analizando la relación entre dos variables, esta se puede visualizar fácilmente en un diagrama de dispersión, al que se puede añadir la recta estimada a partir de los datos.\n\npar(pty = \"s\") ## \"p\"lot \"ty\"pe \"s\"quare (recomendado para gráficos de dispersión)\nplot(airquality$Solar.R, airquality$Ozone)\nabline(a=18.5987, b=0.1272, col = \"red\") ## a = intercept, b = slope\n\n\n\n\n\n\n\n\nEn problemas de regresión múltiple con dos variables explicativas se puede obtener un diagrama de dispersión en 3 dimensiones y añadirle el plano de regresión, pero suele ser compleja su visualización. Con más variables es imposible obtener tales visualizaciones, diagramas de dispersión en \\(k\\) dimensiones e hiperplanos de regresión, lo que conduce a abordar el problema de regresión lineal múltiple mediante un proceso de abstracción.\nAnálisis de residuos\nValoremos la adecuación del modelo examinando los residuos. Primero guardamos el ajuste/estimación basado en los datos en un objeto, que denominamos rls, para su uso posterior:\n\nrls &lt;- lm(Ozone ~ Solar.R, data = airquality)\npar(mfrow = c(2, 2), #presenta los gráficos en formato 2x2\n    pty = \"s\",\n    mex = 0.66,\n    cex = 0.75)\nplot(rls)\n\n\n\n\n\n\n\n\n\nEl primero de los 4 gráficos de diagnóstico (Residuals vs Fitted) refleja heterocedasticidad (varianza no constante, forma de embudo), tal y como ya se podía apreciar en el diagrama de dispersión de Solar.R frente a Ozone que se obtuvo anteriormente. En este gráfico de aquí, conforme aumenta el valor de los valores ajustados (fitted), la dispersión de los residuos se hace más grande, salvo en la parte final.\n\nAlternativamente, se pueden dibujar los residuos studentizados, obtenibles a través de la función rstudent(), frente a los valores ajustados:\nplot(fitted(rls), rstudent(rls))\n\nEl gráfico que mejor refleja esa heterocedasticidad es el tercero (Scale-Location), donde la línea roja de tendencia dista de la horizontalidad (que reflejaría homocedasticidad) teniendo una pendiente positiva (ligera, pero apreciable).\nRespecto a la normalidad, a la vista del segundo gráfico (Q-Q Residuals), se observa una clara desviación de la linea recta punteada (que marcaría el ajuste perfecto a la distribución normal), sobre todo en la parte superior derecha. Para reforzar la impresión de este gráfico se puede acudir a otra visualización y a un contraste. Lo más apropiado es un histograma y el contraste de Shapiro-Wilk:\n\n\nhist(residuals(rls))\n\n\n\n\n\n\n\nshapiro.test(residuals(rls))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(rls)\nW = 0.91418, p-value = 2.516e-06\n\n\nEl histograma refleja una clara asimetría, que dista de la simetría de la distribución normal. Y el p-valor del contraste lleva claramente a rechazar la normalidad.\n\nEl último de los 4 gráficos de residuos (Residuals vs Leverage) señala las observaciones 117 y 62 como los valores con más influencia en los resultados de la regresión. Pero en este último gráfico no presentan un valor grande de la distancia de Cook, que sirve para medir esa influencia. De hecho, no aparecen en el gráfico ni las lineas discontinuas que marcan los límites para considerar un valor grande de la distancia y por lo tanto una posible observación influyente.\n\nLos valores de palanca o apalancamiento (Leverage) se pueden calcular usando la función hatvalues().\n\n\nEn resumen, es claro que hay dos observaciones que pueden influir en los resultados de la regresión, pero también es claro que los problemas observados (heterocedasticidad, falta de normalidad…) no provienen sólo de esos dos datos.\n\nPregunta\nRepita el análisis manteniendo Ozone como variable respuesta/dependiente, cambiando la variable explicativa Solar.R por Wind o Temp. ¿Aprecia algún cambio?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#por-aquí",
    "href": "Cap1-LM.html#por-aquí",
    "title": "1  Modelos lineales",
    "section": "1.5 POR AQUÍ",
    "text": "1.5 POR AQUÍ\n\nAlternativas: Existen distintas funciones/paquetes más o menos sofisticados que realizan este análisis de residuos de distintas maneras. El lector interesado puede explorar: - El paquete ggfortify y su función autoplot. - El paquete performance y su función check_model. - …\n\n\n#Es necesario tener los paquetes instalados previamente\nlibrary(\"ggfortify\")\nautoplot(rls) +\n  theme_minimal()\nlibrary(performance) \nlibrary(see)\ncheck_model(rls)\n\nInterpretación e Inferencia\nSupongamos que fuese todo correcto. El siguiente paso sería el de averiguar la significación de los parámetros, interpretarlos y obtener alguna predicción de interés. Aplicamos la función summary() aplicada a nuestro objeto rls:\n\nsummary(rls)\n\n\nCall:\nlm(formula = Ozone ~ Solar.R, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.292 -21.361  -8.864  16.373 119.136 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 18.59873    6.74790   2.756 0.006856 ** \nSolar.R      0.12717    0.03278   3.880 0.000179 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.33 on 109 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.1213,    Adjusted R-squared:  0.1133 \nF-statistic: 15.05 on 1 and 109 DF,  p-value: 0.0001793\n\n\nCon la salida de summary() obtenemos, además de los coeficientes \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) (columna Estimate), mucha información para su interpretación. En la columna Pr(&gt;|t|) se pueden ver los p-valores de cada parámetro. El del parámetro asociado a la variable Solar.R es muy pequeño, pudiéndose concluir, en caso de que el modelo fuese válido, que la variable Solar.R influye significativamente en la variable Ozone. De la misma manera, el parámetro \\(\\beta_0\\) (Intercept, intersección con el origen) es significativo, lo que indica que el modelo no pasa por el origen. Los valores obtenidos provienen, como se ha visto en la parte teórica, de la estimación del estadístico \\(t\\) que se encuentra en la columna t value, que a su vez se basa en la mencionada estimación de cada parámetro (columna Estimate) y el error de estimación (columna Std. Error). A partir de estos dos valores y la distribución \\(t\\) correspondiente, se pueden obtener los intervalos de confianza “a mano” (paso a paso con R), pero se dispone de la función confint():\n\nconfint(rls)\n\n                 2.5 %     97.5 %\n(Intercept) 5.22460110 31.9728544\nSolar.R     0.06220373  0.1921268\n\n\nSi el modelo fuese válido se pasaría a la interpretación de estos parámetros… Primero, el signo de \\(\\hat{\\beta}_1\\), al ser positivo, indica que conforme aumenta Solar.R también aumenta Ozone. Esta conclusión es clara aunque el modelo no sea válido. Ahora bien, no sólo se obtiene numéricamente esa relación positiva, sino la magnitud de dicha relación. Así, el cambio de una unidad en Solar.R implicaría un cambio medio de aproximadamente 0.1272 unidades en Ozone, con un valor medio de 18.5987 unidades de Ozone en ausencia de radiación solar (¡Ojo! Con los datos disponibles, el mínimo valor de Solar.R es NA, según la tabla del resumen numérico, por lo que estamos hablando de una extrapolación que necesitaría del conocimiento de un experto en la materia para dilucidar su apropiada y/o oportuna interpretación).\nBondad de ajuste\nEn la salida anterior de la función summary() también se pueden observar resultados que ayudan a proporcionar la bondad de ajuste de la regresión simple planteada. El valor que resume la bondad de ajuste es el Multiple R-squared, aunque, como se ha mencionado en la parte teórica, para comparar entre modelos conviene hacerlo con el Adjusted R-squared que arroja un valor de\n\nrls.summ &lt;- summary(rls)  #guardamos el objeto generado con summary()\n#con names() listamos distintos componentes generados con summary() \nnames(rls.summ)           #también se puede consultar con ?summary.lm \n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\"  \"na.action\"    \n\nsummary(rls)$adj.r.squared\n\n[1] 0.1132809\n\n\nEste 11.3% se puede considerar “pobre”. La variable Solar.R explica “pobremente” el Ozone. No obstante, el p-valor global del modelo que se muestra en la última linea de la salida de summary() es significativo, indicando que la recta de regresión es significativa, esto es, que es mejor que proporcionar sólo la media de Ozone para cualquier valor de Solar.R que sería el modelo “básico”.\n\nPregunta\n¿Sabe decir porqué el p-valor de la recta de regresión coincide con el del parámetro asociado a Solar.R?\n\nTambién se puede observar el valor del Residual standard error y sus grados de libertad que se ha mencionado en la parte teórica.\nUn último apunte, en la salida también aparece el mensaje 42 observations deleted due to missingness que indica que no se han considerado aquellas observaciones en las que cualquier variable tiene un NA.\nPredicción\nA pesar de que el modelo no es idóneo, puede que la regresión lineal sirva para el propósito de explicar a grandes rasgos el fenómeno. Pasamos a ilustrar como se obtendrían con R predicciones a partir de la recta estimada:\n\npredict(rls, data.frame(Solar.R = c(10, 100, 300)),\n        interval = \"confidence\")\n\n       fit       lwr      upr\n1 19.87038  7.076164 32.66460\n2 31.31525 23.247138 39.38337\n3 56.74831 47.222078 66.27454\n\npredict(rls, data.frame(Solar.R = c(10, 100, 300)),\n        interval = \"prediction\")\n\n       fit        lwr       upr\n1 19.87038 -43.537910  83.27867\n2 31.31525 -31.310729  93.94124\n3 56.74831  -6.082164 119.57878\n\n\nCon el argumento interval = \"confidence\" obtenemos intervalos de confianza (para valores medios), con interval = \"prediction\" obtenemos intervalos de predicción (para valores individuales) para la predicción de Ozone dados valores de Solar.R. Concretamente para 3 valores: 10, 100 y 300, todos ellos en el rango de valores observados de la variable. La estimación media (fit) coincide en ambos casos, diferenciándose en la amplitud de los intervalos, mucho mayores para los intervalos de predicción, dado que una observación individual puede alejarse mucho más de la media, que una media de varias observaciones para el mismo valor de Solar.R.\n\n1.5.1 lm() múltiple\nModelización\nLa regresión lineal múltiple que se va a abordar aquí, ligeramente distinta a la del libro CDR, directamente en formato de R es:\n\nrlm &lt;- lm(Ozone ~ Solar.R + Wind + Temp + Month, data = airquality) \n## Equivalentemente\n#rlm &lt;- lm(Ozone ~ . - Day, data = airquality)\n\n\nPara no tener que escribir todas las variables, se puede hacer uso de .: lm(y ~ ., data) indica una regresión con y como variable respuesta, y el resto de variables de data como predictores lineales.\nEn el código de arriba además se ha usado el “truco” de quitar una variable con -.\n\nAnálisis de residuos\n\npar(mfrow = c(2, 2), #presenta los gráficos en formato 2x2\n    pty = \"s\",\n    mex = 0.66,\n    cex = 0.75)\nplot(rlm)\n\n\n\n\n\n\n\n\n\nPregunta\n¿Se ha mejorado respecto a la regresión simple?\n\nNótese que ahora, en el 4º gráfico (Residuals vs Leverage) sí que aparece la línea punteada del valor 0.5 de la distancia de Cook, que no llega a alcanzar ningundo de los datos.\nEstimación e interpretación\n\nsummary(rlm)\n\n\nCall:\nlm(formula = Ozone ~ Solar.R + Wind + Temp + Month, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.870 -13.968  -2.671   9.553  97.918 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -58.05384   22.97114  -2.527   0.0130 *  \nSolar.R       0.04960    0.02346   2.114   0.0368 *  \nWind         -3.31651    0.64579  -5.136 1.29e-06 ***\nTemp          1.87087    0.27363   6.837 5.34e-10 ***\nMonth        -2.99163    1.51592  -1.973   0.0510 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.9 on 106 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6199,    Adjusted R-squared:  0.6055 \nF-statistic: 43.21 on 4 and 106 DF,  p-value: &lt; 2.2e-16\n\n\nDe la salida de la función summary() se desprende que de las 4 variables consideradas, la más significativa es Temp, seguida de Wind (ambas con 3 asteriscos), y también es significativa Solar.R, pero a otro nivel (sólo un asterisco, por un valor inferior al 5% pero superior al 1%, cuando en la regresión simple tenía un p-valor más pequeño/significativo, 2 asteriscos). Por último, Month es significativa pero a un nivel del 10% de significación. También el término independiente es significativo.\nSobre la interpretación de los parámetros, se debe tener en cuenta las unidades en las que están recogidas cada una de las variables. Como se ha mencionado, cualquier cambio de escala influirá en dichas estimaciones, y generalmente en sus Std. Error (salvo traslaciones de las variables).\nAdemás, la interpretación de las estimaciones (magnitud) como cambios en la variable respuesta tiene ahora un cambio sustancial. El cambio en la variable respuesta de 1 unidad de una de las variables explicativas está condicionado a que el resto de variables explicativas no cambien, lo que en el contexto de la Economía se denomina ceteris paribus, y que puede que sea imposible de cumplirse. Por ejemplo, el parámetro asociado a Solar.R es, para este modelo de regresión lineal múltiple, 0.0496 menos de la mitad de magnitud que en el caso de regresión simple. Ese sería el cambio medio en las unidades de Ozone para cambios de 1 unidad en Solar.R siempre que Wind, Temp y Month tengan los mismos valores, o, dicho de otro modo, para otro día qu tuviese los mismos valores de Wind, Temp y Month, y cambiase 1 unidad Solar.R. Si es que es factible el cambio de Solar.R con los mismo valores del resto de variables.\nOtra lectura que se debe hacer sobre la interpretación de la magnitud del coeficiente es que, al ser independiente del resto de valores de las variables, es, a su vez, para cualquier combinación de dichos valores. Por ejemplo, para valores de Wind altos, medios o bajos, combinado con valores de Temp altos, medios o bajos, o para cualquier Month (entre 5 y 9, que es el rango observado).\n\nPregunta\n¿Cómo interpretaría el valor negativo del término independiente si se trata de la concentración de ozono en partes por billón?\n\nBondad de ajuste\nLa bondad del ajuste de este modelo, ha mejorado mucho comparada con la regresión simple anterior, pasando ahora a un Adjusted R-squared de 0.606.\nSi nuestro interés sólo fuese predecir valores de Ozone, este modelo podría servirnos.\nPredicción\n¿Cómo se pueden obtener predicciones con un modelo de regresión múltiple? Se deben proporcionar valores a cada una de las variables del modelo:\n\nnuevas.observ &lt;- data.frame(Solar.R = c(110, 110),\n                            Wind    = c(8, 20), \n                            Temp    = c(72, 85), \n                            Month   = c(6, 6))\npredict(rlm, newdata = nuevas.observ,\n        interval = \"confidence\" )\n\n       fit       lwr      upr\n1 37.62288 30.382618 44.86315\n2 22.14613  4.912182 39.38008\n\n\n\n1.5.1.1 Predictores cualitativos\nEl uso de predictores cualitativos debe tratarse de forma diferencial en R. Deben definirse como factor, (factor()). Así, al estimar R los parámetros del modelo donde se incluya dicha variable, generará automáticamente variables ficticias, variables dummys. La función contrasts() permite conocer la codificación que por defecto usa R para dichas variables ficticias (aunque se puede modificar). Para ilustrarlo, redefinimos la variable Month en el conjunto de datos airquality\n\nairquality$Month &lt;- factor (airquality$Month) \ncontrasts(airquality$Month)\n\n  6 7 8 9\n5 0 0 0 0\n6 1 0 0 0\n7 0 1 0 0\n8 0 0 1 0\n9 0 0 0 1\n\n\nLa salida indica que el valor 5 de Month se toma como valor de referencia (en su fila aparecen todo ceros), y se generan 4 variables dummy, una para cada uno de los meses restantes (del 6 al 9).\n\nEn R las variables definidas como factor toma como referencia la primera categoría al ordenar los valores de la variable, bien alfabéticamente (a, b, c…) o bien numéricamente de menor a mayor, si no se ha especificado otro orden.\n\nAjustamos de nuevo el modelo de regresión múltiple.\n\nrlm.cualit &lt;- lm(Ozone ~ . - Day, \n                 data = airquality)\nsummary(rlm.cualit)\n\n\nCall:\nlm(formula = Ozone ~ . - Day, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.344 -13.495  -3.165  10.399  92.689 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -74.23481   26.10184  -2.844  0.00537 ** \nSolar.R       0.05222    0.02367   2.206  0.02957 *  \nWind         -3.10872    0.66009  -4.710 7.78e-06 ***\nTemp          1.87511    0.34073   5.503 2.74e-07 ***\nMonth6      -14.75895    9.12269  -1.618  0.10876    \nMonth7       -8.74861    7.82906  -1.117  0.26640    \nMonth8       -4.19654    8.14693  -0.515  0.60758    \nMonth9      -15.96728    6.65561  -2.399  0.01823 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.72 on 103 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6369,    Adjusted R-squared:  0.6122 \nF-statistic: 25.81 on 7 and 103 DF,  p-value: &lt; 2.2e-16\n\n\nSe puede observar que se muestran parámetros para las 4 variables dummy de los 5 valores de Month. Como se ha indicado, el primer valor de la variable (el 5) lo toma como referencia (se podría cambiar si se quiere otro mes de referencia) y genera 4 variables dicotómicas que reflejan el cambio de la categoría de referencia a cada una de ellas. Matemáticamente el modelo que se estima es: \\[\\begin{align*}\nOzone = &\\beta_0 + \\beta_1 Solar.R + \\beta_2 Wind + \\beta_3 Temp + \\\\\n&\\beta_6 Month6 +  \\beta_7 Month7 +  \\beta_8 Month8 +  \\beta_9 Month9 + \\epsilon  \n\\end{align*}\\] Así, se entiende ahora mejor la visualización de contrasts() obtenida anteriormente. Cuando Month6, Month7… toman todos el valor 0 la estimación que se obtiene es del mes de mayo. Si la variable dummy Month6 toma el valor 1, y las otras 3 toman el valor 0, se obtiene la diferencia media de Ozone respecto al mes de mayo, ceteris paribus. Por lo tanto, que el parámetro asociado a Month6 sea negativo implica que, para cualquier combinación fija de valores del resto de variables, en junio disminuye la media de Ozone respecto a mayo (aunque justo este parámetro/cambio no es significativo!), etc.\n\nPregunta\n¿Como se interpreta ahora el valor del Intercept?\n¿Y cómo se explica el valor negativo de Month6 a la vista del resumen gráfico que se obtuvo con pairs()?\n\n\n\n\n1.5.2 Caso práctico: Boston\nEl segundo de los casos prácticos de modelización lineal que se va a tratar en profundidad utiliza los datos Boston, recogidos en varios paquetes, por ejemplo, en ISLR2 asociado con el libro: https://www.statlearning.com. Los datos recogen valores relacionados con viviendas para 506 distritos censales de Boston. El estudio de la modelización lineal se puede encontrar en: https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch3-linreg-lab.html y el Rscript asociado en: https://hastie.su.domains/ISLR2/Labs/R_Labs/Ch3-linreg-lab.R. De nuevo, aquí se presentarán ejemplos ligeramente distintos y con algo más de profundidad.\n\n1.5.2.1 Exploración de los datos\n\nlibrary(ISLR2)\n#?Boston #Para obtener más información sobre las variables, unidades, etc. \nhead(Boston)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94 33.4\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33 36.2\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21 28.7\n\n\nAhora no parece haber NAs. En el resumen numérico veremos que no hay. No obstante, se puede comprobar con:\n\nsum(is.na(Boston))\n\n[1] 0\n\n\nEstructura y resúmenes\n\nstr(Boston)\n\n'data.frame':   506 obs. of  13 variables:\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n\nsummary(Boston)\n\n      crim                zn             indus            chas        \n Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000  \n 1st Qu.: 0.08205   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000  \n Median : 0.25651   Median :  0.00   Median : 9.69   Median :0.00000  \n Mean   : 3.61352   Mean   : 11.36   Mean   :11.14   Mean   :0.06917  \n 3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000  \n Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000  \n      nox               rm             age              dis        \n Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130  \n 1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02   1st Qu.: 2.100  \n Median :0.5380   Median :6.208   Median : 77.50   Median : 3.207  \n Mean   :0.5547   Mean   :6.285   Mean   : 68.57   Mean   : 3.795  \n 3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08   3rd Qu.: 5.188  \n Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127  \n      rad              tax           ptratio          lstat      \n Min.   : 1.000   Min.   :187.0   Min.   :12.60   Min.   : 1.73  \n 1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40   1st Qu.: 6.95  \n Median : 5.000   Median :330.0   Median :19.05   Median :11.36  \n Mean   : 9.549   Mean   :408.2   Mean   :18.46   Mean   :12.65  \n 3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20   3rd Qu.:16.95  \n Max.   :24.000   Max.   :711.0   Max.   :22.00   Max.   :37.97  \n      medv      \n Min.   : 5.00  \n 1st Qu.:17.02  \n Median :21.20  \n Mean   :22.53  \n 3rd Qu.:25.00  \n Max.   :50.00  \n\n\nA la vista de la estructura y el resumen numérico, se pueden considerar continuas casi todas las variables (internamente están definidas en R como double o int, para manejar números reales con formato de coma flotante de doble precisión que requieren más espacio en memoria o enteros, respectivamente). Llama la atención el resumen de las variables zn, chas y rad. La variable zn está definida internamente como double porque contiene algunos valores con decimales (y mirando su definición es una proporción). Ahora bien, su mediana es 0, por lo que al menos la mitad de los 506 valores son 0, de hecho el 0 aparece 372 veces. Esto, sin duda tendrá un impacto al considerarlo en los modelos. Por su parte las variables chas y rad tienen formato int, pero acudiendo a su definición, chas es dicotómica (lo que también da sentido que su mediana sea 0 y su media se 0.0692), mientras que rad es un índice, que toma valores entre 1 y 24. Dejaremos zn y rad con valores numéricos, pero, para su apropiado manejo en los modelos, es oportuno convertir chas en variable factor de R:\n\nBoston$chas &lt;- factor(Boston$chas)\ncontrasts(Boston$chas)\n\n  1\n0 0\n1 1\n\n\nComo se puede ver con la función contrasts() en los casos de variable dicotómica sólo se genera una variable dummy, quedando por defecto el valor 0 como valor de referencia. Rizando el rizo, vamos a cambiar la definición del valor de referencia para luego observar su impacto en la modelización.\n\nBoston$chas &lt;- factor(Boston$chas, \n                      levels = c(1, 0))\ncontrasts(Boston$chas)\n\n  0\n1 0\n0 1\n\n\n\npairs(Boston, upper.panel = NULL,\n      lower.panel = panel.smooth)\n\n\n\n\n\n\n\n\nA la vista del resumen gráfico, diagramas de dispersión de pares de variables, casi todas las relaciones entre pares de variables parecen no lineales. Como se puede apreciar, la dicotomía de la variable chas genera diagramas de dispersión muy distintos al resto. Y las variables rad y tax llaman la atención por presentar dos grupos de valores alejados, especialmente rad. También, la variable nox presenta dos grupos de valores pero con menor separación entre grupos.\n\n\n1.5.2.2 lm() múltiple\nModelización y estimación\nLa variable de interés a modelizar es medv (“median value”: valor mediano de las casas ocupadas por sus propietarios, en $1000s). En el libro ISLR2 utilizan las otras 12 variables explicativas como predictores. Aquí consideraremos sólo unas cuantas (ojo con lo que recogen cada una de las variables… ¿age?):\n\nrlm.Boston &lt;- lm(medv ~ lstat + rm + age + tax + chas, #chas definida como factor\n                 data = Boston)\nsummary(rlm.Boston)\n\n\nCall:\nlm(formula = medv ~ lstat + rm + age + tax + chas, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.358  -3.345  -1.124   1.930  30.073 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.323355   3.270507   1.322    0.187    \nlstat       -0.588092   0.055117 -10.670  &lt; 2e-16 ***\nrm           4.954530   0.440829  11.239  &lt; 2e-16 ***\nage          0.014624   0.011379   1.285    0.199    \ntax         -0.007009   0.001756  -3.990 7.58e-05 ***\nchas0       -3.898106   0.955381  -4.080 5.24e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.372 on 500 degrees of freedom\nMultiple R-squared:  0.6622,    Adjusted R-squared:  0.6588 \nF-statistic:   196 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\n\nSe han unificado la modelización y estimación, como se hace generalmente. Posteriormente se realizará el análisis de residuos que permitirá dar validez al modelo considerado.\nEn este caso, los parámetros asociados a las variables salen significativos, excepto uno. Este hecho, conduce a eliminar dicha variable del modelo, para obtener uno con sólo variables influyentes sobre medv.\nEl impacto de que algunos parámetros no sean significativos también se puede apreciar en sus intervalos de confianza… Los no significativos incluyen el 0.\n\nconfint(rlm.Boston)\n\n                   2.5 %       97.5 %\n(Intercept) -2.102274869 10.748985289\nlstat       -0.696380539 -0.479802930\nrm           4.088424014  5.820635797\nage         -0.007732991  0.036981489\ntax         -0.010459310 -0.003557925\nchas0       -5.775162051 -2.021050701\n\n\n\nPregunta\n¿Interpretación de los coeficientes?… Recuerda: ceteris paribus.\n\n\n\n1.5.2.3 Quitando predictores\nComo ilustración del proceso iterativo (manual) de modelización, pasamos a estimar un nuevo modelo en el que eliminamos la variable no significativa que se obtuvo en el modelo anterior rlm.Boston, esto es quitando age:\n\nrlm.BostonModif &lt;- lm(medv ~ lstat + rm + tax + chas, data = Boston)\n#Alternativamente, usando la función `update()`.\n#rlm.BostonModif &lt;- update(rlm.Boston, ~ . - age)\nsummary(rlm.BostonModif)\n\n\nCall:\nlm(formula = medv ~ lstat + rm + tax + chas, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.484  -3.400  -1.158   1.909  30.849 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.154797   3.270001   1.271 0.204468    \nlstat       -0.554277   0.048462 -11.437  &lt; 2e-16 ***\nrm           5.060589   0.433317  11.679  &lt; 2e-16 ***\ntax         -0.006412   0.001695  -3.783 0.000174 ***\nchas0       -4.076908   0.945811  -4.310 1.96e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.376 on 501 degrees of freedom\nMultiple R-squared:  0.6611,    Adjusted R-squared:  0.6584 \nF-statistic: 244.3 on 4 and 501 DF,  p-value: &lt; 2.2e-16\n\n\nAhora el modelo sale con todos los parámetros significativos, con un Adjusted R-squared ligeramente peor. No obstante el \\(R^2\\) ajustado es razonablemente bueno si el objetivo es predecir valores de Ozone.\n\nPregunta\n¿Cómo quitar varios predictores simultáneamente?\n¿Cómo hacer este proceso automáticamente? Véase el siguiente capítulo…\n\n\nPregunta\nObtenga este último modelo con la variable chas original (sin convertirla en factor) ¿Qué diferencias observa en la estimación del parámetro asociado a dicha variable?\n\nAnálisis de residuos\nValoremos la adecuación del último modelo examinando sus residuos.\n\npar(mfrow = c(2, 2), #presenta los gráficos en formato 2x2\n    pty = \"s\",\n    mex = 0.66,\n    cex = 0.75)\nplot(rlm.BostonModif)\n\n\n\n\n\n\n\n\n\nLos gráficos 1 y 3 (Residuals vs Fitted y Scale-Location) reflejan heterocedasticidad y no linealidad.\nEn el segundo gráfico (Q-Q Residuals), se observa una clara desviación de la normalidad (marcada por la linea de puntos). Veamos el histograma:\n\n\nhist(residuals(rlm.BostonModif))\n\n\n\n\n\n\n\n\nEl histograma refleja asimetría por lo que claramente rechazamos la normalidad.\n\nPor último en los 4 gráficos de residuos quedan señaladas las observaciones 369 y 373, junto con la 372 que aparece en 3 de los gráficos. Habría que considerar el modelo sin ellas para comprobar su influencia, pero, de nuevo, parece claro que los problemas comentados (heterocedasticidad, falta de normalidad…) no provienen sólo de esas observaciones.\n\n\n\n1.5.2.4 Extensiones de la regresión múltiple\nEl modelo lineal de regresión lineal múltiple permite considerar “constructos” de variables, es decir, considerar “productos” de dos variables, términos no lineales, etc. El tratamiento de esos casos con R es similar a lo visto hasta ahora, con el inconveniente de enfrentarse a su interpretación. Así, entre los “constructos” más habituales se encuentran:\n\nTérminos de interacción: La sintaxis lstat:age le dice a R que incluya un término de interacción entre lstat y age. La sintaxis lstat * age incluye como predictores simultáneamente lstat, age y la interacción lstat:age, es una abreviatura de lstat + age + lstat:age.\n\n\nsummary(lm(medv ~ lstat * age, data = Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\n\nPregunta\n¿Cómo se interpeta que la interacción salga significativa?\n\n\nTransformaciones polinómicas de los predictores: La sintaxis I(lstat^2) introduce en el modelo el predictor cuadrático de lstat.\n\n\nrlm.Boston2 &lt;- lm(medv ~ lstat + I(lstat^2), data = Boston)\nsummary(rlm.Boston2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nCargando paquete requerido: carData\n\nvif(rlm.Boston2)\n\n     lstat I(lstat^2) \n  12.93657   12.93657 \n\n\nEl p-valor cercano a cero asociado con el término cuadrático sugiere que, su inclusión, conduce a un modelo mejorado. Pero, obviamente, introduce un problema claro de colinealidad, que se puede obviar si el interés en el modelo es por su valor predictivo en lugar de explicativo.\nObviamente se pueden incluir términos cúbicos, etc.: I(variable^3)…, varios a la vez, etc. Pero para un ajuste polinómico, conviene acudir a la función poly(), que, por defecto, ortogonaliza los predictores.\n\nrlm.Boston6 &lt;- lm(medv ~ poly(lstat, 6), data = Boston)\nsummary(rlm.Boston6)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 6), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.7317  -3.1571  -0.6941   2.0756  26.8994 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2317  97.252  &lt; 2e-16 ***\npoly(lstat, 6)1 -152.4595     5.2119 -29.252  &lt; 2e-16 ***\npoly(lstat, 6)2   64.2272     5.2119  12.323  &lt; 2e-16 ***\npoly(lstat, 6)3  -27.0511     5.2119  -5.190 3.06e-07 ***\npoly(lstat, 6)4   25.4517     5.2119   4.883 1.41e-06 ***\npoly(lstat, 6)5  -19.2524     5.2119  -3.694 0.000245 ***\npoly(lstat, 6)6    6.5088     5.2119   1.249 0.212313    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.212 on 499 degrees of freedom\nMultiple R-squared:  0.6827,    Adjusted R-squared:  0.6789 \nF-statistic: 178.9 on 6 and 499 DF,  p-value: &lt; 2.2e-16\n\n\nLa interpretación de esta salida sugiere que el modelo polinómico de orden 5 conduce al mejor ajuste del modelo con un polinomio de la variable lstat.\n\nTransformaciones logarítmicas: también se pueden aplicar otro tipo de transformaciones sobre los predictores, siempre que sean apropiadas y oportunas.\n\n\nsummary(lm(medv ~ log(rm), data = Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n1.5.2.5 Comparación de modelos mediante anova()\nCon la función anova() se pueden comparar modelos “jerárquicos”. En los casos expuestos anteriormente, se puede cuantificar hasta qué punto el ajuste cuadrático es superior al ajuste lineal, o que el ajuste de orden 6 no es superior al de orden 5.\n\nrlm.Boston1 &lt;- lm(medv ~ lstat, data = Boston)\nanova(rlm.Boston1, rlm.Boston2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nrlm.Boston5 &lt;- lm(medv ~ poly(lstat, 5), data = Boston)\nanova(rlm.Boston5, rlm.Boston6)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ poly(lstat, 5)\nModel 2: medv ~ poly(lstat, 6)\n  Res.Df   RSS Df Sum of Sq      F Pr(&gt;F)\n1    500 13597                           \n2    499 13555  1    42.364 1.5596 0.2123\n\n\nEn la primera tabla ANOVA se puede ver, en la etiquetada como línea 2, el estadístico F, que recoge el ratio de variabilidad explicada por el Model 2 frente al Model 1. Su elevado valor, junto con un p-valor prácticamente nulo, llevan a considerar el modelo cuadrático muy superior al simple. Al lector observador no le debe sorprender este resultado a la vista del diagrama de dispersión entre medv y lstat, que presenta una clara no linealidad.\nPor su parte, la segunda tabla ANOVA muestra un resultado bien distinto, con un discreto valor del estadístico F que conduce a rechazar que el polinomio de orden 6 explique mejor la respuesta que el de orden 5. Además se puede comprobar que este p-valor coincide con el p-valor asociado al término de orden 6 estimado por la regresión.\n\nPreguna ¿Qué conclusiones obtiene al comparar los modelos rlm.Boston y rlm.BostonModif?\n\n\n\n1.5.2.6 Colinealidad: vif()\nLa colinealidad es un problema que puede aparecer en regresión múltiple, cuando una variable explicativa pueda estar altamente correlacionada con otra u otras explicativa(s). Aquellas variables que presentan colinealidad producen una inflación en la estimación de la varianza, y, con ello, una peor precisión para detectar significatividad. El cálculo del factor de inflación de la varianza (VIF) de cada parámetro se puede obtener con la función vif(), del paquete car:\n\nlibrary(car)\nvif(rlm.Boston)\n\n   lstat       rm      age      tax     chas \n2.710812 1.678750 1.795414 1.533240 1.030405 \n\n\nPara estos datos, la mayoría de los VIF son bajos o moderados.\n\nRegla de decisión práctica: No preocuparse de la multicolinealidad si vif &lt; 5, incluso vif &lt; 10.\n\n\nPregunta\n¿Cómo salen los VIF de la regresión múltiple con los datos airquality?\n\nVeamos un ejemplo claro de colinealidad:\n\nvif(rlm.Boston2)\n\n     lstat I(lstat^2) \n  12.93657   12.93657 \n\n\n\nPregunta\n¿Qué interpretación tiene que los VIF sean iguales?\n\nVamos a generar un caso con alta colinealidad:\n\nset.seed(pi)\nBoston$sum &lt;- Boston$lstat/2 + Boston$rm + rnorm(length(Boston$lstat))\nrlm.BostonColine &lt;- lm(medv ~ lstat + rm + tax + chas + sum, \n                       data = Boston)\nvif(rlm.BostonColine)\n\n    lstat        rm       tax      chas       sum \n13.189150  1.988231  1.430875  1.008564 10.209699 \n\n\n\nPregunta\n¿Qué interpretación tienen ahora los VIF?\n\n\n\nBibliografía\n\n\n\n\nCasero-Alonso, Víctor, y María Durbán. 2024. «Modelización lineal». En Fundamentos de Ciencia de Datos con R. McGraw Hill. https://cdr-book.github.io/cap-lm.html.\n\n\nFaraway, Julian J. 2004. Linear Models with R. Chapman &amp; Hall/CRC.\n\n\nFernández-Avilés, Gema, y José-María Montero. 2024. Fundamentos de Ciencia de Datos con R. McGraw Hill. https://cdr-book.github.io/index.html.\n\n\nPeña, Daniel. 2002. Regresión y diseño de experimentos. Alianza Editorial.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html",
    "href": "Cap2-DoE.html",
    "title": "2  Diseño de experimentos",
    "section": "",
    "text": "2.1 Nociones básicas\nEn este tema se estudian los denominados modelos de diseño de experimentos, que se incluyen entre los modelos lineales. En ellos, como ya se ha indicado, la denominada variable respuesta debe ser cuantitativa/numérica continua. Por el contrario, la(s) variable(s) explicativa(s) debe(n) ser categórica(s), y en este contexto se denomina(n) factor(es), y se supone que influye(n) sobre la variable respuesta.\nUna muy buena referencia para este tema es el libro “Regresión y diseño de experimentos” de Daniel Peña, Peña (2002), concretamente los capítulos 2 a 4. Otro libro de referencia “clásico” es “Design and Analysis of Experiments” de Douglas Montgomery, Montgomery (2008) capítulos …. Otro libro de referencia con un buen contenido matemático y práctico es “Linear Models with R” de Julian Faraway, Faraway (2004) capítulos ….\nComo en el capítulo de modelos lineales de regresión, cabe distinguir entre modelos de un sólo factor, los más sencillos, y dos o más factores, en los que no sólo se tiene en cuenta el efecto de cada factor sobre la respuesta, sino las posibles interacciones entre los mismos. Citamos algunos casos, que desarrollaremos: - Diseños unifactoriales, con un único factor con dos o más niveles (tratamientos). - Diseños bifactoriales: - con o sin réplicas, y con o sin interacción (diseños por bloques). - Diseños de más de dos factores. - Diseños factoriales a 2 niveles (2^k).\nPor extensión, no entraremos en diseños “con nombre propio” como los cuadrados latinos y los grecolatinos, los diseños Plackett-Burman, los Box-Behnken, ni con los modelos con efectos aleatorios, que se desarrollan en el Capítulo 3 de Peña (2002) ¿también los diseños PB y BB?. Tampoco describiremos los diseños factoriales fraccionados (Peña (2002), Capítulo 4), ni los diseños de superficie de respuesta (Peña (2002), Capítulo 5).\n¿Qué es un experimento?\nAcudiendo a la definición de la RAE, es una “operación destinada a descubrir, comprobar o demostrar determinados fenómenos o principios científicos. Mendel realizó importantes experimentos genéticos con guisantes.”.\nEnlazándolo con la frase inicial del Capítulo, un experimento consiste en observar/medir el valor de la variable respuesta (cuantitativa), para distintos valores (categorías) del factor o factores. Por ejemplo, - medir el peso de jamones (kg), para distintos valores del factor “genética”, y/o del factor “estación”, etc. - medir las emisiones directas de CO2 de un motor (g CO2/s), para distintos valores del factor “presión de admisión”, la “temperatura de admisión”, el “% de mezcla de metanol”, etc. - medir el tiempo de vuelo de helicópteros de papel (segundos), para distintos valores del factor “longitud de las alas”, “anchura de las alas”, “gramaje del papel”, “clip”, etc.\n¿Qué significa diseñar un experimento?\nDiseñar un experimento es determinar las condiciones experimentales (valores de las variables controlables) para las que se obtendrán las observaciones de la variable respuesta (que se espera que presenten cambios según las condiciones experimentales), y que proporcionarán información relevante (o no) sobre el fenómeno estudiado para indentificar las razones del cambio. Hay que tener en cuenta que, durante el experimento pueden afectar otras variables no controlables por el experimentador (véanse más adelante los principios básicos del DoE).\nSiguiendo con el ejemplo,\n¿Por qué es importante diseñar un experimento?\nMuy resumidamente, por “costes”… un buen diseño de experimento permite ahorrar tiempo, recursos y riesgos (…costes en el sentido amplio de la palabra), y conduce a un análisis adecuado/correcto. Además, responde al principio de “pensar antes de actuar”.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html#nociones-básicas",
    "href": "Cap2-DoE.html#nociones-básicas",
    "title": "2  Diseño de experimentos",
    "section": "",
    "text": "determinar varias genéticas (Duroc, Traxx, Magnus … ) para comparar el peso del jamón entre ellas, o la combinación genéticas con las estaciones (invierno, primavera, verano, otoño), etc. para encontrar las razones del cambio en el peso del jamón.\ndeterminar los niveles de presión de admisión (1 bar, 1.5 bares…), temperatura de admisión (35ºC, 65ºC …), % de mezcla de metanol (0%, 10%, 20% … de sustitución -energía-), etc. que se van combinar para estudiar su efecto directo y/o conjunto sobre las emisiones directas de CO2 del motor.\ndeterminar la longitud de las alas (10, 15, 20 cm …), la anchura de las alas (4, 6, 8 cm …), el gramaje del papel (80 g/m2, 120 g/m2 …), el uso de clip (con clip, sin clip) para estudiar su efecto sobre el tiempo de vuelo de los helicópteros de papel.\n\n\nNota: En el caso del diseño óptimo de experimentos, se determinarán los mejores valores de la variable (o variables) explicativa(s) \\(X\\) donde observar el valor de la respuesta \\(Y\\). Por ejemplo, en el caso de la regresión lineal simple o recta de regresión, los mejores “lugares” donde observar son los dos valores extremos donde esté definida la variable \\(X\\), dado que una recta queda determinada por 2 “puntos”.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html#principios-básicos-del-doe",
    "href": "Cap2-DoE.html#principios-básicos-del-doe",
    "title": "2  Diseño de experimentos",
    "section": "2.2 Principios básicos del DoE",
    "text": "2.2 Principios básicos del DoE\n\n2.2.1 Aleatorización\nLa aletorización es la piedra angular de los métodos estadísticos. Por aleatorización se entiende tanto la asignación del material experimental como el orden en que se realizan los experimentos. Uno de los requisitos estadísticos es que las observaciones (o los errores) sean variables aleatorias con distribuciones independientes.\nLa aleatorización también ayuda a promediar los efectos de los factores que no conocemos o que no están bajo control.\n\n\n2.2.2 Réplicas\nUna réplica es la repetición de un experimento bajo las mismas condiciones. Permite estimar la variabilidad de los resultados y proporciona una medida de la precisión del experimento.\n\n\n2.2.3 Bloqueo\nEl bloqueo se utiliza para controlar la variabilidad de las respuestas producida por factores que no son de interés, pero se pueden controlar. Consiste en dividir el experimento en bloques homogéneos, de forma que las diferencias entre los bloques sean menores que las diferencias entre los tratamientos. Esto permite reducir la variabilidad residual y aumentar la precisión del experimento.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html#procedimiento-estadístico-de-un-experimento",
    "href": "Cap2-DoE.html#procedimiento-estadístico-de-un-experimento",
    "title": "2  Diseño de experimentos",
    "section": "2.3 Procedimiento estadístico de un experimento",
    "text": "2.3 Procedimiento estadístico de un experimento\nSiguiendo el apartado “Directrices para el diseño de experimentos” de Peña (2002), el procedimiento estadístico de un experimento se puede resumir en los siguientes pasos:\n\nPlanteamiento del problema\n\n\nDefinicion de las variables involucradas. Variable respuesta y factores controlables/explicativos, no controlables, ruido (niveles, rangos)\nSelección del modelo\n\n\nElección del diseño experimental\nRealización del experimento, toma de observaciones de la variable respuesta\nAnálisis estadístico de los datos obtenidos\nConclusiones y recomendaciones.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html#modelo-anova",
    "href": "Cap2-DoE.html#modelo-anova",
    "title": "2  Diseño de experimentos",
    "section": "2.4 Modelo ANOVA",
    "text": "2.4 Modelo ANOVA\n\nNota: Libro de Peña y muchos otros: ANOVA como “ampliación” de comparación de dos muestras de ¿Estadística 1?\n\nLa herramienta principal para el análisis de los modelos de diseño de experimentos es el modelo de ANálisis de la VArianza, modelo ANOVA. Es un modelo lineal que permite modelizar (de nuevo de forma aproximada) el comportamiento de una variable respuesta cuantitativa, \\(Y\\), mediante un función lineal de una o varias variables explicativas categóricas/factores, \\(X_1, \\ldots, X_k\\).$.\nEl caso más simple es el de un único factor con 2 o más categorías/niveles distintos, que se denomina ANOVA de un factor (ANOVA univariante, ANOVA de una vía o One-way ANOVA). En este caso la formulación del modelo lineal es ligeramente distinta (pero equivalente) al del modelo de regresión lineal: \\[Y_{ij} = \\mu + \\tau_i +\\epsilon_{ij}, \\qquad i=1, \\ldots, l \\text{ (niveles); } j=1,\\ldots, n_i \\text{ (réplicas)}\\] donde \\(Y_{ij}\\) son las observaciones/respuestas/mediciones obtenidas como resultado del experimento (variable respuesta), \\(\\mu\\) es la media global de dichas observaciones, común a todos los niveles, \\(\\tau_i\\) es el efecto del nivel \\(i\\) del factor (efecto del tratamiento i-ésimo: parámetro que mide su influencia en la respuesta) y \\(\\epsilon_{ij}\\) es el error aleatorio asociado a la observación \\(j\\) del nivel \\(i\\) del factor.\n\nLa analogía con el modelo de regresión es directa, \\(\\mu\\) es el equivalente a \\(\\beta_0\\), y \\(\\tau_i\\) es el equivalente a \\(\\beta_i\\) de una variable categórica dummy.\n\nLos supuestos habituales sobre el error son los mismos que en el modelo de regresión: media cero, \\(E[\\epsilon]=0\\) , varianza constante, \\(var(\\epsilon)=\\sigma^2I\\), y seguir una distribución normal.\n\nNota: 1. Cuando hay un único factor, con dos niveles distintos, el ANOVA coincide exactamente con la prueba t-de Student.\n2. La generalización a varias variables categóricas (factores) es directa, y se denomina ANOVA de dos o más factores.\n\nEl ANOVA compara la respuesta media de los distintos niveles que conforman los factores, pero lo realiza con la información proporcionada por la variabilidad, de ahí su nombre. Dicho análisis se deriva de la partición de la variabilidad total en los dos componentes ya mencionados en el capítulo de regresión y que aquí son: la variabilidad debida a los efectos de los factores (tratamiento) y la variabilidad debida a los residuos. \\[SC_{total} = SC_{factor} + SC_{residual},\\]\ndonde \\[SC_{total}=\\sum_{i=1}^l \\sum_{j=1}^{n_i} (y_{ij}-\\bar{y})^2\\] es la suma de cuadradados entre las observaciones y el gran promedio,\n\\[SC_{factor}= \\sum_{i=1}^l n_i(\\bar{y}_{i.}-\\bar{y})^2\\] es la suma de cuadrados entre los promedios de los tratamientos y el gran promedio, y\n\\[SC_{residual}=\\sum_{i=1}^l \\sum_{j=1}^{n_i} (y_{ij}-\\bar{y}_{i.})^2\\]\nes la suma de cuadrados de las diferencias de las observaciones dentro de los tratamientos y el promedio de los tratamientos.\nEstas sumas de cuadrados tienen \\(N-1\\), \\(l-1\\) y \\(N-l\\) grados de libertad, respectivamente, siendo \\(N\\) el número total de observaciones y \\(n_i\\) el número de observaciones en el tratamiento i-ésimo. Y por tanto:\n\\[SCM_{Total}=\\frac{SC_{Total}}{N-1}, \\quad\nSCM_{factor}=\\frac{SC_{factor}}{l-1} \\quad \\text{  y  } \\quad\nSCM_{residual}=\\frac{SC_{residual}}{N-l}\\]\n\n2.4.1 Estimación de \\(\\sigma^2\\)\nUn tratamiento matemático de las sumas de cuadrados medios nos da que los valores medios/esperados de las sumas de cuadrados medios son: \\[E(SCM_{residual})=\\sigma^2\\]\n\\[E(SCM_{factor})=\\sigma^2 + \\frac{n\\sum^l \\tau_i^2}{l-1}\\]\nY podemos decir que \\(SCM_{residual}\\) estima \\(\\sigma^2\\) y, si no hay diferencias en las medias de los \\(l\\) tratamientos (\\(\\tau_i=0\\)), \\(SCM_{factor}\\) también.\nEn cambio, si hay diferencias, \\(SCM_{factor}&gt;SCM_{residual}\\).\n\n\n2.4.2 Contraste de hipótesis ANOVA\nEn este caso, el contraste de hipótesis que estamos realizando se enuncia así:\n\\[\\begin{eqnarray}\nH_0 &:& \\tau_1 = \\tau_2 = \\tau_3 = \\tau_4 = \\tau_5 = 0 \\\\\nH_1 &:& \\text{Al menos uno de los } \\tau_i \\text{ es distinto de cero.}\n\\end{eqnarray}\\]\nO lo que es equivalente:\n\\[\\begin{eqnarray}\nH_0 &:& \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5 \\\\\nH_1 &:& \\text{Al menos una de las } \\mu_i \\text{ es distinta.}\n\\end{eqnarray}\\]\n\n\n2.4.3 Comprobación de los supuestos\nComo en regresión, los supuestos se comprueban a través de los residuos. En el caso práctico con R se comprobarán visualmente mediante la función plot(), o través de contrastes: - Shapiro-Wilk (normalidad), - Barlett o Levene (homogeneidad de varianzas): \\[\\begin{eqnarray}\nH_0 &:& \\sigma_1 = \\ldots = \\sigma_l \\\\\nH_1 &:& \\text{Al menos una de las } \\sigma_i \\text{ es distinta de las demás,}\n\\end{eqnarray}\\] - Durbin-Watson (independencia de los residuos).\n\n\n2.4.4 Comparaciones múltiples\nLos contrastes de comparaciones múltiples, post-hoc o a posteriori son contrastes que se muestran relevantes una vez que hemos ajustado nuestro modelo. Se usan para comparar medias de observaciones pertenecientes al mismo nivel, con otras medias de observaciones pertenecientes a otro nivel.\nLa hipótesis nula en estos casos es siempre la igualdad de las medias. \\[\\begin{eqnarray}\nH_0 &:& \\mu_i = \\mu_j \\\\\nH_1 &:& \\mu_i \\neq \\mu_j\n\\end{eqnarray}\\]\nHay que ser muy precavidos porque las comparaciones múltiples inflan el error de tipo I (Falsar \\(H_0\\) cuando en realidad es cierta), porque es probable que en un porcentaje elevado de las comparaciones, las diferencias serán el resultado del error aleatorio. Por ello, es necesario realizar una corrección del nivel de significación \\(\\alpha\\) para que el error tipo I global no sea demasiado grande.\n\nFalsar (RAE): En la ciencia, desmentir una hipótesis o una teoría mediante pruebas o experimentos.\n\nDe entre los distintos métodos de comparaciones múltiples, los más utilizados son:\n\nBonferroni: Un enfoque para asegurarse que el nivel de confianza simultáneo no sea demasiado pequeño es sustituir la significación \\(\\alpha\\) por \\(\\alpha/r\\) donde \\(r\\) es el número de comparaciones. Este método está pensado para realizar un número de comparaciones fijado a priori.\nScheffé: En el método de Scheffé el error tipo I (rechazar \\(H_0\\) cuando es cierta), es a lo sumo \\(\\alpha\\) para cualquiera de las comparaciones posibles. Esto es útil cuando queremos realizar todas y cada una de las posibles comparaciones.\nTukey: Tukey propuso un procedimiento para probar hipótesis para las que el nivel de significación global es exactamente \\(\\alpha\\) cuando los tamaños de las muestras son iguales y es a lo sumo \\(\\alpha\\) cuando los tamaños de las muestras no son iguales. Este procedimiento está pensado cuando resulta de interés realizar todas las comparaciones posibles.\nDiferencia Significativa Mínima de Fisher (LSD): Basado en el estadístico F, en definitiva consiste en realizar todas las pruebas t posibles pero realizando una corrección en el cálculo de la desviación típica. Este método calcula una desviación típica combinada \\(\\sqrt{SCM_{residual}(\\frac{1}{n_i}+\\frac{1}{n_j})}\\) en lugar de usar la de los dos grupos que se comparan. Esto aumenta la potencia del contraste pero no realiza una corrección por comparaciones múltiples.\nLa Least Significative Difference LSD consiste en: \\[LSD=t_{\\alpha/2,N-a}\\sqrt{SCM_{residual}(\\frac{1}{n_i}+\\frac{1}{n_j})}.\\] Y el estadístico de contraste: \\[t_0=\\frac{{\\bar{y}_{i.}-\\bar{y}_{j.}}}{\\sqrt{SCM_{residual}(\\frac{1}{n_i}+\\frac{1}{n_j})}}.\\]\nRango Múltiple Duncan: El test de Duncan parece ser especialmente conservador respecto al error de Tipo II con el riesgo de aumentar el error de Tipo I:\n\n\n\n2.4.5 ¿Qué método de comparación por pares debe usarse?\nDesafortunadamente, no hay una respuesta precisa para esa pregunta. Existen estudios de simulación que dan como el más eficaz al método LSD de Fisher y el de Rango Múltiple de Duncan. Cada método tiene sus ventajas e inconvenientes, se puede consultar una comparativa.\nAdemás hay otros métodos de comparaciones múltiples y sus correspondientes paquetes en R.\nUn caso particular son las comparaciones con un control: En muchos experimentos, uno de los tratamientos es un control o referencia. Solo interesa realizar \\(a-1\\) comparaciones de cada grupo con el control. El test de Dunnett es una modificación de la prueba t.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html#diseños-bifactoriales-diseño-por-bloques",
    "href": "Cap2-DoE.html#diseños-bifactoriales-diseño-por-bloques",
    "title": "2  Diseño de experimentos",
    "section": "2.5 Diseños bifactoriales: diseño por bloques",
    "text": "2.5 Diseños bifactoriales: diseño por bloques\nEl caso más sencillo de diseño de experimentos con 2 factores es el diseño por bloques, de aplicación cuando de los 2 factores, uno es un factor perturbador (sobre el que no existe un interés específico pero su variabilidad puede afectar a la respuesta).\nEn ocasiones, el factor perturbador es desconocido y no controlable. En estos casos la mejor solución es aplicar el principio de aleatorización. Sin embargo, si la fuente de variabilidad es conocida y controlable podemos utilizar la formación de Bloques.\nEl diseño por bloques es una técnica que permite controlar la variabilidad de los resultados debida a factores no controlados, bloques. Se trata de dividir el experimento en bloques homogéneos, de forma que las diferencias entre los bloques sean menores que las diferencias entre los tratamientos. Otra opción posible es manteer fijo el factor perturbador y variar el factor de interés (volviendo al caso del diseño unifactorial).\nSi disponemos en general de \\(a\\) tratamientos (niveles del factor de interés) y \\(b\\) bloques (niveles del factor perturbador), el modelo estadístico será:\n\\[y_{ij}=\\mu+\\tau_i+\\beta_j+\\epsilon_{ij}, \\qquad i=1, \\ldots, a; j=1, \\ldots, b\\]\nEstamos interesados solo en el contraste de hipótesis sobre los efectos \\(\\tau_i\\).\nDescomponiendo ahora la variabilidad de las observaciones teniendo en cuenta los dos factores tenemos:\n\\[SC_{Total}=SC_{tratamiento}+SC_{bloque}+SC_{residual}\\]\n\\[\\sum^a \\sum^b (y_{ij}-\\bar{y})^2=b \\sum^a (\\bar{y}_{i.}-\\bar{y})^2+a \\sum^b (\\bar{y}_{.j}-\\bar{y})^2+\\sum^a \\sum^b (y_{ij}-\\bar{y}_{.j}-\\bar{y}_{i.}+\\bar{y})^2\\]\nSiendo \\((a-1)\\) los grados de libertad para Tratamiento, \\((b-1)\\) para el factor Bloque y \\(ab-1-(a-1)-(b-1)=(a-1)(b-1)\\) para los residuales.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html#diseños-bifactoriales-con-interacción",
    "href": "Cap2-DoE.html#diseños-bifactoriales-con-interacción",
    "title": "2  Diseño de experimentos",
    "section": "2.6 Diseños bifactoriales con interacción",
    "text": "2.6 Diseños bifactoriales con interacción\nConsideramos ahora el caso de 2 factores de interés que podrían tener un efecto sobre la respuesta, y que, además, podrían interactuar entre ellos, lo que añadiría otro efecto sobre la misma.\nSi tenemos el factor A (con a niveles) y el factor B (con b niveles), el diseño bifactorial básico consiste en la combinación de todos los niveles de ambos factores, es decir una cantidad \\(a\\cdot b\\) de experimentos.\nDisponer de réplicas significa obtener combinaciones completas de \\(a\\cdot b\\) experimentos. Así, el diseño bifactorial tendrá \\(a\\cdot b\\) experimentos (sin réplica) o \\(2 a\\cdot b\\), \\(3 a\\cdot b\\), etc. experimentos (con réplica). Las réplicas permiten estimar la variabilidad de las observaciones en cada combinación experimental y realizar contrastes de hipótesis sobre los efectos de los factores y sus interacciones.\nCon este tipo de diseños se intenta responder a dos tipos de preguntas:\n\n¿Qué efectos tienen el factor A y el factor B sobre la respuesta?\n¿Existe algún tratamiento del factor A que produzca de manera regular una mejor respuesta independientemente del factor B?\n\nEl modelo de los efectos en este caso viene dado por:\n\\[y_{ijk}=\\mu + \\tau_i + \\beta_j + (\\tau\\beta)_{ij}+ \\epsilon_{ijk}, \\quad i=1,\\dots,a;  j=1,\\dots,b; k=1,\\dots,n\\]\nLa suma de cuadrados en este caso puede escribirse como:\n\\[SC_{Total}=SC_A+SC_B+SC_{AB}+SC_{residual}\\]\n\\[\\sum^a \\sum^b \\sum^n (y_{ijk}-\\bar{y})^2=\nbn\\sum^a (\\bar{y_{i..}}-\\bar{y})^2+\nan\\sum^b (\\bar{y_{.j.}}-\\bar{y})^2+\\] \\[n\\sum^a\\sum^b (\\bar{y_{ij.}}-\\bar{y_{i..}}-\\bar{y_{.j.}}+\\bar{y})^2+\n\\sum^a \\sum^b \\sum^n (y_{ijk}-\\bar{y_{ij.}})^2\\]\nPor el último componente se observa que debe haber por lo menos dos réplicas (\\(n\\geq 2\\)) para poder obtener una suma de cuadrados del error.\nSiendo los grados de libertad:\n\n\n\nEfecto\nG.L.\n\n\n\n\nA\na-1\n\n\nB\nb-1\n\n\nAB\n(a-1)(b-1)\n\n\nError\nab(n-1)\n\n\nTotal\nabn-1\n\n\n\n\n2.6.1 Una sola réplica\nEn este caso, la varianza del error, \\(\\sigma^2\\) no es estimable.\n\\[SC_{residual}=\\sum^a \\sum^b \\sum^n (y_{ijk}-\\bar{y_{ij.}})^2\\]\nEsto implica que el efecto de la interacción de los dos factores \\((\\tau\\beta)_{ij}\\) y el error experimental no pueden separarse de alguna manera obvia.\n\\[SC_{Total}=SC_A+SC_B+SC_{AB+Residual}\\]\n\\[SC_{AB+Residual}=SC_{Total}-SC_A-SC_B\\]\nSi no hay interacción entre ambos factores \\((\\tau\\beta)_{ij}=0\\), existe un modelo posible:\n\\[y_{ij}=\\mu+\\tau_i+\\beta_j\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html#diseños-de-más-de-dos-factores.",
    "href": "Cap2-DoE.html#diseños-de-más-de-dos-factores.",
    "title": "2  Diseño de experimentos",
    "section": "2.7 Diseños de más de dos factores.",
    "text": "2.7 Diseños de más de dos factores.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html#diseños-factoriales-a-dos-niveles-2k",
    "href": "Cap2-DoE.html#diseños-factoriales-a-dos-niveles-2k",
    "title": "2  Diseño de experimentos",
    "section": "2.8 Diseños factoriales a dos niveles (2^k)",
    "text": "2.8 Diseños factoriales a dos niveles (2^k)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html#lm",
    "href": "Cap2-DoE.html#lm",
    "title": "2  Diseño de experimentos",
    "section": "2.9 Modelo lineal de regresión",
    "text": "2.9 Modelo lineal de regresión\n\n2.9.1 Suma de Cuadrados en la regresión.\nLas distancias verticales entre cada uno de los puntos, \\((x_i,y_i)\\), y la recta de regresión, \\(\\hat{y_i}=\\beta_0 + \\beta_1x_i\\), son los residuos, \\(\\epsilon_i\\). Estas distancias expresan el error aleatorio del modelo. ¿Hasta qué punto es más importante el efecto de la variable \\(X\\) sobre la variable \\(Y\\) que el error de los residuos?\nLa recta de regresión siempre pasa por el centro de los datos \\((\\bar{X},\\bar{Y})\\), ese punto se conoce como centroide. Es el centro de gravedad de la nube de puntos.\nSi las variables \\(X\\) e \\(Y\\) no estuviesen relacionadas entonces el hecho de conocer \\(X\\) no daría información para tener una mejor estimación de \\(Y\\). La mejor predicción que podríamos hacer sería predecir \\(Y\\) con su media, \\(\\bar{Y}\\), sin tener en cuenta el valor de \\(X\\). Este modelo, el mas sencillo, es el que vamos a intentar falsar.\n\\[H_0: \\beta_10\\] \\[H_1: \\beta_1\\neq 0\\]\nPara obtener evidencias que nos permitan rechazar la \\(H_0\\) vamos a estudiar la variabilidad (INFORMACIÓN) de la variable respuesta.\n\\[SC_{total}=SC_y=\\sum (y_i-\\bar{y})^2\\]\n\\(SC_{total}\\) es el numerador de la varianza y se puede calcular multiplicando la varianza por los grados de libertad n-1. Y puede descomponerse en \\(SC_{regresion}\\) y \\(SC_{residual\\).\n\\[SC_{total}=SC_{regresion}+SC_{residual}\\] \\[\\sum (y-\\bar{y})^2={\\sum (\\hat{y}-\\bar{y})^2}+\\sum (y-\\hat{y})^2\\]\nLos grados de libertad totales son n-1, gastamos un grado de libertad al dar la media, \\(\\bar{y}\\), \\(SC_{totales}\\).\nLos grados de libertad de los residuos son n-2, necesitamos \\(\\beta_0\\) y \\(\\beta_1\\) para calcular \\(SC_{residuos}\\).\nCon lo que nos queda 1 grado de libertad para la suma de cuadrados de la regresión \\(SC_{regresion}\\). Es el parametro extra que hemos estimado \\(\\beta_1\\), la pendiente.\nPara completar este estudio de cómo se reparte la variabilidad, promediamos las respectivas sumas de cuadrados por sus grados de libertad (Medias de Cuadrados), en definitiva eso es calcular varianzas.\nNunca seremos capaces de realizar predicciones perfectas, todos los modelos son falsos, pero estamos interesados en comparar el Efecto de X sobre Y con el Error Aleatorio:\n\\[\\frac{\\text{Efecto de X sobre Y}}{\\text{Error Aleatorio}}=\\frac{\\text{Varianza de la Regresión}}{\\text{Varianza Error}}=\\frac{SCM_{regresion}}{SCM_{residual}}=F\\]\nEsto es lo que se conoce en estadística como un Análisis de la Varianza, ANOVA. Nuestro modelo sencillo que intentamos falsar es \\(H_0:\\beta_1=0\\). Para que podamos falsar dicha hipótesis la Varianza de la Regresión debe ser mayor, cuanto más grande mejor, que la Varianza del Error.\nComparamos entonces el estadístico F obtenido de dividir las dos varianzas con una distribución F con los grados de libertad correspondientes, 1 en el numerador y n-2 en el denominador. Calculando el p-valor correspondiente se podrá rechazar (o no) la hipótesis nula.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html#diseño-de-experimentos",
    "href": "Cap2-DoE.html#diseño-de-experimentos",
    "title": "2  Diseño de experimentos",
    "section": "2.10 Diseño de experimentos",
    "text": "2.10 Diseño de experimentos",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html#casos-práctico-helicóptero-de-box",
    "href": "Cap2-DoE.html#casos-práctico-helicóptero-de-box",
    "title": "2  Diseño de experimentos",
    "section": "2.11 Casos práctico: Helicóptero de Box",
    "text": "2.11 Casos práctico: Helicóptero de Box\nConsideramos el ejemplo del helicóptero de Box del que se puede obtener información en https://williamghunter.net/george-box-articles/teaching-engineers-experimental-design-with-a-paper-helicopter.\n\n2.11.1 Diseño unifactorial\nNos preguntamos si la longitud del ala del helicóptero influye en su tiempo de caída/vuelo. Esta simple pregunta permite plantear un diseño de 1 sólo factor, la longitud del ala, frente a la variable respuesta, el tiempo de caída. Para ello escogemos 3 valores distintos (3 categorías) de la longitud del ala.\n\nEl ejemplo es equivalente a preguntarse si influye en el tiempo de caía el grosor del papel (tomando 3 grosores distintos), o si influye el ponerle clip al helicóptero (en este caso sólo 2 valores, ponerselo o no, variable dicotómica), etc. Más adelante veremos cómo manejar 2 o más variables a la vez.\n\nLas mediciones realizadas del tiempo de caída (en segundos) desde una determinada altura de 12 helicópteros de papel (réplicas, no repeticiones) construidos con las mismas dimensiones excepto la longitud del ala (variable explicativa, con 3 categorías: 8, 12, y 16 cm) son:\n\n\n\n8 cm\n12 cm\n16 cm\n\n\n\n\n2.59\n2.97\n3.14\n\n\n2.65\n3.09\n3.17\n\n\n2.76\n3.12\n3.21\n\n\n2.78\n3.23\n3.32\n\n\n\nComparación de 2 niveles\nPrimero consideraremos sólo 2 longitudes de ala de los helicópteros: 8 cm y 16 cm. Posteriormente trabajaremos la comparación entre las 3 longitudes de ala. El ANOVA viene a responder a la pregunta técnica: ¿Las diferencias entre los dos (tres) tratamientos (longitudes de ala) se deben simplemente al azar? ¿o realmente son diferencias auténticas, significativas?\n\nHay que tener en cuenta que, en este caso, el diseño es balanceado (mismo número de elementos en cada tratamiento). El lector puede experimentar añadiendo u omitiendo un dato y observar los cambios que se producen.\n\nLos datos en el orden aleatorio de lanzamiento/medición son:\n\ntiempos &lt;- c(2.97, 3.23, 3.09, 2.65, 2.78, 3.21, 2.59, 3.14, 3.17, 3.12, 3.32, 2.76)\nala &lt;- c(\"12 cm\", \"12 cm\", \"12 cm\", \"8 cm\", \"8 cm\", \"16 cm\", \"8 cm\", \"16 cm\", \"16 cm\", \"12 cm\", \"16 cm\", \"8 cm\")\nala &lt;- factor(ala, levels = c(\"8 cm\", \"12 cm\", \"16 cm\"))\nheli3 &lt;- data.frame(tiempos, ala)\nheli2 &lt;- data.frame(tiempos, ala)[ala != \"12 cm\", ]\n\nRealizamos un resumen gráfico y numérico de los datos:\n\n# Diagrama de caja\nboxplot(tiempos ~ ala, data = heli2)\n# Añade los valores de los 4 experimentos\nstripchart(tiempos ~ ala, data = heli2, vertical = T, pch = 1, add = T) \n\n\n\n\n\n\n\n# Resumen numérico directo: mu_i y sigma^2_i\nmedias &lt;- tapply(heli2$tiempos, heli2$ala, mean)\nvarian &lt;- tapply(heli2$tiempos, heli2$ala, var)\nrbind(medias, varian)\n\n              8 cm 12 cm  16 cm\nmedias 2.695000000    NA 3.2100\nvarian 0.008166667    NA 0.0062\n\n\nCon el análisis descriptivo (gráfico y numérico) parece haber diferencias entre el tiempo de caída de las 2 longitudes de ala. ¿Serán significativas dichas diferencias? Entramos en el terreno inferencial, aplicando el análisis ANOVA, que como bien sabemos, compara medias considerando las varianzas. La hipótesis de partida, hipótesis nula, \\(H_0\\), es la de NO influencia, es decir las medias de ambos tratamientos son la misma, \\(\\mu_{8} = \\mu_{16}\\). Llegados a este punto, al ser una comparación de dos medias, surge la duda de aplicar el contraste \\(t\\) de Student de dos muestras independientes. Vamos a ver que dicho contraste y el ANOVA son equivalentes en este caso. Empezamos con el ANOVA:\n\nanova.h2 &lt;- aov(tiempos ~ ala, \n                data = heli2)\n# Tabla ANOVA\nsummary(anova.h2)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nala          1 0.5305  0.5305   73.84 0.000137 ***\nResiduals    6 0.0431  0.0072                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Como contraste t de Student\nt.test(tiempos ~ ala, \n       data = heli2, \n       var.equal = TRUE) #Supuesto del ANOVA\n\n\n    Two Sample t-test\n\ndata:  tiempos by ala\nt = -8.5933, df = 6, p-value = 0.0001365\nalternative hypothesis: true difference in means between group 8 cm and group 16 cm is not equal to 0\n95 percent confidence interval:\n -0.6616447 -0.3683553\nsample estimates:\n mean in group 8 cm mean in group 16 cm \n              2.695               3.210 \n\n\nEn ambos casos el p-valor obtenido es pequeño, de hecho, el lector atento habrá visto que coinciden en su valor (aunque son dos estadísticos distintos)!\n\nRecordatorio: Hay una relación directa entre la distribución t de Student y la distribución F, concretamente: \\(t^2 = F\\). Puede comprobarse con los datos del ejemplo anterior.\n\nComo se sabe, el p - valor es la probabilidad de que, siendo las medias poblacionales iguales (hipótesis nula), encontrásemos por azar diferencias entre las medias muestrales mayores que las que hemos encontrado. Por lo que al ser dicha probabilidad muy pequeña, podemos falsar la idea de que las medias poblacionales sean iguales (por azar es muy poco probable encontrar las diferencias que se han observado). Podemos decir que la longitud del ala influye en el tiempo de caída.\nEn la tabla ANOVA podemos ver todos los cálculos intermedios para la obtención del estadístico F, a saber, (por orden) grados de libertad, sumas de cuadrados y media de suma de cuadrados.\nTambién podemos “resolver” el problema como modelo lineal!!\nNota técnica: De hecho la función aov() hace una “llamada” a la función lm()\n\nanova.h2.ml &lt;- lm(tiempos ~ ala, \n                  data = heli2)\n# Tabla ANOVA\nanova(anova.h2.ml)\n\nAnalysis of Variance Table\n\nResponse: tiempos\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nala        1 0.53045 0.53045  73.844 0.0001365 ***\nResiduals  6 0.04310 0.00718                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Modelo lineal estimado\nsummary(anova.h2.ml)\n\n\nCall:\nlm(formula = tiempos ~ ala, data = heli2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.10500 -0.05125 -0.02000  0.07000  0.11000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.69500    0.04238  63.595 1.02e-09 ***\nala16 cm     0.51500    0.05993   8.593 0.000137 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08475 on 6 degrees of freedom\nMultiple R-squared:  0.9249,    Adjusted R-squared:  0.9123 \nF-statistic: 73.84 on 1 and 6 DF,  p-value: 0.0001365\n\n\nComo se puede apreciar se llega a la misma tabla ANOVA. Y el modelo estimado aporta información complementaria… Vuelve a aparecer el mismo p-valor (obviamente), por dos lados!\n\nPregunta\n¿Puede interpretar el valor de la estimación de “ala8 cm” obtenida?\n\nComparación de 3 niveles\nConsideramos ahora la comparación de las 3 longitudes de ala. Primero el análisis descriptivo:\n\n# Diagrama de caja\nboxplot(tiempos ~ ala, data = heli3) \n# Añade los valores de los 4 experimentos\nstripchart(tiempos ~ ala, data = heli3, vertical = T, pch = 1, add = T) \n\n\n\n\n\n\n\n# Resumen numérico directo: mu_i y sigma^2_i\nmedias &lt;- tapply(heli3$tiempos, heli3$ala, mean)\nvarian &lt;- tapply(heli3$tiempos, heli3$ala, var)\n# Efecto del tratamiento i-ésimo: tau_i\nefecto &lt;- tapply(heli3$tiempos, heli3$ala, mean) - mean(heli3$tiempos)\nrbind(medias, varian, efecto)\n\n               8 cm    12 cm  16 cm\nmedias  2.695000000 3.102500 3.2100\nvarian  0.008166667 0.011425 0.0062\nefecto -0.307500000 0.100000 0.2075\n\n\nVamos a visualizar ahora los datos mostrando tanto los residuos respecto a la media (“VT”), los efectos por grupo/categoría (“VE”) y los residuos intragrupo (“VNE”):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara el análisis inferencial, ahora no es posible aplicar el contraste de la t de Student para comparar los 3 niveles a la vez, se podrían realizar las comparaciones por pares, pero no quedaría resuelto el problema global.\n\nanova.h3 &lt;- lm(tiempos ~ ala, \n               data = heli3)\n# Tabla ANOVA\nanova(anova.h3)\n\nAnalysis of Variance Table\n\nResponse: tiempos\n          Df  Sum Sq  Mean Sq F value    Pr(&gt;F)    \nala        2 0.59045 0.295225   34.34 6.134e-05 ***\nResiduals  9 0.07737 0.008597                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Modelo lineal estimado\nsummary(anova.h3)\n\n\nCall:\nlm(formula = tiempos ~ ala, data = heli3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.13250 -0.05125 -0.00625  0.07000  0.12750 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.69500    0.04636  58.131 6.64e-13 ***\nala12 cm     0.40750    0.06556   6.215 0.000156 ***\nala16 cm     0.51500    0.06556   7.855 2.56e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09272 on 9 degrees of freedom\nMultiple R-squared:  0.8841,    Adjusted R-squared:  0.8584 \nF-statistic: 34.34 on 2 and 9 DF,  p-value: 6.134e-05\n\n\nLa conclusión es la misma que antes, el ala influye en el tiempo de caída. Pero, al haber 3 niveles se debe mirar si todas las medias son distintas entre sí o sólo hay algunas medias distintas. Con la salida del summary se puede apreciar que la diferencia significativa se da entre las alas de 8 y 12 cm, pero no entre las alas de 12 y 16 cm. Pero no resolvemos el problema completo. ¿Hay diferencia entre las alas de 12 y 16 cm?… Debemos acudir a los contrastes de comparaciones múltiples.\nAnálisis de residuos (Diagnosis)\nAntes de seguir con el análisis inferencial se deben validar las hipótesis del modelo ANOVA. Como ya hicimos con los modelos de regresión, en R se puede acudir a la función plot() aplicada al objeto de tipo lm.\n\npar(mfrow = c(2, 2))\nplot(anova.h3)\n\n\n\n\n\n\n\n\nNo parece haber incumplimiento de ninguna de las hipótesis. Por si quedan dudas de la homogeneidad de varianzas, podemos usar el test de Barlett:\n\nbartlett.test(tiempos ~ ala, \n              data = heli3)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  tiempos by ala\nBartlett's K-squared = 0.24536, df = 2, p-value = 0.8845\n\n\nNo podemos rechazar la homocedasticidad de los grupos.\nComparaciones múltiples\nDe entre las distintos contrastes de comparaciones múltiples, se utilizará el de Tukey que permite una visualización gráfica de las comparaciones lo que resulta muy intuitivo y didáctico:\n\n( heli3.tukey &lt;- TukeyHSD(aov(tiempos ~ ala, \n                              data = heli3)) )\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = tiempos ~ ala, data = heli3)\n\n$ala\n              diff         lwr       upr     p adj\n12 cm-8 cm  0.4075  0.22444552 0.5905545 0.0004097\n16 cm-8 cm  0.5150  0.33194552 0.6980545 0.0000680\n16 cm-12 cm 0.1075 -0.07555448 0.2905545 0.2792751\n\nplot(heli3.tukey)\n\n\n\n\n\n\n\n\nDe la tabla, y más visualmente del gráfico, se deduce que existen diferencias significativas (controlando el nivel de significación global de todos los contrastes) entre los tiempos de caída de los helicópteros con ala 8 y 12 cm, y entre los helicópteros con ala 8 y 16 cm, dado que el intervalo de confianza de la diferencia de medias entre cada par de tiempos de caída no contiene el valor 0. Por el contrario, no podemos decir que los helicópteros con ala 12 o 16 cm tengan diferencias significativas en sus tiempos de caída. De este hecho se puede concluir que el efecto de la longitud del ala no parece lineal,\n\n\n2.11.2 Diseños bifactoriales\nNos planteamos aquí realizar un diseño bifactorial para comparar el efecto de 2 variables categóricas/factores en la variable respuesta (tiempo de caída). Supongamos que interesan, tanto el efecto de la longitud de ala como el efecto de la anchura de ala, en el tiempo de caída. Considerando 2 niveles para la anchura: 4 y 6 cm, se podría definir un diseño factorial con la combinación de los 3 niveles de la longitud de ala definidos anteriormente y los 2 de la anchura. Esto llevaría a 6 (\\(3 \\times 2\\)) tratamientos distintos, de las que se podrían hacer réplicas (o no). Si realizamos 2 réplicas de cada tratamiento, tendremos 12 helicópteros de los que mediremos su tiempo de caída.\n\n#library(grid)\n#library(conf.design)\nlibrary(DoE.base)\n\nCargando paquete requerido: grid\n\n\nCargando paquete requerido: conf.design\n\n\nRegistered S3 method overwritten by 'DoE.base':\n  method           from       \n  factorize.factor conf.design\n\n\n\nAdjuntando el paquete: 'DoE.base'\n\n\nThe following objects are masked from 'package:stats':\n\n    aov, lm\n\n\nThe following object is masked from 'package:graphics':\n\n    plot.design\n\n\nThe following object is masked from 'package:base':\n\n    lengths\n\nset.seed(pi)\n( Dis.bifactorial &lt;- fac.design(c(3,2), \n                              factor.names = list(long = c(\"8cm\", \"12cm\", \"16cm\"), \n                                                  anch = c(\"4cm\", \"6cm\")),\n                              randomize = TRUE, \n                              replications = 2) )\n\ncreating full factorial with 6 runs ...\n\n\n   run.no run.no.std.rp long anch Blocks\n1       1           5.1 12cm  6cm     .1\n2       2           2.1 12cm  4cm     .1\n3       3           4.1  8cm  6cm     .1\n4       4           3.1 16cm  4cm     .1\n5       5           6.1 16cm  6cm     .1\n6       6           1.1  8cm  4cm     .1\n7       7           3.2 16cm  4cm     .2\n8       8           4.2  8cm  6cm     .2\n9       9           2.2 12cm  4cm     .2\n10     10           6.2 16cm  6cm     .2\n11     11           5.2 12cm  6cm     .2\n12     12           1.2  8cm  4cm     .2\nclass=design, type= full factorial \nNOTE: columns run.no and run.no.std.rp  are annotation, \n not part of the data frame\n\n#Consideramos los mismos tiempos del diseño unifactorial\n#pero nótese que no mantenemos los valores de longitud de ala\ntiempos2 &lt;- tiempos\n( Dis.bifactorial.R &lt;- add.response(Dis.bifactorial, response = tiempos2) )\n\n   run.no run.no.std.rp long anch Blocks tiempos2\n1       1           5.1 12cm  6cm     .1     2.97\n2       2           2.1 12cm  4cm     .1     3.23\n3       3           4.1  8cm  6cm     .1     3.09\n4       4           3.1 16cm  4cm     .1     2.65\n5       5           6.1 16cm  6cm     .1     2.78\n6       6           1.1  8cm  4cm     .1     3.21\n7       7           3.2 16cm  4cm     .2     2.59\n8       8           4.2  8cm  6cm     .2     3.14\n9       9           2.2 12cm  4cm     .2     3.17\n10     10           6.2 16cm  6cm     .2     3.12\n11     11           5.2 12cm  6cm     .2     3.32\n12     12           1.2  8cm  4cm     .2     2.76\nclass=design, type= full factorial \nNOTE: columns run.no and run.no.std.rp  are annotation, \n not part of the data frame\n\n\nPlanteamos ahora distintos modelos sobre estos datos.\nDiseño bifactorial sin interacción\n\nmod.bifact &lt;- lm(tiempos2 ~ long + anch, data = Dis.bifactorial.R)\nanova(mod.bifact)\n\nAnalysis of Variance Table\n\nResponse: tiempos2\n          Df   Sum Sq  Mean Sq F value  Pr(&gt;F)  \nlong       2 0.313850 0.156925  4.1945 0.05678 .\nanch       1 0.054675 0.054675  1.4614 0.26122  \nResiduals  8 0.299300 0.037412                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mod.bifact)\n\n\nCall:\nlm.default(formula = tiempos2 ~ long + anch, data = Dis.bifactorial.R)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27000 -0.08625 -0.00250  0.09125  0.26750 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.05000    0.09671  31.537 1.11e-09 ***\nlong12cm     0.12250    0.13677   0.896   0.3966    \nlong16cm    -0.26500    0.13677  -1.938   0.0887 .  \nanch1        0.06750    0.05584   1.209   0.2612    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1934 on 8 degrees of freedom\nMultiple R-squared:  0.5518,    Adjusted R-squared:  0.3838 \nF-statistic: 3.283 on 3 and 8 DF,  p-value: 0.07939\n\n\nUn complemento gráfico:\n\nboxplot(tiempos2 ~ long, data = Dis.bifactorial.R)\n\n\n\n\n\n\n\nboxplot(tiempos2 ~ anch, data = Dis.bifactorial.R)\n\n\n\n\n\n\n\nboxplot(tiempos2 ~ long * anch, data = Dis.bifactorial.R)\n\n\n\n\n\n\n\n\n¿Y el análisis de residuos?\n\npar(mfrow = c(2,2))\nplot(mod.bifact)\n\n\n\n\n\n\n\n\nDiseño bifactorial con interacción En este caso podría tener sentido que las variables longitud y anchura del ala interactuaran… A mayor/menor valor de ambas mayor/menor será la superficie del ala.\n\nmod.bifact.interacc &lt;- lm(tiempos2 ~ long * anch, data = Dis.bifactorial.R)\nanova(mod.bifact.interacc)\n\nAnalysis of Variance Table\n\nResponse: tiempos2\n          Df   Sum Sq  Mean Sq F value  Pr(&gt;F)  \nlong       2 0.313850 0.156925  4.1819 0.07289 .\nanch       1 0.054675 0.054675  1.4570 0.27283  \nlong:anch  2 0.074150 0.037075  0.9880 0.42569  \nResiduals  6 0.225150 0.037525                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mod.bifact.interacc)\n\n\nCall:\nlm.default(formula = tiempos2 ~ long * anch, data = Dis.bifactorial.R)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-0.225 -0.065  0.000  0.065  0.225 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     3.05000    0.09686  31.490 6.81e-08 ***\nlong12cm        0.12250    0.13698   0.894    0.406    \nlong16cm       -0.26500    0.13698  -1.935    0.101    \nanch1           0.06500    0.09686   0.671    0.527    \nlong12cm:anch1 -0.09250    0.13698  -0.675    0.525    \nlong16cm:anch1  0.10000    0.13698   0.730    0.493    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1937 on 6 degrees of freedom\nMultiple R-squared:  0.6629,    Adjusted R-squared:  0.3819 \nF-statistic: 2.359 on 5 and 6 DF,  p-value: 0.1629\n\npar(mfrow = c(2,2))\nplot(mod.bifact.interacc)\n\n\n\n\n\n\n\n\n\n\n2.11.3 Diseño trifactorial\nSi las 12 mediciones anteriores proviniesen de un diseño factorial con 3 variables, por ejemplo, añadiendo la variable “clip” a las 2 anteriores, tendríamos sólo una observación por tratamiento. Esto implicaría que no se pueden estimar todos los parámetros del modelo completo (nos quedamos sin grados de libertad), pero sí se pueden estimar modelos con inferior número de parámetros, como el modelo de efectos principales:\n\n( Dis.trifact &lt;- fac.design(c(3,2,2),\n                            factor.names = list(long = c(\"8cm\", \"12cm\", \"16cm\"),\n                                                anch = c(\"4cm\", \"6cm\"),\n                                                clip = c(\"Si\", \"No\")),\n                              randomize = T ) )\n\ncreating full factorial with 12 runs ...\n\n\n   long anch clip\n1  12cm  4cm   No\n2  16cm  6cm   No\n3  12cm  6cm   Si\n4  12cm  4cm   Si\n5   8cm  6cm   No\n6   8cm  6cm   Si\n7  12cm  6cm   No\n8  16cm  6cm   Si\n9   8cm  4cm   No\n10  8cm  4cm   Si\n11 16cm  4cm   Si\n12 16cm  4cm   No\nclass=design, type= full factorial \n\ntiempos3 &lt;- tiempos\n( Dis.trifact.R &lt;- add.response(Dis.trifact, \n                                response = tiempos3) )\n\n   long anch clip tiempos3\n1  12cm  4cm   No     2.97\n2  16cm  6cm   No     3.23\n3  12cm  6cm   Si     3.09\n4  12cm  4cm   Si     2.65\n5   8cm  6cm   No     2.78\n6   8cm  6cm   Si     3.21\n7  12cm  6cm   No     2.59\n8  16cm  6cm   Si     3.14\n9   8cm  4cm   No     3.17\n10  8cm  4cm   Si     3.12\n11 16cm  4cm   Si     3.32\n12 16cm  4cm   No     2.76\nclass=design, type= full factorial \n\nmod.trifact &lt;- lm(tiempos3 ~ long + anch + clip, \n                  data = Dis.trifact.R)\nsummary(mod.trifact)\n\n\nCall:\nlm.default(formula = tiempos3 ~ long + anch + clip, data = Dis.trifact.R)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.262500 -0.167083  0.009167  0.178750  0.235000 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.070000   0.117497  26.128 3.08e-08 ***\nlong12cm    -0.245000   0.166167  -1.474    0.184    \nlong16cm     0.042500   0.166167   0.256    0.805    \nanch1        0.004167   0.067837   0.061    0.953    \nclip1       -0.085833   0.067837  -1.265    0.246    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.235 on 7 degrees of freedom\nMultiple R-squared:  0.4212,    Adjusted R-squared:  0.09041 \nF-statistic: 1.273 on 4 and 7 DF,  p-value: 0.365",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html#por-aquí",
    "href": "Cap2-DoE.html#por-aquí",
    "title": "2  Diseño de experimentos",
    "section": "2.12 POR AQUí",
    "text": "2.12 POR AQUí\n\n2.12.1 Extras\nDuda: ANOVA de Medidas repetidas ¿? NO, sólo mencionar ;-)\n\n2.12.1.1 Esquema libro de PEÑA\n1ª parte: Diseño de Experimentos.\n\n(CAP 2) ANOVA: modelo, estimación, contraste F, contrastes múltiples, diagnosis, transformaciones para homocedasticidad\n(CAP 3) Varios factores: principios del DOE, 2 factores, ANOVA, 2 factores + interacción (con o sin réplicas), más de 2 factores, cuadrado latino, grecolatino, modelos con efectos aleatorios\n(CAP 4) Diseños factoriales a 2 niveles: 2^2, 2^3, 2^k, fracciones, aplicaciones, estimación, diagnosis.\n\n\n\n2.12.1.2 Prueba de Kruskal-Wallis (no paramétrico)\nLa prueba de Kruskal-Wallis es una prueba no paramétrica, basada en los rangos (orden) de las respuestas, en lugar de sus valores originales. Por lo tanto, es menos sensible a la presencia de valores atípicos y a la forma de la distribución de las respuestas. Se utiliza para comparar las (pseudo)medianas de dos o más grupos independientes. Es una extensión de la prueba de Mann-Whitney U (o Wilcoxon) para más de dos grupos. Se usa cuando los supuestos del ANOVA no se cumplen, como la normalidad de los datos o la homogeneidad de varianzas.\nEl supuesto sobre la obtención de las muestras (muestras aleatorias simples independientes de las respectivas poblaciones) sigue siendo importante. Además, se supone que en cada una de las poblaciones consideradas, la variable respuesta tiene una distribución continua.\n\\[H_0:\\text{la distribución es la misma para todas las poblaciones}\\] \\[H_1:\\text{la distribución es sistemáticamente distinta para alguna población}\\]\nLa prueba de Kruskal-Wallis de rangos se basa en ordenar las respuestas de todos los grupos, y a continuación aplicar el ANOVA de un factor a los rangos y no a los valores originales. Si en total tenemos N observaciones, los rangos son valores enteros positivos de 1 a N.\nSea \\(R_i\\) la suma de los rangos de la i-ésima muestra. El estadístico de Kruskal-Wallis es\n\\[H=\\frac{12}{N(N+1)}\\sum\\frac{R_i^2}{n_i}-3(N+1)\\]\nCuando los tamaños de las muestras \\(n_i\\) son grandes y todas las poblaciones tienen la misma distribución continua, \\(H\\) tiene aproximadamente una distribución \\(\\chi^2\\) con \\(I - 1\\) grados de libertad. Así, valores “grandes” de \\(H\\) llevan a rechazar la hipótesis nula de que todas las poblaciones tienen la misma distribución.\nCuanto más distintas sean estas sumas, mayor será la evidencia de que las respuestas de unos grupos son sistemáticamente mayores que las de otros.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html#licesio-diseños-de-medidas-repetidas",
    "href": "Cap2-DoE.html#licesio-diseños-de-medidas-repetidas",
    "title": "2  Diseño de experimentos",
    "section": "2.13 (LICESIO) Diseños de medidas repetidas",
    "text": "2.13 (LICESIO) Diseños de medidas repetidas",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html#modelo-lineal-de-regresión-múltiple",
    "href": "Cap2-DoE.html#modelo-lineal-de-regresión-múltiple",
    "title": "2  Diseño de experimentos",
    "section": "2.14 Modelo lineal de regresión múltiple",
    "text": "2.14 Modelo lineal de regresión múltiple\n\nSaturado: Un parámetro para cada observación. Ajuste perfecto. Grados de libertad 0.\nMaximal: Contiene p variables y sus interacciones. Muchos de estos términos son despreciables. Grados de libertad \\(n-p-1\\).\nMinimal y Adecuado: Contiene las variables e interacciones significativas. Grados de libertad \\(n-p'-1\\).\nModelo ‘Nulo’:Único parametro, \\(\\bar{y}\\). Grados de libertad \\(n-1\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html#bibliografía",
    "href": "Cap2-DoE.html#bibliografía",
    "title": "2  Diseño de experimentos",
    "section": "2.15 Bibliografía",
    "text": "2.15 Bibliografía\n\n\n\n\nFaraway, Julian J. 2004. Linear Models with R. Chapman &amp; Hall/CRC.\n\n\nMontgomery, Douglas C. 2008. Design and Analysis of Experiments. Seventh. Wiley.\n\n\nPeña, Daniel. 2002. Regresión y diseño de experimentos. Alianza Editorial.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap3-GLM.html",
    "href": "Cap3-GLM.html",
    "title": "3  Modelos lineales generalizados",
    "section": "",
    "text": "3.1 Modelo. Función de enlace\nEn el capítulo @ref(sec-LM) de modelos lineales se introdujo la idea de utilizar modelos estadísticos para entender cómo una variable depende de otras que podrían influir en su comportamiento. En dicho capítulo, la variable dependiente/respuesta considerada era continua y se pretendía explicar o predecir a partir de otras variables (de distinto tipo), pero, en ocasiones la variable respuesta es de otro tipo, discreta o categórica y también se pretende ver cómo influyen otras variables en ella. Por ejemplo:\nEstos ejemplos no se pueden abordar correctamente con el modelo de regresión lineal múltiple presentado, pues la variable respuesta, al ser discreta, y sobre todo si es categórica, no sigue una distribución Normal, lo cual es un requisito en regresión. Concretamente, algunas variables respuestas discretas se pueden modelizar mejor con una distribución de Poisson, como las variables que son conteos (como el número de días de hospitalización, que sólo toman valores enteros no negativos). Las variables respuesta de tipo categórica dicotómica (como tener o no una enfermedad) se modelizan con una distribución de Bernoulli, muy alejada de la distribución Normal.\nPara abordar estos problemas con variable respuesta no continua, se introduce en este capítulo el modelo lineal generalizado (GLM) (cuidado que el calificativo generalizado tiene una connotación distinta a general, que es habitual utilizar para hablar de lo que hemos llamado modelo de regresión lineal múltiple), que amplía el marco de la regresión lineal permitiendo que la variable dependiente siga cualquier distribución dentro de la familia exponencial. Además, este modelo admite que la varianza de los errores no sea constante.\nNos centraremos en dos aplicaciones concretas de los GLM: la regresión logística, adecuada para variables binarias, y la regresión de Poisson, ideal para datos de conteo. Ambos enfoques permiten modelar correctamente los ejemplos planteados.\nUn par de referencias bibliográficas sobre este tema, que conjugan teoría y práctica, son las ya mencionadas en capítulos anteriores: James et al. (2013) y Fernández-Avilés y Montero (2024). Una referencia bibliográfica clásica sobre GLMs es McCullagh y Nelder (1989).\nLa forma de definir el modelo lineal generalizado difiere de la del modelo lineal de regresión. La idea detrás del modelo lineal de regresión es predecir valores de la variable respuesta \\(Y\\), a partir de la combinación lineal de los predictores \\(X = (X_1, \\ldots , X_k)\\) y sus correspondientes parámetros estimables \\(\\beta = (\\beta_1, \\ldots , \\beta_k)\\). Si la variable \\(Y\\) no es continua, pongamos por caso que es dicotómica, y toma valores 0 y 1, la predicción de una respuesta media obtenida para unos determinados valores de \\(X\\) difícilmente proporcionará siempre los valores 0 ó 1 (que son sus dos únicos valores admisibles). Es por ello que se introduce la denominada función de enlace, \\(g\\), que, escogida cuidadosamente, permitirá obtener tales valores. Matemáticamente: \\[E(Y) = g^{-1}(X \\beta),\\] donde:\nLo que no cambia respecto al modelo lineal de regresión son las etapas del análisis de un GLM:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales generalizados</span>"
    ]
  },
  {
    "objectID": "Cap3-GLM.html#modelo.-función-de-enlace",
    "href": "Cap3-GLM.html#modelo.-función-de-enlace",
    "title": "3  Modelos lineales generalizados",
    "section": "",
    "text": "\\(Y\\) es el vector que contiene las variables respuesta, que puede seguir cualquier distribución de probabilidad de la familia exponencial: Normal (por tanto el modelo de regresión lineal es un caso particular de GLM), Bernoulli/binomial (utilizada en la denominada regresión logística), la Poisson, la gamma, etc..\n\\(E(Y)\\), será el valor medio de la variable respuesta,\n\\(X \\beta\\), es el predictor lineal, la “estructura” que aportan los predictores, que intentan explicar el comportamiento de la variable respuesta,\n\\(g(\\cdot)\\), la función de enlace, la que otorga la generalización a los GLM, y que relaciona la “estructura” con la distribución de probabilidad de la variable respuesta. En (McCullaghyNelder1989?) se indican la _función de enlace canónica* asociada a cada distribución de probabilidad (pueden utilizarse otras funciones para cada distribución):\n\n\n\n\n\n\n\n\n\nDistribución\nFunción de Enlace (\\(\\mu = E(Y)\\))\n\n\n\n\n\nNormal\n\\(g(\\mu) = \\mu\\)\nIdentidad\n\n\nBernoulli\n\\(g(\\mu) = \\logit(\\mu) = \\log\\left(\\frac{\\mu}{1 - \\mu}\\right)\\)\nLogit\n\n\nPoisson\n\\(g(\\mu) = \\log(\\mu)\\)\nLogaritmo\n\n\nGamma\n\\(g(\\mu) = \\frac{1}{\\mu}\\)\nInversa\n\n\n\n\nTanto en la distribución Bernoulli, como en la Poisson, aparece el logaritmo (neperiano) en la función de enlace, lo que conducirá a efectos multiplicativos de los factores o covariables sobre la respuesta, como se verá más claramente en la Sec. @ref(SECCinterp). Este es un punto que las distingue de la regresión lineal, en la que los efectos son aditivos.\n\n\n\nEspecificación: se estiman los parámetros de la estructura (predictor lineal) predefinida de antemano. A diferencia de la regresión lineal múltiple, que utiliza estimadores de mínimos cuadrados, en los GLMs se utilizan estimadores de máxima verosimilitud (si no se tiene normalidad en la variable respuesta, los estimadores de mínimos cuadrados no serán eficientes). Tras la estimación se deberá realizar la correspondiente diagnosis.\nPredicción: El modelo (predeterminado) estimado se utiliza para predecir nuevas respuestas. Veremos que dichas respuestas podrán ser, según sea el caso: valores, probabilidades de ocurrencia, etc. Esta parte se utiliza sobre todo para el enfoque de “Machine Learning”, al utilizar los errores de predicción como métrica para comparar modelos y elegir el que mejores predicciones realice para el problema entre manos.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales generalizados</span>"
    ]
  },
  {
    "objectID": "Cap3-GLM.html#glms-particulares",
    "href": "Cap3-GLM.html#glms-particulares",
    "title": "3  Modelos lineales generalizados",
    "section": "3.2 GLMs particulares",
    "text": "3.2 GLMs particulares\nDe entre los distintos casos particulares de modelos lineales generalizados, predomina el modelo de regresión logística, que veremos a continuación. Se suele etiquetar como modelo para problemas de clasificación (supervisada), lo que tiene una gran relevancia en multitud de campos científicos, como la Epidemiología, Psicología, Economía, etc. Hablar de clasificación proviene del hecho de que la variable respuesta \\(Y\\) que queremos explicar o predecir consiste en la pertenencia, o no, a un determinado grupo de interés.\n\nSe distingue entre clasificación supervisada y no supervisada cuando se conoce (o no) la clasificación correcta de los datos de entrenamiento (en el conjunto de datos hay una variable que contiene dicho valor de clasificación, o no la hay). La regresión logística predomina sobre otros métodos de clasificación supervisada (Análisis Discriminante, \\(K\\)-vecinos más próximos (KNN), etc.) al poseer una interpretabilidad que no poseen los otros métodos (véase la imagen del Capítulo 2 de James et al. (2013))\n\nEl otro modelo lineal generalizado que veremos es el de la regresión de Poisson.\n\n3.2.1 Regresión Logística\nLa aplicación de una regresión logística se asocia a una variable respuesta dicotómica. que pretendemos explicar a partir del conocimiento de otras variables. Habitualmente se considera que toma el valor \\(Y = 0\\) si no pertenece al grupo, e \\(Y = 1\\) si pertenece al grupo (por ejemplo, diagnosticados de enfermedad X, morosos, piezas defectuosas, etc.). Al igual que en el modelo de regresión múltiple, a partir de un conjunto de \\(k\\) variables explicativas \\(X_1, X_2, \\ldots, X_k\\) queremos predecir \\(Y\\), y clasificar al individuo en el grupo, o no.\nSi modelizamos la variable respuesta, mediante una distribución de Bernoulli, dicha distribución quedará caracterizada por la probabilidad de pertenencia al grupo, \\(p\\). Así, \\(P[Y = 1] = p\\) y \\(P[Y = 0] = 1-p\\), y \\(E[Y] = p\\). Con los datos disponibles (tanto de la variable \\(Y\\) como de las \\(k\\) variables explicativas) se intentará estimar dicho valor \\(p\\), es decir, \\(\\hat{p}\\), mejor dicho se estimará el modelo, con la intención de clasificar nuevos individuos o elementos (predecir el grupo al que pertenecerá), con la información que proporcionan sus valores de las variables explicativas. En el proceso de estimación se determinaran aquellas variables que influyen significativamente en la clasificación.\nSegún la formulación del modelo lineal generalizado: \\[E[Y] = g^{-1}(\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_k X_k).\\] Y concretamente para la regresión logística: \\[ \\text{logit}(p) = \\log{ \\Big(\\dfrac{p}{1-p}  \\Big)} = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_k X_k.\\] Al estimar el modelo como si fuese una regresión múltiple, el predictor lineal no proporcionará valores plausibles de la variable respuesta \\(Y\\), sino que proporciona valores de logit(\\(p\\)), es decir, de la estimación del logaritmo del ratio entre la probabilidad de pertenecer al grupo de interés, \\(p\\), y la de no pertenecer a dicho grupo, \\(1-p\\), o dicho en escala logarítmica, la diferencia entre pertenecer y no pertenecer al grupo. La ventaja de utilizar el logit, es que transforma los valores del predictor que podrían estar en cualquier rango de valores, a valores para \\(p\\): \\[ p = \\dfrac{e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_k X_k}}{1+e^{\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_k X_k}},\\] que tienden a 0 por la izquierda y a 1 por la derecha, sin llegar a alcanzar nunca estos valores, por lo que la predicción será siempre un valor válido para una probabilidad. La función logística, en la parte central de su dominio, es prácticamente lineal. Esto implica que para modelizar respuestas de probabilidad moderada, el modelo lineal y el logístico no diferirán mucho. La contrapartida es que que se pierde la linealidad entre el predictor y la probabilidad conforme las probabilidades se aproximan a 0 o a 1, y la interpretación de los parámetros estimados se complica.\n\nComo se ha mencionado, existen otras posibles funciones enlace, como la inversa de la distribución normal tipificada que conduce al denominado modelo probit (muy utilizado en Economía), o tomar la inversa de la distribución uniforme. La curva de la función logística y la de la función probit son muy similares, pero la logística tiene colas más pesadas (véase Peña (2002), página 642).\n\n\n3.2.1.1 Interpretación del modelo\nEl predictor lineal se puede reescribir en forma de diferencias con respecto a la media de cada variable: \\[\\text{logit}(p) = \\beta_0 + \\beta_1 (x_{1i} - \\bar{x}_1) + \\ldots + \\beta_k (x_{ki} - \\bar{x}_k).\\] Así, \\(\\beta_0\\), la ordenada en el origen, es el valor del logit cuando las variables son iguales a sus medias. En tal caso, si \\(\\beta_0 = 0\\), se tendría que logit\\((p) = 0\\) lo que conduce a \\(1 = p_i/(1-p_i)\\) y por lo tanto \\(p_i = 0.5\\). Y si \\(\\beta_0 \\neq 0\\) se tendrá que \\(p_i = 1/(1 + \\exp(-\\beta_0))\\).\nDe la expresión inicial dada del logit, al aplicar la exponencial a ambos términos se llega a: \\[p = \\exp(\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_k X_k) = \\exp(\\beta_0)\\prod_{i = 1}^k \\exp(\\beta_i)^{X_i}\\] Así, en un modelo de regresión logística, los valores estimados de los parámetros, \\(\\hat{\\beta}_i\\) son difíciles de interpretar, pues, proporcionan, ceteris paribus (sin variar el valor de las demás variables), el cambio en escala logarítmica de los llamados odds, \\(\\frac{\\hat p}{1-\\hat p}\\), por unidad de cambio en \\(x_i\\). La interpretación se suele hacer a través de las funciones \\(\\exp(\\hat\\beta_i)\\) denominadas odds ratios, que indican cuánto se modifican las probabilidades de pertenecer o no al grupo de interés por unidad de cambio en la variable \\(x_i\\). Es decir, si mantenemos todas las variables en los mismos valores, menos una variable que cambia en una unidad de un individuo/elemento a otro, todas las exponenciales darán como resultado 1 excepto el término \\(\\exp(\\hat\\beta_i)\\), y \\(\\exp(\\hat\\beta_0)\\) que no depende de las \\(x_i\\). Y si cambian dos variables en una unidad, aparecerán ambos términos multiplicándose. De aquí se deduce que el modelo de regresión logística es multiplicativo, en lugar de aditivo como es el modelo de regresión lineal. Así, si \\(e^{\\hat{\\beta}_i}&lt;1\\) disminuye la probabilidad de pertenecer al grupo de interés, aumentando dicha probabilidad cuando \\(e^{\\hat{\\beta}_i}&gt;1\\), ceteris paribus.\n\nEl modelo de regresión de Poisson, al tener también el logaritmo en la función de enlace, heredará esta propiedad de modelo multiplicativo.\n\n\n\n3.2.1.2 Estimación de los parámetros\n\n\n3.2.1.3 Adecuación del modelo\nLa adecuación del modelo se puede realizar mediante contrastes de hipótesis…\n(Harrel2015)\nLos supuestos que deben verificarse son la linealidad entre cada variable y el logaritmo de las odds y la ausencia de interacción entre variables explicativas y factores (lo que hace que las pendientes sean paralelas, no cambien las pendientes).\n\n3.2.1.3.1 Poisson\n(statbook4) With the residual deviance and residual degrees of freedom, we test for overdispersion: The residual deviance (736.33) is much greater than the residual degrees of freedom (477), indicating substantial overdispersion, so before interpreting any of the effects, we should refit the model using quasipoisson errors:\n\n\n\n3.2.1.4 Curva ROC\nLa aplicación más común de la regresión logística es utilizarla como método de clasificación, más allá de estimar probabilidades, basándose en los valores de las variables explicativas que influyen en el modelo. Esta clasificación se realiza estableciendo un umbral de probabilidad, que determina la pertenencia a un grupo u otro. Por ejemplo, se podría considerar que un individuo pertenece al grupo si la probabilidad estimada supera 0.6, y que no pertenece en caso contrario.\nPara evaluar la eficacia del modelo en esta tarea de clasificación, se recurre al análisis de los aciertos y errores cometidos, en función del umbral de probabilidad escogido. Para ello, se introducen dos métricas fundamentales: la sensibilidad y la especificidad, que se calculan a partir de la matriz de confusión. Véamoslas con un ejemplo. Imaginemos que tenemos una prueba rápida para diagnosticar una enfermedad, que no siempre acierta. Y por otro lado, supongamos que se conoce realmente quién tiene esa enfermedad. Esquemáticamente:\n\n\n\n\n\n\n\n\nMatriz de confusión\nEnfermedad Presente\nEnfermedad Ausente\n\n\n\n\nPrueba Positiva\nVerdadero Positivo (VP)\nFalso Positivo (FP)\n\n\nPrueba Negativa\nFalso Negativo (FN)\nVerdadero Negativo (VN)\n\n\n\nLa sensibilidad, también conocida como tasa de verdaderos positivos, indica la proporción de casos positivos correctamente identificados por el modelo: \\[ \\textit{sensibilidad} = \\dfrac{VP}{VP+FN}\\]\nPor su parte, la especificidad, o tasa de verdaderos negativos, refleja la capacidad del modelo para identificar correctamente los casos negativos: \\[ \\textit{especificidad} = \\dfrac{VN}{VN+FP}\\]\nEstas métricas pueden representarse gráficamente para distintos valores del umbral de clasificación, lo que permite observar cómo varían la sensibilidad y la especificidad a lo largo del rango de probabilidades (de 0 a 1). La herramienta habitualmente utilizada para este análisis es la curva ROC (Receiver Operating Characteristic), que muestra la sensibilidad frente a la tasa de falsos positivos (es decir, \\(1 -\\) especificidad) para diferentes puntos de corte (umbrales). Esta representación se ha convertido en un estándar para evaluar modelos de clasificación. Un clasificador aleatorio presentaría una curva ROC coincidente con la diagonal, mientras que en un clasificador perfecto la curva abarcaría toda la parte superior del gráfico. El resto de clasificadores se moverán entre ellos dos.\nEl AUC (Area Under the Curve) asociado a una curva ROC proporciona una medida cuantitativa del rendimiento del modelo. El valor máximo de AUC es 1, indicando un método de clasificación perfecto, asociado a una curva en ángulo recto que abarca todo el área del gráfico, mientras que un modelo sin capacidad predictiva (equivalente a una clasificación aleatoria) tendría una curva ROC alineada con la diagonal y un AUC de 0.5. Cuanto más próximo a 1 esté el valor de AUC mayor capacidad discriminante tendrá el modelo. Esta métrica resulta especialmente útil para comparar distintos modelos de regresión logística o incluso otros algoritmos de clasificación.\n\nEl más conocido de los métodos de clasificación supervisada es la regresión logística, cuyos resultados poseen una interpretabilidad que otros métodos (Análisis Discriminante, \\(K\\)-vecinos más próximos (KNN), etc.) no poseen (véase la imagen del Capítulo 2. de James et al. (2013)) En ISLR2 también se incluye en el Lab Regresión de Poisson. Por último, hecho en falta en el material original mostrar en este apartado el tema de curvas ROC. Aparece más adelante en el Lab de SVM (apartado 9.3).\n\n\n\n\n3.2.2 Regresión de Poisson\nCuando la variable respuesta es razonable modelizarla con la distribución de Poisson, por ejemplo, cuando es de tipo conteo, se puede realizar una regresión de Poisson, cuyo objetivo es explicar dicha variable en función de \\(k\\) variables explicativas \\(X_1, X_2, \\ldots, X_k\\) que influyen (o no) sobre ella. En este caso, la distribución de Poisson queda caracterizada por su media, \\(\\lambda\\) (que coincide también con su varianza). Con los datos disponibles se estimará el modelo, con la intención de predecir valores de la variable respuesta \\(\\hat{Y}\\) con la información que proporcionan los valores de las variables explicativas, de aquellas que salgan significativas. Aquí tampoco tiene cabida el modelo de regresión lineal, porque, entre otras, las predicciones podrían arrojar valores negativos, o no enteros.\nComo se ha visto en regresión logística, dado que la función de enlace es de tipo logarítmico, los efectos de las variables explicativas serán también _multiplicativos*.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales generalizados</span>"
    ]
  },
  {
    "objectID": "Cap3-GLM.html#caso-práctico-cleveland",
    "href": "Cap3-GLM.html#caso-práctico-cleveland",
    "title": "3  Modelos lineales generalizados",
    "section": "3.3 Caso práctico: cleveland",
    "text": "3.3 Caso práctico: cleveland\nPara realizar regresión lineal generalizada con R, se puede utilizar la función glm() del paquete stats (cargado por defecto al iniciar sesión). Con ella realizaremos tanto la regresión logística, como la regresión de Poisson.\n\n3.3.1 Análisis exploratorio\nVas a ver un caso práctico basado en los datos cleveland incluidos en el paquete CDR y estudiado en Casero-Alonso y Durbán (2024). Se pueden ver otros ejemplos, con otros conjuntos de datos, en James et al. (2013) (lab del capítulo 4).\n\nlibrary(CDR)\nhead(cleveland)\n\n# A tibble: 6 × 6\n  diag  edad       dep        sexo  tdolor dhosp\n  &lt;fct&gt; &lt;labelled&gt; &lt;labelled&gt; &lt;fct&gt; &lt;fct&gt;  &lt;int&gt;\n1 0     63         2.3        1     1          0\n2 1     67         1.5        1     4          4\n3 1     67         2.6        1     4          3\n4 0     37         3.5        1     3          0\n5 0     41         1.4        0     2          1\n6 0     56         0.8        1     2          1\n\nstr(cleveland)\n\ntibble [303 × 6] (S3: tbl_df/tbl/data.frame)\n $ diag  : Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 1 1 2 1 2 2 ...\n $ edad  : 'labelled' int [1:303] 63 67 67 37 41 56 62 57 63 53 ...\n  ..- attr(*, \"label\")= chr \"Edad en años\"\n $ dep   : 'labelled' num [1:303] 2.3 1.5 2.6 3.5 1.4 0.8 3.6 0.6 1.4 3.1 ...\n  ..- attr(*, \"label\")= chr \"epresión ST inducida por ejercicio en relación al reposo\"\n $ sexo  : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 1 2 1 1 2 2 ...\n $ tdolor: Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 4 4 3 2 2 4 4 4 4 ...\n $ dhosp : int [1:303] 0 4 3 0 1 1 7 0 1 3 ...\n\nsummary(cleveland)\n\n diag         edad            dep       sexo    tdolor      dhosp      \n 0:164   Min.   :29.00   Min.   :0.00   0: 97   1: 23   Min.   :0.000  \n 1:139   1st Qu.:48.00   1st Qu.:0.00   1:206   2: 50   1st Qu.:1.000  \n         Median :56.00   Median :0.80           3: 86   Median :2.000  \n         Mean   :54.44   Mean   :1.04           4:144   Mean   :2.033  \n         3rd Qu.:61.00   3rd Qu.:1.60                   3rd Qu.:3.000  \n         Max.   :77.00   Max.   :6.20                   Max.   :8.000  \n\n\nComo se puede apreciar, en este caso tenemos sólo 6 variables, 3 de ellas factores. Para la regresión logística, la variable dicotómica que interesa explicar es diag, el diagnostico de accidente coronario (consultando la ayuda se puede ver que 1 significa sí diagnosticado de accidente coronario), a partir del resto de variables: edad, dep (depresión en el segmento ST inducida por ejercicio en relación al reposo), sexo, tdolor(tipo de dolor), dhosp (días de hospitalización). Para la regresión de Poisson, la variable discreta a explicar será dhosp, en función del resto (incluida diag).\n\npairs(cleveland, lower.panel = NULL)\n\n\n\n\n\n\n\npairs(cleveland, upper.panel = NULL)\n\n\n\n\n\n\n\n\nSe ha separado la visualización de pairs para mostrar los gráficos de dispersión de cada una de las variables, por un lado, frente a diag como variable respuesta (primera fila del primer gráfico depairs()), y por otro lado, frente a dhosp como variable respuesta (última fila del segundo gráfico depairs()).\n\nUn detalle llamativo es que, a pesar de que diag toma valores 0 y 1, en los gráficos se muestra con valores 1 y 2, lo que puede cambiarse, como se puede ver en Casero-Alonso y Durbán (2024). ¿Sabría cómo hacerlo?\n\n\n\n3.3.2 Regresión logística\nVamos a ajustar el modelo de regresión logística indicado anteriormente. En formato de fórmula de R: diag ~ .. Utilizamos la función glm(), que tiene la misma sintaxis que lm(), pero se debe especificar el argumento family. Así, la función glm() también permite realizar regresión lineal, indicando family = gaussian. Para ajustar una regresión logística es argumento debe ser family = binomial.\n\nglm.diag &lt;- glm(diag ~ .,\n                data = cleveland,\n                family = binomial)\nsummary(glm.diag)\n\n\nCall:\nglm(formula = diag ~ ., family = binomial, data = cleveland)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -8.15790    1.62589  -5.017 5.23e-07 ***\nedad         0.05015    0.02247   2.232  0.02564 *  \ndep          0.92345    0.20269   4.556 5.21e-06 ***\nsexo1        1.15434    0.44938   2.569  0.01021 *  \ntdolor2      0.57374    0.83593   0.686  0.49250    \ntdolor3      0.22284    0.71943   0.310  0.75676    \ntdolor4      2.54786    0.70137   3.633  0.00028 ***\ndhosp        1.10953    0.16810   6.600 4.10e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 417.98  on 302  degrees of freedom\nResidual deviance: 183.30  on 295  degrees of freedom\nAIC: 199.3\n\nNumber of Fisher Scoring iterations: 6\n\n\nCon la función summary() obtenemos las estimaciones del predictor lineal, y podemos ver qué variables resultan significativas y cuales no. Pero las estimaciones de los parámetros obtenidos no son las fácilmente interpretables. Los más interpretables son los odds ratios:\n\nexp(coef(glm.diag))\n\n (Intercept)         edad          dep        sexo1      tdolor2      tdolor3 \n2.864642e-04 1.051424e+00 2.517958e+00 3.171935e+00 1.774885e+00 1.249619e+00 \n     tdolor4        dhosp \n1.277974e+01 3.032947e+00 \n\n\nAsí, $e^{{}} = $1.0514241, indica que, ceteris paribus y en media, una diferencia de 1 año de edad (los datos de edad vienen dados en años) aumenta la probabilidad de ser diagnosticado de la enfermedad frente a no ser diagnosticado, en un 5%. Es decir, dos pacientes con el resto de valores iguales que se diferencia en 1 año de edad, el mayor tendrá una probabilidad ligeramente mayor de ser diagnosticado. Si lo llevamos a una diferencia de 15 años, manteniendo el resto de valores iguales, esa probabilidad aumenta en un $e^{{}15}-1 = $112.2%, es decir, más que duplica la probabilidad (lo que se ve mejor observando que $e^{_{}15} = $2.1).\nY las variables dep, sexo y dhosp tienen odds ratio por encima de 2, incluso de 3 (las dos últimas), indicando que un aumento de una unidad en dichas variables más que duplica (triplica) los odds del paciente, excepto para sexo que no tiene sentido el aumento de una unidad sino el cambio de un sexo a otro, al ser una variable factor (si se consulta la ayuda de los datos se verá que son los hombres los que, en media y ceteris paribus, más que triplican el odds de las mujeres).\n\nEl signo de las estimaciones de los parámetros sirven para explicar si el odds ratio asociado a la variable aumenta o disminuye la probabilidad de pertenecer al grupo de interés. Sobre todo si los parámetros son significativos. Valores negativos (y significativos) de las estimaciones se convertirán, con la transformación exponencial en valores por debajo de 1, indicando que el odds ratio disminuye. Es decir, al convertirse el modelo en multiplicativo, la referencia que antes era ser significativamente distinto de “0” para saber si la variable “sumaba” o “restaba” al valor de la respuesta, aquí el valor “neutro” pasa a ser el “1”, por lo que interesa que sea significativamente distinto de “1”, para indicar si la variable “aumenta” o “disminuye” significativamente el valor de probabilidad.\n\nEn regresión lineal, que un parámetro sea significativo se interpreta como significativamente distinto de cero, por el hecho de que el modelo considera efectos aditivos de las variables. El modelo de regresión logística es de efectos multiplicativos y, por ello, el valor “neutro” es el 1. Así, un parámetro significativo se interpreta ahora como significativamente distinto de 1.\nAdecuación del modelo\nEn la salida del summary() también aparecen las deviance del modelo nulo y del ajustado, junto con sus grados de libertad y el AIC del modelo ajustado. Estos valores nos permiten comparar modelos y valorar su adecuación. En el modelo de regresión logística: \\[\\text{Deviance} = - 2 \\log(L)\\] donde \\(L\\) es la verosimilitud. La diferencia entre la Deviance de un modelo más elaborado y el modelo nulo se distribuye como una \\(\\chi^2\\), lo que nos permite realizar inferencia y decidir cuál es el mejor modelo de los posibles. Este test de la razón de verosimilitud se obtiene en R con la función anova():\n\nanova(glm.diag, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: diag\n\nTerms added sequentially (first to last)\n\n       Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                     302     417.98              \nedad    1   15.447       301     402.54 8.487e-05 ***\ndep     1   51.894       300     350.64 5.858e-13 ***\nsexo    1   23.982       299     326.66 9.726e-07 ***\ntdolor  3   62.153       296     264.51 2.037e-13 ***\ndhosp   1   81.202       295     183.30 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nComo se puede comprobar, interesa añadir todas las variables. Cambiemos el orden de las variables a modo de comparación:\n\nglm.diag.bis &lt;- glm(diag ~ dhosp + tdolor + dep + sexo + edad,\n                data = cleveland,\n                family = binomial)\nanova(glm.diag.bis, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: diag\n\nTerms added sequentially (first to last)\n\n       Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                     302     417.98              \ndhosp   1  143.572       301     274.41 &lt; 2.2e-16 ***\ntdolor  3   50.421       298     223.99 6.500e-11 ***\ndep     1   30.134       297     193.86 4.032e-08 ***\nsexo    1    5.383       296     188.47   0.02033 *  \nedad    1    5.169       295     183.30   0.02300 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEn las salidas anteriores se puede observar como la “ganancia” en Residual deviance varía al incluir cada variable (en un orden especificado distinto) al modelo que ya contiene las anteriores variables. La mayor disminución (y por tanto ganancia) se da al incluir primero la variable dhosp (en lugar de incluir primero edad). Aunque el resultado final es el mismo, es óptima la inclusión de todas las variables (en cualquiera de los dos órdenes), llegando a la misma Residual deviance\nOtro contraste de adecuación/ajuste de un modelo de regresión logística, muy utilizado en la práctica, es el de Hosmer-Lemeshov. Comprueba si las probabilidades predichas por el modelo se corresponden bien con las observaciones reales. Lo hace agrupando los datos en deciles (10 grupos) según las probabilidades predichas, utilizando un estadístico de tipo chi-cuadrado (que mide la discrepancia entre lo observado y lo esperado). Como habitualmente, se rechazará que el modelo realiza un buen ajuste si el p-valor obtenido es bajo (típicamente inferior a 0.05), por lo que permite detectar problemas de especificación del modelo, como omisión de variables relevantes o mala elección de la forma funcional.\n\nlibrary(ResourceSelection)\n\nResourceSelection 0.3-6      2023-06-27\n\nhoslem.test(glm.diag$y, glm.diag$fitted.values)\n\n\n    Hosmer and Lemeshow goodness of fit (GOF) test\n\ndata:  glm.diag$y, glm.diag$fitted.values\nX-squared = 9.4523, df = 8, p-value = 0.3056\n\n\n\nPregunta\n¿A qué conclusión se llega con el contraste de Hosmer-Lemeshov?\n\nMedidas de bondad de ajuste Para el modelo de regresión logística no tiene sentido obtener el coeficiente de determinación \\(R^2\\), pero existen alternativas para medir la variabilidad explicada por el modelo, y cuya interpretación es equivalente.\n\nlibrary(DescTools)\nPseudoR2(glm.diag, which = \"all\")\n\n       McFadden     McFaddenAdj        CoxSnell      Nagelkerke   AldrichNelson \n      0.5614544       0.5231753       0.5390725       0.7204056       0.4364656 \nVeallZimmermann           Efron McKelveyZavoina            Tjur             AIC \n      0.7528645       0.6334330       0.7702445       0.6287870     199.3042278 \n            BIC          logLik         logLik0              G2 \n    229.0140903     -91.6521139    -208.9910692     234.6779106 \n\n\nLa más habitual, y por ello aparece la primera, es la Pseudo \\(R^2\\) de McFadden. Otras habituales son la de Cox y Snell y la de Nagelkerke. Esta última es más efectiva ya que la de Cox y Snell nunca puede alcanzar el 1.\nEl Pseudo \\(R^2\\) de McFadden se puede obtener “a mano” así:\n\nnull &lt;- glm(diag ~ 1, \n            data = cleveland, \n            family = \"binomial\", )\n1 - logLik(glm.diag) / logLik(null)\n\n'log Lik.' 0.5614544 (df=8)\n\n\nPredicción\nCon la función predict() se pueden obtener predicciones dados nuevos valores de los predictores. El argumento type admite varias opciones: por defecto type = \"link\" genera el logit (log odds), type = \"response\" genera probabilidades del tipo \\(P(Y = 1|X)\\). Si no se proporcionan nuevos datos se calculan las predicciones para los datos utilizados para ajustar el modelo de regresión logística (fitted.values).\n\npred.diag.prob &lt;- predict(glm.diag, type = \"response\") #probabilidades\npred.diag.prob[1:10]\n\n         1          2          3          4          5          6          7 \n0.15181263 0.99122791 0.99037412 0.15534739 0.04205422 0.14512306 0.99981411 \n         8          9         10 \n0.09995984 0.75135234 0.98779120 \n\npred.diag.link &lt;- predict(glm.diag, type = \"link\") #predictor lineal\npred.diag.link[1:10]\n\n        1         2         3         4         5         6         7         8 \n-1.720455  4.727369  4.633628 -1.693262 -3.125831 -1.773375  8.590146 -2.197671 \n        9        10 \n 1.105838  4.393314 \n\n\nComo se puede apreciar probabilidades bajas se asocian a predictores negativos, mientras que probabilidades altas se asocian a predictores positivos. Para obtener una predicción concreta se deben proporcionar valores a las variables explicativas incluidas en el modelo ajustado:\n\npaciente1 &lt;- data.frame(edad = 50, dep = 3, sexo = \"0\", tdolor = \"1\", dhosp = 1)\npaciente2 &lt;- data.frame(edad = 50, dep = 3, sexo = \"1\", tdolor = \"1\", dhosp = 2)\npacientes &lt;- data.frame(rbind(paciente1, paciente2))\npredict(glm.diag,\n        newdata = pacientes,\n        type = \"link\")\n\n        1         2 \n-1.770740  0.493137 \n\npredict(glm.diag,\n        newdata = pacientes,\n        type = \"response\")\n\n        1         2 \n0.1454504 0.6208451 \n\n\nLa diferencia entre los dos pacientes se localiza en sexo y que uno de ellos tiene 1 día más de hospitalización, lo que tiene un gran impacto en la respuesta predicha, ya sea la predicción de su log(odds) o la de la probabilidad de ser diagnosticado/a con la enfermedad.\nClasificación\nCon estas predicciones no se está realizando clasificación alguna. Para ello se debe escoger un valor umbral de probabilidad o punto de corte con el que clasificar, bien escogido arbitrariamente, bien obtenido por el análisis de la curva ROC. Aquí, tomamos el valor de ejemplo mencionado en la parte teórica: 0.6.\n\npred.diag &lt;- rep(\"0\", length(cleveland$diag))\npunto.corte &lt;- 0.6\npred.diag[pred.diag.prob &gt; punto.corte] = \"1\"\n\nMatriz de confusión\nAhora podemos comparar estas predicciones con las etiquetas verdaderas de la variable diag, esto es, construir la matriz de confusión (que depende del punto de corte escogido)\n\naddmargins(table(pred.diag, cleveland$diag))\n\n         \npred.diag   0   1 Sum\n      0   153  24 177\n      1    11 115 126\n      Sum 164 139 303\n\n\nLa clasificación es bastante satisfactoria, se han obtenido 153+115 aciertos (diagonal principal de la matriz de confusión) y sólo se han cometido 24+11 errores de clasificación, lo que supone una tasa de error de 35/303 (precisión de 268/303):\n\nmean(pred.diag !=  cleveland$diag)\n\n[1] 0.1155116\n\nmean(pred.diag ==  cleveland$diag)\n\n[1] 0.8844884\n\n\nVista la proporción, más de 1 de cada 10 casos clasificados erróneamente, los resultados no parecen tan satisfactorios. Aunque, sin duda, es mejor que un clasificador aleatorio:\n\nset.seed(pi)\npred.diag.aleat &lt;- sample(c(0,1), \n                          size = length(cleveland$diag), \n                          replace = TRUE)\naddmargins(table(pred.diag.aleat, cleveland$diag))\n\n               \npred.diag.aleat   0   1 Sum\n            0    73  68 141\n            1    91  71 162\n            Sum 164 139 303\n\n1 - mean(pred.diag.aleat ==  cleveland$diag)\n\n[1] 0.5247525\n\n\nSensibilidad y especificidad Además de la precisión (porcentaje total de aciertos) o la equivalente tasa de error, de la matriz de confusión se pueden obtener las métricas sensibilidad y especificidad, entre otras.\n\nlibrary(caret)\n\nCargando paquete requerido: ggplot2\n\n\nCargando paquete requerido: lattice\n\n\n\nAdjuntando el paquete: 'caret'\n\n\nThe following objects are masked from 'package:DescTools':\n\n    MAE, RMSE\n\nconfusionMatrix(table(pred.diag, cleveland$diag),\n                positive = \"1\")\n\nConfusion Matrix and Statistics\n\n         \npred.diag   0   1\n        0 153  24\n        1  11 115\n                                         \n               Accuracy : 0.8845         \n                 95% CI : (0.843, 0.9182)\n    No Information Rate : 0.5413         \n    P-Value [Acc &gt; NIR] : &lt; 2e-16        \n                                         \n                  Kappa : 0.7657         \n                                         \n Mcnemar's Test P-Value : 0.04252        \n                                         \n            Sensitivity : 0.8273         \n            Specificity : 0.9329         \n         Pos Pred Value : 0.9127         \n         Neg Pred Value : 0.8644         \n             Prevalence : 0.4587         \n         Detection Rate : 0.3795         \n   Detection Prevalence : 0.4158         \n      Balanced Accuracy : 0.8801         \n                                         \n       'Positive' Class : 1              \n                                         \n\n\nNota: Es importante la indicación de la clase positiva, dado que el cálculo de la sensibilidad, especificidad y valores relaciones depende de ello.\nLos resultados arrojan unos valores de precisión, sensibilidad y especificidad buenos.\n\nLas referencias de valores “buenos” son subjetivas. Y sujetas al contexto. Habitualmente por encima del 80% son buenos datos. Pero hay que tener en cuenta los costes y riesgos de una mala clasificación (por ejemplo, dar un tratamiento médico cuando no hace falta, y no darlo cuando sí hace falta).\n\nEs importante indicar que son necesarias las distintas métricas, dado que un buen dato de precisión podría estar ocultando información muy importante. Pongamos de ejemplo un caso extremo. Tenemos diez pacientes, 2 de ellos están enfermos, y 8 están sanos. Supongamos que un modelo de regresión logística clasifica a todos los pacientes como sanos. Entonces, la precisión y la especificidad serían del 80% y el 100% respectivamente. Pero la sensibilidad sería del 0%, y el modelo sería prácticamente inútil para diagnosticar la enfermedad, o en términos más técnicos, la capacidad discriminante del modelo sería muy baja.\nCurva ROC La manera habitual de seleccionar el punto de corte es acudir al análisis de la curva ROC.\n\npar(mfrow = c(1, 2))\nlibrary(Epi)\ngraf &lt;- ROC(\n  form = diag ~ ., data = cleveland,\n  plot = \"ROC\", las = 1\n)\n\n\n\n\n\n\n\n\nLa curva ROC asociada a nuestro modelo de regresión logística muestra esa idea de buen clasificador. La curva se encuentra claramente por encima de la diagonal, mostrando un calidad de clasificación mucho mejor que si lo hiciéramos aleatoriamente. El AUC es de 0.942, y que tiene sentido interpretar comparándolo con el de otros modelos, o métodos de clasificación. Entre otras cosas fácilmente deducibles, en la figura aparece el valor óptimo para el punto de corte, el que produce los valores indicados de sensibilidad, especificidad, etc.\n\npred.diag &lt;- rep(\"0\", length(cleveland$diag))\npunto.corte &lt;- 0.554 #valor redondeado\npred.diag[pred.diag.prob &gt; punto.corte] = \"1\"\nconfusionMatrix(table(pred.diag, cleveland$diag),\n                positive = \"1\")\n\nConfusion Matrix and Statistics\n\n         \npred.diag   0   1\n        0 153  22\n        1  11 117\n                                          \n               Accuracy : 0.8911          \n                 95% CI : (0.8505, 0.9238)\n    No Information Rate : 0.5413          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.7794          \n                                          \n Mcnemar's Test P-Value : 0.08172         \n                                          \n            Sensitivity : 0.8417          \n            Specificity : 0.9329          \n         Pos Pred Value : 0.9141          \n         Neg Pred Value : 0.8743          \n             Prevalence : 0.4587          \n         Detection Rate : 0.3861          \n   Detection Prevalence : 0.4224          \n      Balanced Accuracy : 0.8873          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nEn la comprobación se obtienen los valores mostrados en la curva ROC.\n\n\n3.3.3 Regresión de Poisson\nAprovechamos el mismo conjunto de datos, cleveland, para ajustar una regresión de Poisson, aprovechando que la variable dhosp (número de días de hospitalización de un paciente) toma valores numéricos discretos y tiene sentido modelizarla mediante una distribución de Poisson. Intentaremos encontrar el modelo de regresión de Poisson que mejor explique dhosp a partir del resto de variables incluidas en el conjunto de datos.\nAnálisis exploratorio\nVisualizamos la relación de cada una de las variables explicativas con la respuesta mediante la función ggpairs() del paquete GGally:\n\nGGally::ggpairs(cleveland,\n                mapping = ggplot2::aes(colour = diag))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nDe toda la salida anterior nos interesa focalizarnos en la última fila de gráficos, en los que dhosp se visualiza en el eje Y. El primer gráfico muestra indicios de que la variable diag influye en los días de hospitalización, al mostrar una distribución diferente de los gráficos de barras. Lo mismo parece suceder en el cuarto gráfico, en el que se tiene en el eje X la variable sexo, por lo que también parece indicar que influye en dhosp. Hay que tener en cuenta que al incluir como color la variable diag, este cuarto gráfico y el resto, son más difíciles de visualizar, el color aporta cierta distorsión. También en el quinto gráfico de la última linea, donde se tiene tdolor como variable explicativa, se aprecia distinta distribución de los diagramas de barras, especialmente del último valor de tdolor respecto al resto, aunque hay que hacer notar que la variable tdolor no está “balanceada”. En cuanto a las dos variables numéricas, edady dep, las nubes de puntos sugieren una tendencia, pero con mucha dispersión entorno a ella.\nRecordemos que esta visualización “individual” de cada uno de los predictores frente a la respuesta, es una visión parcial del problema de regresión múltiple (sea regresión general o regresión generalizada).\nAjuste e interpretación Empezamos considerando el modelo en formato de fórmula de R: dhosp ~ .. donde manejamos predictores de distinto tipo (diag es dicotómica, edad numérica, y tdolor categórica), lo que permite ilustrar las distintas interpretaciones de sus parámetros.\n\nglm.dhosp &lt;- glm(dhosp ~ .,\n                 data = cleveland, \n                 family = \"poisson\")\nsummary(glm.dhosp)\n\n\nCall:\nglm(formula = dhosp ~ ., family = \"poisson\", data = cleveland)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.2168175  0.3324906  -0.652   0.5143    \ndiag1        1.1138690  0.1127728   9.877   &lt;2e-16 ***\nedad         0.0005531  0.0049004   0.113   0.9101    \ndep         -0.0313456  0.0359078  -0.873   0.3827    \nsexo1        0.2141697  0.1033855   2.072   0.0383 *  \ntdolor2      0.1137591  0.2074925   0.548   0.5835    \ntdolor3      0.1145403  0.1896331   0.604   0.5458    \ntdolor4      0.1180416  0.1786624   0.661   0.5088    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 510.75  on 302  degrees of freedom\nResidual deviance: 322.48  on 295  degrees of freedom\nAIC: 985.63\n\nNumber of Fisher Scoring iterations: 5\n\nexp(coef(glm.dhosp))\n\n(Intercept)       diag1        edad         dep       sexo1     tdolor2 \n  0.8050769   3.0461210   1.0005533   0.9691406   1.2388328   1.1204822 \n    tdolor3     tdolor4 \n  1.1213578   1.1252909 \n\n\nLa función glm() tiene implementada una función de enlace para cada distribución que se especifique mediante el argumento family. Para el caso family = \"poisson\" la función de enlace es el logaritmo (efectos multiplicativos), aunque existe la posibilidad de aportar otra función de enlace.\nRespecto a la interpretación de los parámetros estimados, primero, sólo los predictores diag y sexo son significativos (al 5%), confirmando lo intuido de modo parcial en el diagrama de dispersión, que edad no influye en la respuesta, y tdolor tampoco por el desbalanceo entre las categorías.\nY segundo, la interpretación se debe hacer sobre la exponencial de las estimaciones, para tener las mismas unidades que la respuesta, en este caso, días de hospitalización. También decir que se predicen valores medios y que la interpretación de cada parámetro es ceteris paribus. Entrando en detalle, para las dos variables significativas, ambas de tipo dicotómico:\n\nEl parámetro estimado para diag1 es 1.1139 y su exponencial es 3.05. Por tanto, si se tiene diag=1 en lugar de diag=0 (categoría tomada como referencia, al aparecer diag1 en la salida), el número medio de días de estancia en el hospital será 3.05 veces mayor .\nPara sexo, lo expresamos de otro modo equivalente: un hombre se espera que esté en el hospital 1.24 días por cada día que esté una mujer (sexo=0 que es la categoría tomada como referencia).\n\nAunque las otras variables no son significativas, si lo fueran:\n\nlas variables edad y dep al ser numéricas, su interpretación se hace considerando el cambio de 1 unidad de su valor (o el cambio que sea de interés). Por ejemplo, para edad, para una diferencia de 1 año entre dos pacientes, el número medio de días en el hospital se ve multiplicado por 1.0006 (impacto ínfimo, también aunque la diferencia de edad fuese de 15 años -como en el ejemplo de regresión logística-: por cada día que esté hospitalizado el más joven, el de 15 años más, estará \\(e^{\\beta_{edad}*15}=\\) 1.0083 días). Nótese que, al ser un valor del odds ratio tan próximo a 1 (o el valor del parámetro estimado tan cercano a 0), se tiene un indicio de no significatividad, el intervalo de confianza del odds ratio contendrá al 1 (el intervalo de confianza del parámetro contendrá al 0). Para la interpretación de dep también habría que tener en cuenta su pequeño rango de valores.\nSobre la variable tdolor: al aparecer tdolor2, etc. se intuye que la categoría de referencia es tdolor = 1. Así que, de nuevo, la interpretación se hará expresando los días que, en media, estaría el paciente de cada categoría por cada día que estuviese un paciente de la categoría tdolor=1. Se puede observar que los pacientes de las categorías 2, 3 y 4 estarían, en torno a 1.12 días por cada día de los pacientes de la categoría 1. Todos aproximadamente el mismo tiempo y cercano a 1, lo que es indicio de que la variable no presenta diferencias significativas entre categorías, como se ha mencionado anteriormente.\n\nVistos los resultados, consideramos el modelo en el que se dejan sólo las variables significativas:\n\nglm.dhosp2 &lt;- glm(dhosp ~ diag + sexo,\n                  data = cleveland,   \n                  family = \"poisson\")\nsummary(glm.dhosp2)\n\n\nCall:\nglm(formula = dhosp ~ diag + sexo, family = \"poisson\", data = cleveland)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.09798    0.09895  -0.990   0.3221    \ndiag1        1.09449    0.09361  11.692   &lt;2e-16 ***\nsexo1        0.20820    0.10133   2.055   0.0399 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 510.75  on 302  degrees of freedom\nResidual deviance: 323.80  on 300  degrees of freedom\nAIC: 976.95\n\nNumber of Fisher Scoring iterations: 5\n\nexp(coef(glm.dhosp2))\n\n(Intercept)       diag1       sexo1 \n   0.906665    2.987658    1.231461 \n\n\nNótese que las estimaciones cambian ligeramente respecto al primer modelo estimado (excepto el término independiente, que no es de interés).\nAdecuación Como en la regresión logística, vamos a aplicar la función anova() para valorar la adecuación del modelo, de las variables incluidas.\n\nanova(glm.dhosp, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: dhosp\n\nTerms added sequentially (first to last)\n\n       Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    \nNULL                     302     510.75             \ndiag    1  182.581       301     328.17  &lt; 2e-16 ***\nedad    1    0.116       300     328.06  0.73356    \ndep     1    0.947       299     327.11  0.33047    \nsexo    1    4.166       298     322.94  0.04125 *  \ntdolor  3    0.467       295     322.48  0.92618    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(glm.dhosp2, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: dhosp\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    \nNULL                   302     510.75             \ndiag  1  182.581       301     328.17  &lt; 2e-16 ***\nsexo  1    4.369       300     323.80  0.03661 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa conclusión es clara, en el primer modelo, al introducir las variables en ese orden, algunas no aportan al ajuste, lo que tiene un impacto en el AIC (véase en la salida anterior de summary()). El segundo modelo más apropiado, con un AIC ligeramente menor.\nAdemás, en los ajustes glm.dhosp y glm.dhosp2 hay información sobre las deviance del modelo Null y la del modelo estimado, Residual deviance, con las que se puede realizar el contraste de comparación de modelos (el simple frente al elaborado) mencionado en la Sec. @ref(secADECUACION).\n\npchisq(glm.dhosp$deviance, glm.dhosp$df.residual, lower.tail = F)\n\n[1] 0.130278\n\npchisq(glm.dhosp2$deviance, glm.dhosp2$df.residual, lower.tail = F)\n\n[1] 0.1649776\n\n\nAl ser los p-valores superiores a 0.05, ambos modelos se puede considerar que explican “mejor” (globalmente) que el modelo nulo, pero hemos visto que el segundo, con sólo variables significativas, es más adecuado.\nPredicción\nUna vez comprobada la adecuación del modelo que presenta el mejor ajuste, se pueden proporcionar predicciones de la variable respuesta, en nuestro ejemplo, número medio de días de hospitalización.\n\npacientes &lt;- data.frame(diag = c(\"1\", \"1\", \"0\", \"0\"),\n                        sexo = c(\"1\", \"0\", \"1\", \"0\"))\npredict(glm.dhosp2, pacientes, type = \"response\")\n\n       1        2        3        4 \n3.335788 2.708805 1.116523 0.906665 \n\n\nSe han escogido adecuadamente los valores de las 2 variables explicativas para 4 pacientes: los pacientes 1 y 2 presentan enfermedad coronaria (diag=1) mientras que los pacientes 3 y 4 no; los pacientes 1 y 3 son hombres (sexo=1), mientras que el 2 y el 4 son mujeres. El resto de variables no se proporcionan, al no estar incluidas en el modelo. Las predicciones obtenidas indican que el paciente 1 (hombre con enfermedad coronaria) estará hospitalizado más días, en media, que el resto, seguido de cerca por la paciente 2 (mujer con enfermedad coronaria).\nTambién se pueden dibujar las predicciones de todo el conjunto de datos, de las 303 observaciones, que glm() ha guardado como fitted.values. Para el modelo glm.dhosp2 sería:\n\ncleveland$hat &lt;- glm.dhosp2$fitted.values\nggplot(cleveland, aes(x = edad, y = hat, colour = diag)) +\n  geom_point() +\n  labs(x = \"Edad\", y = \"Días hospitalización\")\n\n\n\n\n\n\nPredicciones de todo el conjunto de datos del modelo con sólo variables significativas (izquierda) y el modelo completo (derecha).\n\n\n\n\nEn el gráfico de la izquierda se visualiza la diferencia encontrada en las predicciones de los 4 pacientes calculados anteriormente. Mientras que en el gráfico de la derecha se aprecian “dispersiones” respecto a los valores horizontales predichos por sexo y edad, provocadas por el resto de variables no significativas incluidas en el modelo completo.\n\n\n3.3.4 Otros métodos de clasificación\nEn el libro “ISLR2” se puede encontrar código de R para aplicar otros métodos de clasificación al mismo conjunto de datos Smarket. Concretamente:\n\nAnálisis Discriminante Lineal (LDA: Linear Discriminant Analysis), y su comparación con la regresión logística.\nAnálisis Discriminante Cuadrático (QDA: Quadratic Discriminant Analysis)\nNaive Bayes\nKNN (K vecinos más próximos, \\(K\\)-Nearest Neighbors)\n\nTambién se aplica KNN al conjunto de datos Caravan, comentando la necesidad del escalado de variables para aplicar este método, y comparando sus resultados con la regresión logística.\nBiblio: FDCR cap 16: https://cdr-book.github.io/cap-glm.html\n\n\n\n\nCasero-Alonso, Víctor, y María Durbán. 2024. «Modelos lineales generalizados». En Fundamentos de Ciencia de Datos con R. McGraw Hill. https://cdr-book.github.io/cap-glm.html.\n\n\nFernández-Avilés, Gema, y José-María Montero. 2024. Fundamentos de Ciencia de Datos con R. McGraw Hill. https://cdr-book.github.io/index.html.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, y Robert Tibshirani. 2013. An introduction to statistical learning: with applications in R. Second. Vol. 103. Springer. https://www.statlearning.com/.\n\n\nMcCullagh, P., y J. A. Nelder. 1989. Generalized Linear Models. Second. Vol. 37. Monographs on Statistics y Applied Probability. Chapman; Hall.\n\n\nPeña, Daniel. 2002. Regresión y diseño de experimentos. Alianza Editorial.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales generalizados</span>"
    ]
  },
  {
    "objectID": "Cap4-Superv.html",
    "href": "Cap4-Superv.html",
    "title": "4  Análisis de supervivencia o fiabilidad",
    "section": "",
    "text": "Conceptos básicos\nEstimador de Kaplan-Meier\nRegresión de cox\nReferencia: Capítulo de Víctor Casero y Emilio López Cano curso editorial panamericana",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Análisis de supervivencia o fiabilidad</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html",
    "href": "Cap5-Seleccion.html",
    "title": "5  Selección de variables",
    "section": "",
    "text": "5.1 Métodos de selección\nCuando en un problema de regresión se dispone de un elevado número de variables explicativas es posible que algunas de ellas no estén correlacionadas con la variable respuesta o su relación no resulte estadísticamente significativa. También puede existir el problema de la colinealidad, con una o varias variables explicativas que contienen información redundante respecto a otras variables explicativas. En estos casos, es deseable encontrar un subconjunto de variables que expliquen suficientemente bien la respuesta, o que generen mejores predicciones que el modelo completo. Estos hechos justifican la necesidad de realizar una selección apropiada de variables.\nLa tarea de generar y evaluar todos los modelos posibles que combinan subconjuntos de variables puede ser, sin duda, ingente. Siendo \\(k\\) el número de variables, el número de modelos posibles es \\(2^k\\). Por ello, en la práctica se recurre a seleccionar un conjunto reducido de modelos candidatos, entre los cuales se elige el “mejor”. Esta elección puede basarse en los enfoques explicativo o predictivo, como se ha mencionado, aunque el más habitual es el enfoque predictivo, que será en el que nos centraremos. Es por ello, que aquí no se prestará atención al análisis de residuos para validar el modelo, dado que la validación vendrá dada por la calidad de las predicciones que realice el modelo obtenido.\nEsta selección de variables es especialmente útil cuando el número de variables es mayor que el número de observaciones (\\(k &gt; n\\)), situación común en algunos campos científicos, como la genómica, el tratamiento de imágenes médicas, etc. Es más, con más variables que observaciones es imposible estimar de forma única todos los parámetros del modelo completo.\nDos buenas referencias para este capítulo son Durbán (2024) y James et al. (2013).\nSe han desarrollado distintos procedimientos de selección, automática, de variables explicativas, que permitan obtener un modelo de regresión óptimo (desde alguna perspectiva).\nUno de los métodos más utilizados, por su simplicidad y eficacia, es la regresión paso a paso (stepwise regression). Este método permite construir el modelo de forma progresiva, evaluando el impacto de cada variable. Se basa en añadir, o eliminar, al modelo variables explicativas en función de su contribución estadística al modelo, evaluada mediante algún criterio (que veremos en el siguiente apartado). Este método surge como una alternativa computacionalmente menos costosa al método de selección del mejor subconjunto (Best Subset Selection). Este último método evalúa (exhaustivamente) todas los combinaciones posibles de modelos con un número fijo de variables (por ejemplo, todos los modelos con 4 variables, que serían \\({k \\choose 4}\\), o con 5 variables \\({k \\choose 5}\\), etc., de las \\(k\\) variables disponibles).\nOtro conjunto de métodos, muy utilizados en “Machine Learning”, son los métodos de regularización, que consisten en penalizar la complejidad del modelo para evitar el sobreajuste, es decir, reducen el impacto de las variables menos relevantes. Entre ellos veremos la Ridge Regression y Lasso.\nLos métodos anteriores se suelen combinar con un procedimiento muy difundido y utilizado en “Machine Learning”, el método de validación cruzada. Consiste en evaluar los modelos con diferentes subconjuntos de variables mediante técnicas como k-fold cross-validation. Para la elección del mejor modelo se utiliza el enfoque de predicción, acudiendo a la minimización del error de predicción en un subconjunto de los datos previamente reservados, no usados para el ajuste, para el que se usa el resto de los datos (enfoque de datos de entrenamiento y datos de validación).\nOtra opción muy habitual es reducir la dimensión del problema, la dimesión de las variables explicativas. No es estrictamente un método de selección, sino que el objetivo es proyectar las \\(k\\) variables explicativas en un subespacio de dimensión más pequeña (mediante componentes principales: combinaciones lineales de las variables explicativas originales).\nPor último hay que mencionar los métodos basados en árboles. Pertenecen a este grupo las técnicas de Árboles de decisión, Random Forests, y Gradient Boosting, entre otros. Hay que comentar que estos no son métodos de regresión lineal, aunque ayudan a identificar variables relevantes, pues suelen proporcionar medidas de la importancia de las variables, lo que permite al usuario elegir qué variables seleccionar.\nDe todos los mencionados, nos centraremos en los 3 primeros grupos.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html#métodos-de-selección",
    "href": "Cap5-Seleccion.html#métodos-de-selección",
    "title": "5  Selección de variables",
    "section": "",
    "text": "La selección automática con cualquiera de estos procedimientos debe estar sujeta al contexto y a la lógica del problema. Que el procedimiento lleve a que una variable influye significativamente en la respuesta, pero que no tenga sentido práctico puede deberse a la realización de numerosos contrastes de significación, donde, a base de intentos, se obtienen más contrastes significativos de los reales, por ejemplo, al 5% de significación se rechazan (en media) 5 de cada 100 contrastes.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html#criterios-de-selección",
    "href": "Cap5-Seleccion.html#criterios-de-selección",
    "title": "5  Selección de variables",
    "section": "5.2 Criterios de selección",
    "text": "5.2 Criterios de selección\nPara elegir qué modelo es “mejor” de entre los candidatos, se acude a criterios de selección, que generalmente pueden estar basados, bien, en la bondad del ajuste del modelo a los datos (tienden a escoger modelos sobreajustados, con más parámetros de los necesarios); o bien, en la capacidad predictiva del modelo. En la práctica se utilizan los segundos, y principalmente:\n\nAIC (Akaike Information Criterion)\nBIC (Bayesian Information Criterion)\nC\\(_p\\) de Mallows\n\nEstos 3 criterios (y otros) se pueden expresarse en una forma general que reflejan un balance entre la bondad de ajuste del modelo y su complejidad (basada en el número de parámetros). Concretamente:\n\\[\\text{Criterio} = n \\cdot \\ln(\\hat{\\sigma}^2) + \\lambda(p)\\]\nDonde:\n\n\\(n\\): número de observaciones\n\n\\(\\hat{\\sigma}^2\\): estimación MV de la varianza residual del modelo completo\n\\(\\lambda(p)\\): penalización por complejidad del modelo, que depende de \\(p\\), el número de parámetros a estimar (incluyendo la constante), y que varía según el criterio:\n\nPara AIC: \\(\\lambda(p) = 2p\\)\nPara BIC: \\(\\lambda(p) = \\ln(n)p\\)\nPara C\\(_p\\): se reestructura como penalización sobre \\(\\text{RSS}_p\\), la suma de cuadrados de los residuos del modelo con \\(p\\) parámetros: \\[C_p = \\frac{\\text{RSS}_p}{\\hat{\\sigma}^2} - n + 2p\\]\n\n\n\nUna deducción muy detallada del estadístico C\\(_p\\) de Mallows puede encontrarse en Peña (2002) (apartado 11.3.2 y Apéndice 11A). Donde se demuestra que el criterio de minimizar C\\(_p\\) es equivalente al criterio AIC.\n\nCon la penalización basada en el número de parámetros se busca combatir modelos sobreajustados. El criterio BIC penaliza más la complejidad que AIC cuando el tamaño muestral es “grande” (basta \\(n&gt;7\\) dado que \\(\\ln(8) &gt; 2\\)), por lo que BIC es un método más parsimonioso, tiende a proporcionar modelos más simples (más cuanto mayor sea \\(n\\)).\nEstos criterios son medidas relativas, se utilizan para comparaciones entre modelos, que podrían ser todos “malos”. El modelo preferido será el que tenga menor valor del criterio.\n\n¡Ojo! Los criterios mencionados no permiten una comparación válida entre modelos cuya variable respuesta difiere.\n\nTambién existen otros criterios basados en medidas de bondad del ajuste del modelo, tales como el coeficiente de determinación (R\\(^2\\)), su versión corregida (R\\(^2\\) ajustado) o la varianza residual. Sin embargo, estos criterios únicamente permiten comparaciones, en igualdad de condiciones, entre modelos que poseen el mismo número de parámetros, por lo que su uso debe realizarse con cautela.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html#selección-paso-a-paso",
    "href": "Cap5-Seleccion.html#selección-paso-a-paso",
    "title": "5  Selección de variables",
    "section": "5.3 Selección paso a paso",
    "text": "5.3 Selección paso a paso\nEl método de regresión paso a paso tiene dos variantes:\n\nSelección hacia adelante (forward): se parte del modelo nulo —que solo incluye la constante, sin variables explicativas— y se van incorporando variables explicativas, una a una. En cada paso la que más “mejore” el modelo previo y hasta, o bien un modelo maximal propuesto (con aquellas variables que se consideren oportunas), o bien el modelo completo (que incluye todas las variables).\n\nEn formato algoritmo\nSiendo \\(k\\) el número máximo de variables a considerar, bien sea del modelo maximal o del completo.\nPaso 1. Sea \\(M_0\\) el modelo nulo.\nPaso 2. Para \\(i = 0, 1, \\dots, k - 1\\) , de los \\(k - i\\) modelos que se obtienen al añadir una variable explicativa adicional a las ya incluidas en \\(M_i\\), seleccionamos el mejor, y lo denotamos por \\(M_{i+1}\\).\nAquí “mejor” es el modelo que produce el mayor incremento de variabilidad explicada, \\(R^2\\), al añadir sólo una de las variables que todavía no han entrado al modelo.\n\nEn las salidas de software se puede reconocer dicha variable como la de mayor valor del estadístico \\(t\\) (de las restantes variables).\n\nPaso 3. El proceso finaliza escogiendo uno de los \\(k\\) modelos (\\(M_0, \\ldots, M_k\\)) según uno de los criterios mencionados: AIC, BIC, etc.\nLa ventaja computacional es que el número de modelos a evaluar, \\(1 + \\frac{k(k + 1)}{2}\\), es menor que en la selección del mejor subconjunto, \\(2^k\\), cuanto mayor es el número de variables disponibles. Pero tiene una desventaja, no garantiza el mejor modelo posible de todos (por ejemplo, si existe multicolinealidad).\n\nSelección hacia atrás (backward): se parte del modelo maximal (o el modelo completo), y se van eliminando, una a una, la variable que menos pérdida supongan para el modelo. La de menor valor del estadístico \\(t\\) asociado (que no sea significativa). El proceso finaliza eligiendo de entre los \\(k\\) modelos, el de menor AIC, o BIC,… etc.\n\nComo la selección hacia adelante evaluará \\(1 + \\frac{k(k + 1)}{2}\\) modelos (ventaja computacional frente a otros métodos), pero no garantiza encontrar el mejor modelo. Además, se añade la limitación (que puede ser importante) de necesitar un número de observaciones mayor que el número de variables \\(n&gt;k\\), de modo que el modelo completo (o, en su caso, el maximal considerado) pueda ajustarse. En contraste, la selección hacia adelante sí puede utilizarse incluso cuando \\(n &lt; p\\), lo que le otorga una ventaja competitiva, pues le convierte en el único método viable cuando \\(k\\) es muy grande.\nAlgunos paquetes de software incluyen también la selección bidireccional, combinación de ambos enfoques, permitiendo tanto la inclusión como la eliminación de variables en cada iteración, según su contribución al modelo. Obviamente es el procedimiento más flexible y suele ofrecer mejores resultados en la práctica.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html#selección-por-regularización",
    "href": "Cap5-Seleccion.html#selección-por-regularización",
    "title": "5  Selección de variables",
    "section": "5.4 Selección por regularización",
    "text": "5.4 Selección por regularización\nLos métodos de selección basados en regularización (denominados en inglés Shrinkage -contracción-) se basan en modificar el procedimiento de estimación de mínimos cuadrados de los parámetros añadiendo un término de penalización sobre la magnitud de los parámetros. Los parámetros estimados se reducen/contraen hacia cero (“regularizan”) en comparación con las estimaciones por mínimos cuadrados, provocando una disminución en la varianza de los parámetros estimados del modelo. Buscan así, mejorar la capacidad predictiva del modelo y controlar el sobreajuste de los mismos. Por contra, pueden no seleccionar las variables de forma explícita como los métodos anteriores. Eso sí, en contextos de alta dimensión (\\(k &gt;n\\)) no se puede aplicar directamente mínimos cuadrados porque la matriz de diseño no tiene rango completo.\nEn este capítulo vamos a ver los dos más importantes: Regresión Ridge y Regresión Lasso.\nRegresión Ridge\nEn este método se añade el término de penalización, suma de los cuadrados de los parámetros (es decir aplica una norma \\(L^2\\)). Puede no llegar a relizar una selección efectiva de variables, es decir, no llegar a eliminar las variables menos relevantes, pero reduce su impacto (al reducir las estimaciones de los parámetros).\nSu objetivo es minimizar la siguiente función: \\[\\text{RSS} + \\lambda \\sum_{j=1}^{k} \\beta_j^2 = \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{k} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{k} \\beta_j^2\\]\ndonde - RSS es la suma de cuadrados de los residuos, - \\(\\lambda\\) es el parámetro de penalización, un parámetro ajustable/ elegible, que controla la intensidad de la penalización. Si \\(\\lambda = 0\\) no hay penalización y los estimadores son los de mínimos cuadrados. - \\(\\beta_j\\) son los parámetros del modelo.\n\nA la vista de la fórmula, el procedimiento no es invariante a la escala de los predictores, a diferencia del estimador de mínimos cuadrados ordinarios. Por ello, se recomienda estandarizar las variables para aplicar regresión Ridge.\n\nLasso\nEl método Lasso (Least Absolute Shrinkage and Selection Operator) penaliza mediante la suma de los valores absolutos de los parámetros (es decir considera una norma \\(L_1\\)): \\[\\text{RSS} + \\lambda \\sum_{j=1}^{k} |\\beta_j| \\] Tiene la propiedad de forzar algunos parámetros a ser exactamente cero, cuando el parámetro \\(\\lambda\\) es suficientemente grande, lo que implica una selección de variables efectiva, mejorando la predicción, y aumentando la interpretabilidad del modelo.\nComparativa\nAunque ninguno de los dos métodos domina universalmente al otro, en determinados contextos se prefiere uno u otro. La regresión lasso es preferible cuando se espera que únicamente un pequeño subconjunto de predictores sea relevante, mientras que la regresión ridge es útil cuando todos los predictores (o la mayoría) contribuyen. Así, Ridge permite, “colateralmente”, ayudar con el problema de la colinealidad de variables explicativas, regularizando sus estimaciones.\nElección de \\(\\lambda\\)\nLa eficacia de la selección mediante Ridge o Lasso depende de la elección adecuada del parámetro \\(\\lambda\\). En la práctica se utiliza validación cruzada para determinar el valor óptimo que minimiza el error de predicción.\nInterpretación geométrica\nComo penalizaciones de tipo norma \\(L^2\\) o norma \\(L_1\\), se tiene una interpretación geométrica de las restricciones que se imponen. Para Ridge, la restricción es una región esférica, mientras que para Lasso es una región en forma de rombo, favoreciendo soluciones sparse (dispersas). Una explicación detallada y la correspondiente figura ilustrativa (que se incluye abajo) se pueden encontrar, tanto en James et al. (2013), como en Durbán (2024).\n\nOtros métodos de regularización\nPor no extendermos más, no entraremos en otros métodos de regularización. El lector interesado puede buscar información sobre Elastic Net (que es una combinación lineal de Ridge y Lasso, especialmente útil cuando se sabe que hay muchas variables correlacionadas), Adaptive Lasso, Group Lasso, etc.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html#selección-por-validación-cruzada.",
    "href": "Cap5-Seleccion.html#selección-por-validación-cruzada.",
    "title": "5  Selección de variables",
    "section": "5.5 Selección por validación cruzada.",
    "text": "5.5 Selección por validación cruzada.\nEl procedimiento de selección de variables por validación cruzada (cross-validation) es una técnica de remuestreo ampliamente utilizada en “Machine Learning”, con un claro enfoque predictivo de los modelos, en lugar de explicativo (como se ha visto en el capítulo anterior). Su objetivo principal es evaluar la capacidad de generalización de un modelo, es decir, cómo se comporta al predecir datos no utilizados durante el ajuste.\nLa idea básica consiste en particionar (aleatoriamente) el conjunto de datos en dos subconjuntos disjuntos: uno para el entrenamiento (train), donde se ajusta el modelo, y otro para la validación (test), donde se evalúa su rendimiento. Esta partición suele ser desbalanceada, reservando típicamente un 75%, 80% o 90% de los datos para entrenamiento, y el restante 25%, 20% o 10% para validación (evaluación de su capacidad predictiva). En la práctica, el error de entrenamiento puede calcularse fácilmente a partir de las observaciones utilizadas en su entrenamiento. Pero, dicha tasa de error de entrenamiento tiende a subestimar la tasa de error del conjunto de validación, especialmente si el conjunto de entrenamiento tiene pocas observaciones, lo que puede llevar a invalidar la generalización del modelo obtenido por entrenamiento. Por otro lado, interesa que la tasa de error sea lo menor posible en dicho conjunto de validación. Pero recordemos que la partición es aleatoria, por lo que dicha tasa de error puede variar con otra partición. Por lo que el resultado debe basarse en un consenso claro tras los resultados de varias particiones.\nPues bien, el procedimiento de validación cruzada viene a aportar una solución para la búsqueda de ese consenso. La partición a realizar se hace de forma cruzada, dividiendo (aleatoriamente) el conjunto total de datos en \\(k\\) subconjuntos (en inglés se les denomina folds) con el mismo tamaño (o aproximadamente). En cada iteración, se selecciona uno de los subconjuntos como conjunto de validación y se utilizan los \\(k-1\\) restantes para entrenamiento. Este proceso se repite \\(k\\) veces, de modo que cada observación se utiliza una vez para validar y \\(k-1\\) veces para entrenar. Se dice por ello que el método es eficiente al utilizar todos los datos tanto para entrenamiento como para validación.\nCada modelo ajustado (fijado el número de variables) se evalúa mediante una métrica de error, como el Error Cuadrático Medio (MSE: Mean Squared Error) para problemas de regresión, que mide la precisión de las predicciones. Finalmente, se calcula el promedio de los \\(k\\) MSEs (o la métrica elegida) para seleccionar el modelo que minimiza el error de predicción.\nElección del número de folds\nHabitualmente se utiliza \\(k = 5\\) ó \\(10\\) (5 o 10-fold cross-validation), pues ofrecen un buen equilibrio entre sesgo y varianza del modelo a generalizar. Otros valores de \\(k\\), tienden a producir estimaciones con alta varianza (cuando \\(k\\) es pequeño) o se vuelven computacionalmente costosos (cuanto mayor es \\(k\\)), aunque con menor sesgo.\nEstimación final\nUna vez identificado el número óptimo de variables mediante validación cruzada, se ajusta dicho modelo sobre el conjunto completo de datos. Esto permite obtener estimaciones más precisas de los parámetros, ya que el mejor modelo de \\(p\\) variables en el conjunto completo puede diferir de los modelos obtenidos en cada subconjunto de entrenamiento.\n\nEn Peña (2002) también se aborda la validación del modelo aunque no desde el enfoque mencionado arriba.\n\n\n5.5.1 Leave-One-Out CV\nLa estimación LOOCV (validación cruzada dejando uno fuera) es un caso particular de validación cruzada en el que, como su nombre indica, el número de folds coincide con el número total de observaciones. Esto lleva a un coste computacional considerable si el número de observaciones, \\(n\\) es grande, al tener que entrenar \\(n\\) modelos. Ahora bien, para modelos lineales estimados mediante mínimos cuadrados existe un fórmula que ahorra tiempo de cálculo de LOOCV, pues se puede obtener al ajustar un sólo modelo: \\[ LOOCV(n) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i}{1 - h_i} \\right)^2\\] donde:\n\n\\(\\hat{y}_i\\) es el valor ajustado para la observación \\(i\\) en el modelo original,\n\\(h_i\\) es la influencia o leverage , que para el caso de regresión simple sería: \\[hi = \\dfrac{1}{n}+\\dfrac{(x_i − \\bar{x})^2}{\\sum_{j=1}^n(x_{j} − \\bar{x})^2}\\]\n\nComo se puede apreciar la fórmula es la del MSE ordinario salvo la “normalización” de cada \\(i\\)-ésimo residuo considerado su leverage \\(h_i\\), que recoge lo que influye cada observación en el ajuste del modelo.\n\nEl leverage varía entre \\(1/n\\) y \\(1\\), y así, las observaciones con alto leverage se inflan en esta fórmula del LOCCV.\n\nEl LOOCV tiene, en general, menos sesgo y no tiende a sobeestimar la tasa de error de validación, que el enfoque de un sólo conjunto de entrenamiento y validación. Y, por construcción, su resultado es siempre el mismo para el conjunto de datos considerado.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html#caso-práctico-boston",
    "href": "Cap5-Seleccion.html#caso-práctico-boston",
    "title": "5  Selección de variables",
    "section": "5.6 Caso práctico: Boston",
    "text": "5.6 Caso práctico: Boston\nSe van a ver 4 métodos de selección, a saber:\n\nSelección stepwise (incluye selección del mejor subconjunto de variables)\nSelección mediante validación cruzada\nRegresión ridge\nRegresión lasso\n\nEl propósito es obtener un modelo de predicción para medv a partir de las variables que más influyan en ella, utilizando los distintos métodos.\n\n5.6.1 Selección stepwise\nComo se ha mencionado en la parte teórica, los métodos de selección de variables stepwise proporcionan el “mejor” modelo, obtenido por adicción (forward) o eliminación (backward) de las variables explicativas.\nEn R, como se está viendo, existen distintas funciones que permiten aplicar estos procedimientos:\n\npaquete base de R, función step().\npaquete MASS, función stepAIC().\npaquete leaps, función regsubsets().\npaquete StepReg, función stepwise().\n\nAlgunas funciones, como step() o stepAIC() utilizan el criterio de información de Akaike (AIC) para la selección del modelo. En otras funciones se puede elegir otro criterio. También algunas permiten la selección bidireccional. En los libros James et al. (2013) y Fernández-Avilés y Montero (2024) se puede encontrar ejemplos de utilización de la función regsubsets().\n\n\n5.6.2 step()\nCon esta función se obtiene la aplicación más simple y directa de este método:\n\nlibrary(ISLR2)\n# redefinimos la variable `chas` al ser dicotómica\nBoston$chas &lt;- factor(Boston$chas)\nrlm &lt;- lm(medv ~ . , data = Boston)\nstep.rlm &lt;- step(rlm)\n\nStart:  AIC=1599.85\nmedv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + \n    tax + ptratio + lstat\n\n          Df Sum of Sq   RSS    AIC\n- indus    1      1.08 11350 1597.9\n- age      1      1.69 11351 1597.9\n&lt;none&gt;                 11349 1599.8\n- chas     1    245.31 11595 1608.7\n- tax      1    256.28 11606 1609.2\n- zn       1    263.59 11613 1609.5\n- crim     1    311.49 11661 1611.6\n- rad      1    430.71 11780 1616.7\n- nox      1    546.10 11896 1621.6\n- ptratio  1   1157.70 12507 1647.0\n- dis      1   1258.52 12608 1651.1\n- rm       1   1744.36 13094 1670.2\n- lstat    1   2733.54 14083 1707.0\n\nStep:  AIC=1597.9\nmedv ~ crim + zn + chas + nox + rm + age + dis + rad + tax + \n    ptratio + lstat\n\n          Df Sum of Sq   RSS    AIC\n- age      1      1.69 11352 1596.0\n&lt;none&gt;                 11350 1597.9\n- chas     1    251.21 11602 1607.0\n- zn       1    262.99 11614 1607.5\n- tax      1    299.68 11650 1609.1\n- crim     1    313.07 11664 1609.7\n- rad      1    453.61 11804 1615.7\n- nox      1    574.23 11925 1620.9\n- ptratio  1   1168.01 12518 1645.5\n- dis      1   1333.19 12684 1652.1\n- rm       1   1750.50 13101 1668.5\n- lstat    1   2743.21 14094 1705.4\n\nStep:  AIC=1595.98\nmedv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + \n    lstat\n\n          Df Sum of Sq   RSS    AIC\n&lt;none&gt;                 11352 1596.0\n- chas     1    254.21 11606 1605.2\n- zn       1    261.75 11614 1605.5\n- tax      1    298.57 11651 1607.1\n- crim     1    313.27 11666 1607.8\n- rad      1    452.16 11804 1613.7\n- nox      1    601.74 11954 1620.1\n- ptratio  1   1168.51 12521 1643.5\n- dis      1   1496.35 12848 1656.6\n- rm       1   1848.38 13201 1670.3\n- lstat    1   3043.23 14395 1714.2\n\n\nComo se puede ver al consultar la ayuda de la función, por defecto muestra todos los pasos dados para la obtención del mejor modelo (trace=1) y realiza la selección en ambas direcciones (\"both\").\nLa comparativa de modelos hasta llegar al “mejor” se guarda en el value $anova y el modelo ajustado final, junto con su bondad de ajuste, etc. se puede obtener con la conocida función summary():\n\nstep.rlm$anova\n\n     Step Df Deviance Resid. Df Resid. Dev      AIC\n1         NA       NA       493   11349.42 1599.855\n2 - indus  1 1.081197       494   11350.50 1597.903\n3   - age  1 1.686474       495   11352.19 1595.978\n\nsummary(step.rlm)\n\n\nCall:\nlm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + \n    tax + ptratio + lstat, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1814  -2.7625  -0.6243   1.8448  26.3920 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.451747   4.903283   8.454 3.18e-16 ***\ncrim         -0.121665   0.032919  -3.696 0.000244 ***\nzn            0.046191   0.013673   3.378 0.000787 ***\nchas1         2.871873   0.862591   3.329 0.000935 ***\nnox         -18.262427   3.565247  -5.122 4.33e-07 ***\nrm            3.672957   0.409127   8.978  &lt; 2e-16 ***\ndis          -1.515951   0.187675  -8.078 5.08e-15 ***\nrad           0.283932   0.063945   4.440 1.11e-05 ***\ntax          -0.012292   0.003407  -3.608 0.000340 ***\nptratio      -0.930961   0.130423  -7.138 3.39e-12 ***\nlstat        -0.546509   0.047442 -11.519  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.789 on 495 degrees of freedom\nMultiple R-squared:  0.7342,    Adjusted R-squared:  0.7289 \nF-statistic: 136.8 on 10 and 495 DF,  p-value: &lt; 2.2e-16\n\n\n\nPregunta\n¿Obtiene el mismo “mejor” modelo final utilizando el metodo hacia atrás? ¿Y utilizando el método hacia adelante? ¿A qué conclusión llega con ello?\nNota: Para aplicar el método hacia adelante debe definir bien el scope.\n\nLa mencionada función MASS::stepAIC() es una implementación más sofisticada que step() de este método, que permite un mayor rango de objetos (como glm que veremos en capítulos posteriores). Más info/ejemplos en: https://fhernanb.github.io/libro_regresion/selec.html\n\n\n5.6.3 StepReg::stepwise()\nEl paquete StepReg es considerado por muchos usuarios el más completo y flexible para selección stepwise, dado que se puede aplicar a distintos tipos de modelos (todos los que veremos en esta asignatura, lm, glm… y más: cox …). Ofrece una gran variedad de criterios de selección, más de los vistos en la parte teórica. Por último, implementa las direcciones habituales (\"forward\", \"backward\", \"bidirection\") y la selección del mejor subconjunto (\"subset\", al igual que hace la función leaps::regsubsets(), que permiten seleccionar el mejor modelo de 1, 2, 3, … variables). Más info en la ayuda de la función ?stepwise o, mucho mejor, en su vignette: https://cran.r-project.org/web/packages/StepReg/vignettes/StepReg.html que incluso contiene las fórmulas y referencias bibliográficas de los criterios seleccionables (AIC, BIC, etc.)\nAplicando esta función a nuestros caso práctico obtenemos:\nNota: ¡Ojo! Es posible que algunas de las funciones tarden unos segundos en completar su ejecución.\n\nlibrary(StepReg)\nBoston.Step &lt;- stepwise(formula = medv ~ .,\n                        data = Boston,\n                        type = \"linear\",\n                        strategy = c(\"forward\", \"backward\"),\n                        metric = c(\"AIC\", \"BIC\"))\n#Boston.Step #Muestra los 4 ajustes obtenidos\n#Mostramos aquí el ajuste \"extendido\" de uno de ellos\nsummary(Boston.Step$forward$BIC)\n\n\nCall:\nlm(formula = medv ~ 1 + lstat + rm + ptratio + dis + nox + chas + \n    zn + crim + rad + tax, data = data, weights = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1814  -2.7625  -0.6243   1.8448  26.3920 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.451747   4.903283   8.454 3.18e-16 ***\nlstat        -0.546509   0.047442 -11.519  &lt; 2e-16 ***\nrm            3.672957   0.409127   8.978  &lt; 2e-16 ***\nptratio      -0.930961   0.130423  -7.138 3.39e-12 ***\ndis          -1.515951   0.187675  -8.078 5.08e-15 ***\nnox         -18.262427   3.565247  -5.122 4.33e-07 ***\nchas1         2.871873   0.862591   3.329 0.000935 ***\nzn            0.046191   0.013673   3.378 0.000787 ***\ncrim         -0.121665   0.032919  -3.696 0.000244 ***\nrad           0.283932   0.063945   4.440 1.11e-05 ***\ntax          -0.012292   0.003407  -3.608 0.000340 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.789 on 495 degrees of freedom\nMultiple R-squared:  0.7342,    Adjusted R-squared:  0.7289 \nF-statistic: 136.8 on 10 and 495 DF,  p-value: &lt; 2.2e-16\n\nplot(Boston.Step, strategy = \"forward\", process = \"overview\")\n\n\n\n\n\n\n\n#plot(Boston.Step, strategy = \"forward\", process = \"details\")\n\nEn este ejemplo, se obtiene el mismo modelo final (y por tanto las mismas estimaciones), aplicando tanto paso a paso hacia adelante, como hacia atrás, y tanto con el criterio de selección AIC como con el BIC. Obviamente, esto no ocurre siempre.\nVeamos ahora un ejemplo de aplicación de \"subset\":\n\nBoston.Subset &lt;- stepwise(formula = medv ~ .,\n                          data = Boston,\n                          type = \"linear\",\n                          strategy = c(\"subset\"),\n                          metric = \"AIC\")\nBoston.Subset\n\n$subset\n$subset$AIC\n\nCall:\nlm(formula = medv ~ 1 + crim + zn + chas + nox + rm + dis + rad + \n    tax + ptratio + lstat, data = data, weights = NULL)\n\nCoefficients:\n(Intercept)         crim           zn        chas1          nox           rm  \n   41.45175     -0.12166      0.04619      2.87187    -18.26243      3.67296  \n        dis          rad          tax      ptratio        lstat  \n   -1.51595      0.28393     -0.01229     -0.93096     -0.54651  \n\n\nLa salida anterior muestra el mismo “mejor” modelo obtenido anteriormente. Ahora bien, internamente se han generado los mejores modelos para cada número determinado de variables (1 variable, 2 variables, etc.). En la viñeta de la función se puede encontrar el siguiente código que permite visualizarlo:\n\nplot_list &lt;- setNames(\n  lapply(c(\"subset\"),function(i){\n    setNames(\n      lapply(c(\"details\",\"overview\"),function(j){\n        plot(Boston.Subset,strategy=i,process=j)\n    }),\n    c(\"details\",\"overview\")\n    )\n  }),\n  c(\"subset\")\n)\ncowplot::plot_grid(\n  plotlist = plot_list$subset,\n  ncol = 1,\n  rel_heights = c(2, 1)\n)\n\n\n\n\n\n\n\n\nAsí, el mejor modelo con “2” variables será el que contiene el término independiente 1 y la variable lstat; el mejor de “3” variables incluye las “2” anteriores y rm, etc. Así, el mejor modelo con hasta “5” variables será (en formato R) 1 + lstat + rm + ptratio + dis.\n\nPregunta\n¿Cómo obtener ahora los parámetros del mejor modelo ajustado de \\(k\\) variables?\n\n\n\n5.6.4 Selección mediante validación cruzada\nCambiamos el orden respecto a la teoría, para aplicar \\(k\\)-fold validación cruzada para la selección de variables. Lo aplicamos, como anteriormente, al conjunto de datos Boston.\nDe nuevo, en R existen distintos paquetes y funciones que permiten aplicar validación cruzada en problemas de regresión:\n\npaquete boot, función cv.glm().\npaquete caret, funciones train() junto con trainControl().\npaquete mlr, función resample().\n…\n\nVeamos un ejemplo con la función cv.glm() del paquete boot, que usaremos también en un capítulo posterior.\nNota: Al haber un componente aleatorio en la partición de los subconjuntos de entrenamiento y validación, es importante utilizar la función set.seed() que establece la semilla de aleatorización que permite al lector la reproducibilidad de resultados.\nAdemás, es posible que algunas de las funciones tarden unos segundos en completar su ejecución.\n\n# Establecemos semilla de aleatorización\nset.seed(pi^2)\n# Ajustamos un modelo de regresión lineal completo\n# con la función glm() que veremos en detalle en otro capítulo\n# y que es necesaria para la función cv.glm\nmodelo.completo &lt;- glm(medv ~ ., \n                       family = gaussian,\n                       data = Boston)\nsummary(modelo.completo)\n\n\nCall:\nglm(formula = medv ~ ., family = gaussian, data = Boston)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.617270   4.936039   8.431 3.79e-16 ***\ncrim         -0.121389   0.033000  -3.678 0.000261 ***\nzn            0.046963   0.013879   3.384 0.000772 ***\nindus         0.013468   0.062145   0.217 0.828520    \nchas1         2.839993   0.870007   3.264 0.001173 ** \nnox         -18.758022   3.851355  -4.870 1.50e-06 ***\nrm            3.658119   0.420246   8.705  &lt; 2e-16 ***\nage           0.003611   0.013329   0.271 0.786595    \ndis          -1.490754   0.201623  -7.394 6.17e-13 ***\nrad           0.289405   0.066908   4.325 1.84e-05 ***\ntax          -0.012682   0.003801  -3.337 0.000912 ***\nptratio      -0.937533   0.132206  -7.091 4.63e-12 ***\nlstat        -0.552019   0.050659 -10.897  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 23.02113)\n\n    Null deviance: 42716  on 505  degrees of freedom\nResidual deviance: 11349  on 493  degrees of freedom\nAIC: 3037.8\n\nNumber of Fisher Scoring iterations: 2\n\n# procedimiento de validación cruzada\nlibrary(boot)\ncv.10fold &lt;- cv.glm(Boston, modelo.completo, K = 10)\n# Estimación del error de predicción\ncv.10fold$delta\n\n[1] 23.79801 23.72486\n\n\nComo “resumen” del proceso de \\(10\\)-fold validación cruzada obtenemos los valores delta, estimaciones del promedio de error de predicción, la primera es la estimación estándar y la segunda es una versión con corrección de sesgo. Estos 2 datos por sí solos no aportan información. Comparemoslos con los de una validación cruzada de \\(2\\)-fold y con el MSE del modelo nulo (al que siempre se puede acudir, en ausencia de variables explicativas).\n\nset.seed(pi^2)\n# 2-fold CV\ncv.2fold &lt;- cv.glm(Boston, modelo.completo, K = 2)\n# Estimación del error de predicción\ncv.2fold$delta\n\n[1] 27.05146 25.34112\n\n# MSE modelo nuelo\nvar(Boston$medv)\n\n[1] 84.58672\n\n\nLos valores de delta para \\(2\\)-fold CV son obviamente peores que para \\(10\\)-fold CV, y ambos, muchísimo mejores que el MSE del modelo nulo con los datos completos.\n\nPregunta\n¿Porqué intuitivamente se supone que un \\(2\\)-fold CV proporciona peores errores que un \\(10\\)-fold CV? Pruebe a realizar un \\(15\\)-fold CV y compruebe si mejora el MSE del \\(10\\)-fold CV. Pruebe con distintas semillas de aleatorización para comprobar la dependencia de los resultados respecto a la semilla proporcionada.\n\nProbemos a realizar otra \\(10\\)-fold CV ajustando otro modelo, para intentar comprobar si baja el error de predicción.\n\nset.seed(pi^2)\n# Ajustamos el modelo de regresión obtenido con stepwise()\n# de nuevo con la función glm()\nformula.step &lt;- Boston.Step$forward$AIC$call$formula\nmodelo.step &lt;- glm(formula.step, \n                   family = gaussian, \n                   data = Boston)\nsummary(modelo.step)\n\n\nCall:\nglm(formula = formula.step, family = gaussian, data = Boston)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.451747   4.903283   8.454 3.18e-16 ***\nlstat        -0.546509   0.047442 -11.519  &lt; 2e-16 ***\nrm            3.672957   0.409127   8.978  &lt; 2e-16 ***\nptratio      -0.930961   0.130423  -7.138 3.39e-12 ***\ndis          -1.515951   0.187675  -8.078 5.08e-15 ***\nnox         -18.262427   3.565247  -5.122 4.33e-07 ***\nchas1         2.871873   0.862591   3.329 0.000935 ***\nzn            0.046191   0.013673   3.378 0.000787 ***\ncrim         -0.121665   0.032919  -3.696 0.000244 ***\nrad           0.283932   0.063945   4.440 1.11e-05 ***\ntax          -0.012292   0.003407  -3.608 0.000340 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 22.93371)\n\n    Null deviance: 42716  on 505  degrees of freedom\nResidual deviance: 11352  on 495  degrees of freedom\nAIC: 3033.9\n\nNumber of Fisher Scoring iterations: 2\n\n# Validación cruzada\ncv.10fold &lt;- cv.glm(Boston, modelo.step, K = 10)\ncv.10fold$delta\n\n[1] 23.55353 23.49375\n\n\nComo se puede ver, el error de predicción baja ligeramente respecto al modelo completo.\nEstimación LOOCV\nCon la función cv.glm() también se puede obtener la estimación LOOCV, aunque lamentablemente no tiene implementada la fórmula “abreviada” para LOOCV, por lo que el coste computacional puede ser excesivo.\n\nset.seed(pi)\n# LOOCV\ncv.LOOCV &lt;- cv.glm(Boston, modelo.completo)\ncv.LOOCV$delta\n\n[1] 24.15953 24.15777\n\n\nOtros ejemplos\nEn los libros James et al. (2013) y Fernández-Avilés y Montero (2024) se puede encontrar ejemplos en los que se genera código para la realización de la partición “a mano”, promediar los errores de predicción, etc. utilizando la función regsubsets() también mencionada en el apartado anterior.\n\n\n5.6.5 Regresiones ridge y lasso (regularización)\nPara ajustar modelos de regresión ridge y lasso utilizaremos la función glmnet() del paquete glmnet.\nLos argumentos a proporcionar a la función glmnet() difieren de lo visto hasta ahora, en lugar de proporcionarle una fórmula de tipo y ~ x1 + x2, se le debe proporcionar el vector de respuestas y la matriz de variables explicativas. Además, por defecto, la función estandariza las variables para que estén en la misma escala (elemento clave en las regresiones ridge y lasso como se ha mencionado, y que puede desactivarse: standardize = FALSE). También glmnet() incluye el argumento alpha que determina el tipo de modelo a ajustar. Cuando se establece alpha = 0, se ajusta un modelo de regresión ridge; en cambio, si se estable alpha = 1, se ajusta un modelo de regresión lasso (y cualquier valor intermedio entre 0 y 1 equivale a una combinación lineal entre ridge y lasso, esto es, a aplicar una elastic net) Por último, el argumento lambda permite introducir los valores deseados del parámetro de penalización (por defecto toma 100 valores).\n\nObservación: una restricción de la función glmnet() es que sólo puede manejar entradas numéricas, por lo que las variables cualitativas deben convertirse en variables dummys, por ejemplo, mediante la función factor().\n\nComenzamos ajustando un modelo de regresión ridge como primer paso.\n\nlibrary(glmnet)\n\nCargando paquete requerido: Matrix\n\n\nLoaded glmnet 4.1-8\n\nx &lt;- as.matrix(Boston[, -13])  # Explórese la función `model.matrix()`\ny &lt;- Boston$medv               \nmod.ridge &lt;- glmnet(x, y, alpha = 0)\n\nLa mejor manera de mostrar la información recogida en el objeto mod.ridge (y recomendada por los creadores del paquete) es utilizando las funciones plot(), print(), coef() y predict():\n\nplot(mod.ridge, xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\nCada curva corresponde a una variable (etiquetadas en este caso), en función del logaritmo de \\(\\lambda\\), el parámetro de penalización. A mayor penalización, los parámetros tienden a cero, siendo la variable 5 la que más tarda en tender hacia cero. A menor penalización, las estimaciones de los parámetros se asemejan a las de mínimos cuadrados (técnicamente cuando \\(\\lambda = 0\\)). En el eje horizontal superior se indican los grados de libertad del modelo, número de parámetros distintos de cero (12 en todos los casos).\n\nprint(mod.ridge) #equivalente a mod.ridge\n\n\nCall:  glmnet(x = x, y = y, alpha = 0) \n\n    Df  %Dev Lambda\n1   12  0.00 6778.0\n2   12  0.76 6176.0\n3   12  0.83 5627.0\n4   12  0.91 5127.0\n5   12  1.00 4672.0\n6   12  1.10 4257.0\n7   12  1.20 3878.0\n8   12  1.32 3534.0\n9   12  1.44 3220.0\n10  12  1.58 2934.0\n11  12  1.73 2673.0\n12  12  1.89 2436.0\n13  12  2.07 2219.0\n14  12  2.27 2022.0\n15  12  2.48 1843.0\n16  12  2.71 1679.0\n17  12  2.96 1530.0\n18  12  3.24 1394.0\n19  12  3.54 1270.0\n20  12  3.86 1157.0\n21  12  4.22 1054.0\n22  12  4.60  960.7\n23  12  5.01  875.4\n24  12  5.46  797.6\n25  12  5.95  726.7\n26  12  6.47  662.2\n27  12  7.03  603.4\n28  12  7.64  549.8\n29  12  8.29  500.9\n30  12  8.99  456.4\n31  12  9.74  415.9\n32  12 10.54  378.9\n33  12 11.39  345.3\n34  12 12.29  314.6\n35  12 13.25  286.6\n36  12 14.27  261.2\n37  12 15.34  238.0\n38  12 16.46  216.8\n39  12 17.64  197.6\n40  12 18.87  180.0\n41  12 20.15  164.0\n42  12 21.48  149.5\n43  12 22.86  136.2\n44  12 24.28  124.1\n45  12 25.73  113.1\n46  12 27.22  103.0\n47  12 28.74   93.9\n48  12 30.28   85.5\n49  12 31.84   77.9\n50  12 33.41   71.0\n51  12 35.00   64.7\n52  12 36.58   59.0\n53  12 38.16   53.7\n54  12 39.73   48.9\n55  12 41.29   44.6\n56  12 42.84   40.6\n57  12 44.36   37.0\n58  12 45.87   33.7\n59  12 47.34   30.7\n60  12 48.78   28.0\n61  12 50.19   25.5\n62  12 51.57   23.2\n63  12 52.90   21.2\n64  12 54.20   19.3\n65  12 55.45   17.6\n66  12 56.65   16.0\n67  12 57.81   14.6\n68  12 58.92   13.3\n69  12 59.97   12.1\n70  12 60.98   11.1\n71  12 61.93   10.1\n72  12 62.83    9.2\n73  12 63.68    8.4\n74  12 64.48    7.6\n75  12 65.22    6.9\n76  12 65.91    6.3\n77  12 66.56    5.8\n78  12 67.16    5.2\n79  12 67.71    4.8\n80  12 68.22    4.4\n81  12 68.69    4.0\n82  12 69.12    3.6\n83  12 69.52    3.3\n84  12 69.88    3.0\n85  12 70.21    2.7\n86  12 70.52    2.5\n87  12 70.79    2.3\n88  12 71.05    2.1\n89  12 71.28    1.9\n90  12 71.49    1.7\n91  12 71.69    1.6\n92  12 71.86    1.4\n93  12 72.02    1.3\n94  12 72.17    1.2\n95  12 72.30    1.1\n96  12 72.42    1.0\n97  12 72.53    0.9\n98  12 72.63    0.8\n99  12 72.72    0.7\n100 12 72.80    0.7\n\n\nEn esta salida se obtienen los grados de libertad, el % de devianza explicada y el valor de \\(\\lambda\\). Se aprecia que los grados de libertad siempre son 12, la regresión ridge, como se ha comentado, no realiza selección de variables efectiva. El % de devianza y Lambda son inversamente proporcionales, conforme disminuye lambda, la penalización, aumenta el % de devianza explicada. Como se ha mencionado el ajuste es para 100 valores de lambda (calculados en función del número de variables y observaciones, y de alpha, entre otros), aunque glmnet() se detendría antes de tiempo si % de devianza no cambia lo suficiente de una lambda a otro.\n\ncoef(mod.ridge, s = c(0.7, 6778))\n\n13 x 2 sparse Matrix of class \"dgCMatrix\"\n                       s1            s2\n(Intercept)  32.580969595  2.253281e+01\ncrim         -0.099950913 -4.193841e-37\nzn            0.032571142  1.435758e-37\nindus        -0.044924593 -6.550405e-37\nchas          3.040795956  6.410260e-36\nnox         -12.592691149 -3.425864e-35\nrm            3.902180496  9.194049e-36\nage          -0.001979554 -1.244068e-37\ndis          -1.117830946  1.102639e-36\nrad           0.137192219 -4.071671e-37\ntax          -0.006115460 -2.582636e-38\nptratio      -0.840383674 -2.178965e-36\nlstat        -0.492502012 -9.596458e-37\n\n\nLa función coef() permite obtener las estimaciones de los parámetros para todos los modelos de regresión ridge ajustados (los 100!!) o para un valor concreto de \\(\\lambda\\) de los predeterminados (aunque hay que especificarlos como s). Aquí se ha optado por mostrar los \\(\\lambda\\) más pequeño y más grande de la salida de print() para observar los distintos valores de las estimaciones de los parámetros.\nPara comparación, obtengamos también su norma \\(L^2\\):\n\nsqrt(sum(coef(mod.ridge, s = 0.7)^2))\n\n[1] 35.31004\n\nsqrt(sum(coef(mod.ridge, s = 6778)^2))\n\n[1] 22.53281\n\n\nComo era esperable, un \\(\\lambda\\) mayor implica un menor valor de norma \\(L^2\\).\n\nEl lector interesado puede explorar el funcionamiento de la función predict(), que permite obtener predicciones para cualquier valor de \\(\\lambda\\), y más detalles del paquete glmnet en https://glmnet.stanford.edu/articles/glmnet.html#linear-regression-family-gaussian-default\n\n\n5.6.5.1 Elección de \\(\\lambda\\) por CV\nElegir el mejor valor del parámetro de penalización, \\(\\lambda\\), a mano, de entre los 100 modelos obtenidos con la función glmnet(), puede ser tedioso. En la práctica se utiliza la función cv.glmnet() que permite seleccionar automáticamente el mejor valor de \\(\\lambda\\) por validación cruzada. De forma predeterminada, la función realiza una \\(10\\)-fold CV (ajustable con el argumento nfolds). Como todos los procedimientos de validación cruzada, para reproducibilidad de resultados, se debe establecer una semilla de aleatorización.\n\nset.seed(pi)\n# Ajuste modelo ridge por CV\nmod.cv.ridge &lt;- cv.glmnet(x, y,\n                          type.measure = \"mse\",\n                          alpha = 0)\n#Resumen de la CV\nprint(mod.cv.ridge)\n\n\nCall:  cv.glmnet(x = x, y = y, type.measure = \"mse\", alpha = 0) \n\nMeasure: Mean-Squared Error \n\n    Lambda Index Measure    SE Nonzero\nmin  0.678   100   25.57 4.194      12\n1se  5.248    78   29.59 5.586      12\n\n# Gráfico \nplot(mod.cv.ridge)\n\n\n\n\n\n\n\n\nCon el resumen se obtiene el valor mínimo (óptimo) de \\(\\lambda\\), y el mayor valor tal que su error se encuentra a 1 error standard del mínimo, 1se, que posteriormente aparecen marcados en el gráfico. El único “problema” es que en el resumen no aparecen en escala logarítmica, lo que suele confundir:\n\n# Mejor lambda por CV\nmod.cv.ridge$lambda.min\n\n[1] 0.6777654\n\nlog(mod.cv.ridge$lambda.min)\n\n[1] -0.3889541\n\nmod.cv.ridge$lambda.1se\n\n[1] 5.247691\n\nlog(mod.cv.ridge$lambda.1se)\n\n[1] 1.657788\n\n\nPor último, obtenemos las estimaciones de los parámetros del modelo para el mejor \\(\\lambda\\):\n\n# Estimaciones del modelo con el mejor lambda\ncoef(mod.cv.ridge, s = \"lambda.min\")\n\n13 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  32.751149447\ncrim         -0.100336258\nzn            0.032827923\nindus        -0.044165780\nchas          3.039078301\nnox         -12.715946236\nrm            3.899711111\nage          -0.001878906\ndis          -1.126495302\nrad           0.139516293\ntax          -0.006197994\nptratio      -0.842522564\nlstat        -0.494019020\n\n\nComo se ha comentado en la parte teórica, ninguna de las estimaciones de los parámetros es cero, ¡la regresión ridge no realiza una selección de variables efectiva!\n\nPregunta\n¿Se obtendrá menor MSE que el del apartado de validación cruzada con el modelo obtenido por stepwise? Ayuda:\n\n::: {.cell}\n\n```{.r .cell-code}\npred.ridge &lt;- predict(mod.cv.ridge, s = \"lambda.min\", newx = x)\nmean((pred.ridge - y)^2)\n```\n:::\n\n\n\n5.6.6 Lasso\nLa idea de aplicar lasso es que puede producir un modelo más reducido (parsimonioso, sparse) y por lo tanto, más interpretable que la regresión ridge.\nPara ajustar un modelo lasso utilizamos, como anteriormente, la función glmnet(), cambiando el argumento a alpha = 1.\n\nmod.lasso &lt;- glmnet(x, y, alpha = 1)\nplot(mod.lasso, label = TRUE) #por defecto xvar = norma L1\n\n\n\n\n\n\n\nplot(mod.lasso, xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\nEn este caso, se presentan dos gráficos que varian en los valores del eje X. En el primero se dibujan los coeficientes respecto de la norma \\(L_1\\), mientras que en el segundo se dibujan respecto al logaritmo de \\(\\lambda\\). Em ambos cada curva corresponde a una variable, y mirando el segundo, por comparación con el mostrado en la regresión ridge, ahora no es la variable 5 la última que tiende a cero, sino la variable 8 la última que se hace efectivamente cero, pues, como se ve, las curvas alcanzan el cero y se mantienen en él, mostrando como el modelo lasso (basado en la norma \\(L_1\\)) sí que realiza una selección efectiva de variables. Además, ahora en el eje horizontal superior sí que cambian los grados de libertad del modelo, siendo 0 el último valor, indicando que no queda ninguna variable explicativa, ni término independiente en el modelo. Y para el valor \\(\\log(\\lambda)=0\\) se puede ver que los grados de libertad son 4, por lo que para \\(\\lambda = e^1\\) sólo quedan 4 variables con parámetro no nulo en el modelo lasso. Obtengamos los parámetros para este caso:\n\ncoef(mod.lasso, s = exp(1))\n\n13 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept) 12.9983574\ncrim         .        \nzn           .        \nindus        .        \nchas         .        \nnox          .        \nrm           2.6290850\nage          .        \ndis          .        \nrad          .        \ntax          .        \nptratio     -0.1056136\nlstat       -0.3982620\n\n\nDonde se ve qué cuatro variables quedan en este modelo lasso, y sus correspondientes parámetros estimados.\nAl igual que en la regresión ridge, con print() se obtiene la tabla asociada al gráfico anterior. En ella se pueden ver los grados de libertad y el % de devianza explicada para cada valor de \\(\\lambda\\):\n\nprint(mod.lasso)\n\n\nCall:  glmnet(x = x, y = y, alpha = 1) \n\n   Df  %Dev Lambda\n1   0  0.00 6.7780\n2   1  9.24 6.1760\n3   2 17.38 5.6270\n4   2 25.27 5.1270\n5   2 31.82 4.6720\n6   2 37.26 4.2570\n7   2 41.78 3.8780\n8   2 45.52 3.5340\n9   2 48.64 3.2200\n10  3 51.56 2.9340\n11  3 54.33 2.6730\n12  3 56.62 2.4360\n13  3 58.53 2.2190\n14  3 60.12 2.0220\n15  3 61.43 1.8430\n16  3 62.52 1.6790\n17  3 63.43 1.5300\n18  3 64.18 1.3940\n19  3 64.81 1.2700\n20  3 65.33 1.1570\n21  3 65.76 1.0540\n22  4 66.19 0.9607\n23  4 66.63 0.8754\n24  5 67.02 0.7976\n25  5 67.36 0.7267\n26  5 67.64 0.6622\n27  6 67.88 0.6034\n28  7 68.13 0.5498\n29  8 68.52 0.5009\n30  8 69.08 0.4564\n31  8 69.55 0.4159\n32  8 69.94 0.3789\n33  8 70.26 0.3453\n34  9 70.56 0.3146\n35  9 70.87 0.2866\n36  9 71.12 0.2612\n37 10 71.34 0.2380\n38 10 71.53 0.2168\n39 10 71.68 0.1976\n40  9 71.80 0.1800\n41 11 72.00 0.1640\n42 11 72.24 0.1495\n43 11 72.45 0.1362\n44 11 72.61 0.1241\n45 11 72.75 0.1131\n46 11 72.86 0.1030\n47 11 72.96 0.0939\n48 11 73.04 0.0855\n49 11 73.10 0.0779\n50 11 73.16 0.0710\n51 10 73.20 0.0647\n52 10 73.24 0.0590\n53 10 73.27 0.0537\n54 10 73.30 0.0489\n55 10 73.32 0.0446\n56 10 73.34 0.0406\n57 10 73.35 0.0370\n58 10 73.36 0.0337\n59 10 73.37 0.0307\n60 10 73.38 0.0280\n61 10 73.39 0.0255\n62 10 73.39 0.0232\n63 11 73.40 0.0212\n64 11 73.40 0.0193\n65 11 73.41 0.0176\n66 11 73.41 0.0160\n67 11 73.41 0.0146\n68 11 73.42 0.0133\n69 11 73.42 0.0121\n70 11 73.42 0.0111\n71 11 73.42 0.0101\n72 12 73.42 0.0092\n73 12 73.42 0.0084\n74 12 73.42 0.0076\n75 12 73.43 0.0069\n76 12 73.43 0.0063\n77 12 73.43 0.0058\n\n\n\nRecuerdese que los valores de \\(\\lambda\\) los calcula la función glmnet() a partir del número de observaciones y variables, y de alpha, entre otros. Y que, en este caso, no llega a mostrar los 100 valores de \\(\\lambda\\) al no haber ganancia del % de devianza de un \\(\\lambda\\) a otro (técnicamente si el cambio fraccional en la devianza es inferior a \\(10^-5\\) o se alcanza el 99.9% de devianza explicada).\n\nComo en el regresión ridge, obtendremos los parámetros estimados para los dos casos extremos y calcularemos su norma \\(L_1\\):\n\ncoef(mod.lasso, s = 0.0058)\n\n13 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  41.065327780\ncrim         -0.119334087\nzn            0.045690527\nindus         0.004503582\nchas          2.850851523\nnox         -18.302389305\nrm            3.676787428\nage           0.002567541\ndis          -1.480569874\nrad           0.275178169\ntax          -0.011957982\nptratio      -0.930366766\nlstat        -0.549733110\n\nsqrt(sum(coef(mod.lasso, s = 0.0058)^2))\n\n[1] 45.23756\n\ncoef(mod.lasso, s = 6.7780)\n\n13 x 1 sparse Matrix of class \"dgCMatrix\"\n                  s1\n(Intercept) 22.53281\ncrim         .      \nzn           .      \nindus        .      \nchas         .      \nnox          .      \nrm           .      \nage          .      \ndis          .      \nrad          .      \ntax          .      \nptratio      .      \nlstat        .      \n\nsqrt(sum(coef(mod.lasso, s = 6.7780)^2))\n\n[1] 22.53281\n\n\n\n5.6.6.1 Elección de \\(\\lambda\\) por CV\nProcedemos a obtener por CV el mejor valor para \\(\\lambda\\) para el modelo lasso. Teniendo la precaución de cambiar type.measure para indicar la métrica MAE, la apropiada para la norma \\(L_1\\)\n\nset.seed(pi)\n# Ajuste modelo ridge por CV\nmod.cv.lasso &lt;- cv.glmnet(x, y,\n                          type.measure = \"mae\",\n                          alpha = 1)\n# Mejor lambda\nmod.cv.lasso$lambda.min\n\n[1] 0.07792655\n\nlog(mod.cv.lasso$lambda.min)\n\n[1] -2.551989\n\n# Gráfico \nplot(mod.cv.lasso)\n\n\n\n\n\n\n\n# Estimaciones del modelo con el mejor lambda\ncoef(mod.cv.lasso, s = \"lambda.min\")\n\n13 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  35.817147782\ncrim         -0.093523880\nzn            0.034046511\nindus        -0.002138392\nchas          2.764345929\nnox         -15.215905652\nrm            3.856744686\nage           .          \ndis          -1.249210756\nrad           0.157219959\ntax          -0.006877068\nptratio      -0.886616520\nlstat        -0.544438423\n\n\nAhora, para el mejor \\(\\lambda\\) obtenido por CV sí se obtienen dos estimaciones de parámetros con valor \\(0\\), los asociados a las variables indus y age (que ya habíamos visto que eran poco significativas). En este caso, la selección de variables para el mejor \\(\\lambda\\) no es importante, pero ya conocíamos que la mayoría de variables sí que influye significativamente en la respuesta. Eso sí, cambian las estimaciones respecto al modelo lineal con dichas variables:\n\nsummary(lm(medv ~ . - indus - age, \n           data = Boston))\n\n\nCall:\nlm(formula = medv ~ . - indus - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1814  -2.7625  -0.6243   1.8448  26.3920 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.451747   4.903283   8.454 3.18e-16 ***\ncrim         -0.121665   0.032919  -3.696 0.000244 ***\nzn            0.046191   0.013673   3.378 0.000787 ***\nchas1         2.871873   0.862591   3.329 0.000935 ***\nnox         -18.262427   3.565247  -5.122 4.33e-07 ***\nrm            3.672957   0.409127   8.978  &lt; 2e-16 ***\ndis          -1.515951   0.187675  -8.078 5.08e-15 ***\nrad           0.283932   0.063945   4.440 1.11e-05 ***\ntax          -0.012292   0.003407  -3.608 0.000340 ***\nptratio      -0.930961   0.130423  -7.138 3.39e-12 ***\nlstat        -0.546509   0.047442 -11.519  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.789 on 495 degrees of freedom\nMultiple R-squared:  0.7342,    Adjusted R-squared:  0.7289 \nF-statistic: 136.8 on 10 and 495 DF,  p-value: &lt; 2.2e-16\n\n\n\nPregunta\n¿Qué modelo es mejor para predecir: regresión ridge o lasso?\n\n\n\n\n5.6.7 Epílogo\nEn el material asociado al libro James et al. (2013) se pueden encontrar ejemplos de estas técnicas de selección de variables, con el conjunto de datos Hitters (véase https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch6-varselect-lab.html)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html#bibliografía",
    "href": "Cap5-Seleccion.html#bibliografía",
    "title": "5  Selección de variables",
    "section": "Bibliografía",
    "text": "Bibliografía\n\n\n\n\nDurbán, María. 2024. «Modelos sparse y métodos penalizados de regresión». En Fundamentos de Ciencia de Datos con R. McGraw Hill. https://cdr-book.github.io/cap-lm.html.\n\n\nFernández-Avilés, Gema, y José-María Montero. 2024. Fundamentos de Ciencia de Datos con R. McGraw Hill. https://cdr-book.github.io/index.html.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, y Robert Tibshirani. 2013. An introduction to statistical learning: with applications in R. Second. Vol. 103. Springer. https://www.statlearning.com/.\n\n\nPeña, Daniel. 2002. Regresión y diseño de experimentos. Alianza Editorial.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  }
]