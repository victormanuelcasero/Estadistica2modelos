[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estadística II: modelos",
    "section": "",
    "text": "Prefacio\nEste “Quarto Book” contiene el material preparado por el profesor para la asignatura Estadística II impartida en el tercer curso del Grado en Matemáticas de la Universidad de Castilla-La Mancha. Se trata de la tercera asignatura de “Estadística” que se cursa en el plan de estudios, después de Elementos de Probabilidad y Estadística y Estadística I, por lo que se asume que el estudiante posee conocimientos previos en probabilidad, estadística y manejo del software R.\nEl objetivo principal de esta asignatura es introducir al estudiante en el manejo de distintos tipos de modelos estadísticos. En concreto, se abordarán:\nCada uno de estos modelos podría constituir una asignatura completa. Por ello, para ajustarlos al formato cuatrimestral, se ha optado por tratar cada tema con distinto nivel de profundidad, buscando el difícil balance entre el rigor teórico y la aplicación práctica de estos modelos (ilustrada mediante el software R).\nEl contenido se basa en la experiencia docente del profesor y en diversas referencias bibliográficas, entre las que destacan:\nAunque este material podría haber incluido otros modelos (como regresión ordinal, modelos GAM,…), técnicas (árboles de decisión, bootstrap,…) o enfoques (machine learning, diseño óptimo de experimentos,…), su incorporación excedería el alcance de una asignatura cuatrimestral. No obstante, muchos de estos contenidos están presentes en la bibliografía citada, a la que el estudiante/lector puede acudir para ampliar conocimientos.\nEste material no está exento de errores o erratas.\nSe agradece cualquier comunicación al respecto para su corrección.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#software-r",
    "href": "index.html#software-r",
    "title": "Estadística II: modelos",
    "section": "Software: R",
    "text": "Software: R\nLa parte práctica de la asignatura se desarrollará utilizando el lenguaje de programación R (https://www.r-project.org/), en combinación con la interfaz RStudio.\n\n\n\n\n\n\nSe recomienda disponer de las versiones más recientes de ambos programas:\n\nDescarga de R: https://cloud.r-project.org/\n\nDescarga de RStudio: https://posit.co/downloads/\n\n\n\n\nManuales básicos\n\nR Para principiantes, E. Paradis.\n\nIntroducción a R\n\nMás documentación en: https://cran.r-project.org/other-docs.html\n\nBlog recomendado\n\nR Bloggers\n\nAyuda\nVéase la página de inicio de la ayuda en RStudio.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#agradecimientos",
    "href": "index.html#agradecimientos",
    "title": "Estadística II: modelos",
    "section": "Agradecimientos",
    "text": "Agradecimientos\nAunque el contenido de este material es fruto de mi experiencia docente, deseo expresar un agradecimiento especial a tres personas. En primer lugar, a mi compañero, y sin embargo amigo, Licesio J. Rodríguez-Aragón (UCLM), por haber compartido generosamente sus valiosos materiales y comentarios, que han sido fundamentales en la elaboración de este trabajo. En segundo lugar, a mi colega y también amiga María Durbán (UC3M), con quien he tenido la fortuna de colaborar en varios capítulos de libros sobre distintos temas de esta asignatura, y con quien he mantenido un diálogo constante y enriquecedor que me ha permitido profundizar y aprender sobre ellos. En tercer lugar, pero por ello no menos importante, a mi compañero, y por supuesto amigo, Sergio Pozuelo Campos (UCLM), que ha leído el material con gran detalle y dedicación, lo que ha dado lugar a conversaciones muy estimulantes y esclarecedoras sobre diversos aspectos del contenido.\nTambién quiero agradecer a los compañeros del Área de Estadística del Departamento de Matemáticas de la UCLM. En particular, a Mariano Amo Salas, por sus aportaciones, especialmente al apartado de análisis de supervivencia; y a Irene García Camacha, por su apoyo y sugerencias en la docencia del Grado en Matemáticas.\nTambién debo mencionar a Juan Manuel Rodríguez Díaz (USAL), compañero de grupo de investigación, por facilitarme sus materiales sobre modelos lineales y diseño de experimentos, que me han aportado numerosas ideas. Finalmente, agradezco a los autores de las referencias biblográficas que se citan (destaco a Román Salmerón Gómez -rnoremlas-), así como a los desarrolladores de los paquetes de R utilizados, por su trabajo y dedicación.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#bibliografía",
    "href": "index.html#bibliografía",
    "title": "Estadística II: modelos",
    "section": "Bibliografía",
    "text": "Bibliografía\n\n\n\n\nFaraway, Julian J. 2004. Linear Models with R. Chapman &amp; Hall/CRC.\n\n\nFernández-Avilés, Gema, y José-María Montero. 2024. Fundamentos de Ciencia de Datos con R. McGraw Hill. https://cdr-book.github.io/index.html.\n\n\nHarrell Jr., Frank E. 2015. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis. 2nd ed. Springer Series en Statistics. Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, y Robert Tibshirani. 2013. An introduction to statistical learning: with applications in R. 2nd ed. Vol. 103. Springer. https://www.statlearning.com/.\n\n\nPeña, Daniel. 2002. Regresión y diseño de experimentos. Alianza Editorial.\n\n\nRivas López, María Jesús, y Jesús López Fidalgo. 2000. Análisis de supervivencia. Vol. 10. Cuadernos de estadística. Editorial La Muralla.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "Cap1-LM.html",
    "href": "Cap1-LM.html",
    "title": "1  Modelos lineales",
    "section": "",
    "text": "1.1 Modelo estadístico de regresión\n(version “2025-09-24”)\nEn este tema se estudian los denominados modelos lineales. El caso paradigmático es la regresión lineal simple, caso particular del modelo de regresión lineal múltiple. Los modelos de diseño de experimentos (que se estudian en el Capítulo 2) también son modelos lineales. En ambos modelos, la variable denominada respuesta debe ser cuantitativa/numérica continua, a diferencia del modelo lineal generalizado (que se estudia en el Capítulo 3).\nUn par de buenas referencias bibliográficas para este tema son: Peña (2002), concretamente la segunda parte, capítulos 5 a 10, y Faraway (2004), que conjuga contenido matemático y práctico en los capítulos 1 a 7. Otra referencia, con un enfoque más aplicado, es el capítulo 15 “Modelización lineal” de Casero-Alonso y Durbán (2024), https://cdr-book.github.io/cap-lm.html del libro “Fundamentos de Ciencia de Datos con R”, Fernández-Avilés y Montero (2024), que denominamos en el texto como CDR.\nParafraseando a G.E.P. Box …Todos los modelos son falsos. Pero algunos son útiles… Es imposible describir la realidad de forma exacta mediante un modelo, pero puede ser útil utilizar un modelo aproximado, basado en los “datos”, que permita entender y explicar el fenómeno o experimento de interés. En matemáticas un modelo es una relación matemática, no necesariamente algebraica, que permite entender el fenómeno. En estadística, a los modelos matemáticos se les añade un término de error, para incluir lo que la parte estructural (algebraica) no es capaz de explicar. Este término de error se desea aleatorio, estocástico, y tiene el papel de “cajón de sastre”.\nEl modelo lineal de regresión lineal simple permite modelizar el comportamiento de una variable cuantitativa, denominada respuesta o dependiente, denotada por \\(\\mathbf{Y}\\), mediante una función lineal de otra variable cuantitativa, denominada explicativa, regresora o predictora, \\(\\mathbf{X}\\), que se supone está correlacionada con ella (correlación no implica causalidad). La forma habitual de expresar el modelo es: \\[ \\mathbf{Y} = \\beta_0 + \\beta_1 \\mathbf{X} + \\epsilon, \\qquad \\epsilon \\sim N(0, \\sigma^2)\\]\ndonde \\(\\mathbf{Y} = (y_1, y_2, \\ldots, y_n)^\\top\\) es el vector de las \\(n\\) observaciones de la variable respuesta, \\(\\mathbf{X} = (x_1, x_2, \\ldots, x_n)^\\top\\) es el vector de las \\(n\\) observaciones de la variable explicativa, \\(\\beta_i \\ (i=0 \\ ó \\ 1)\\) son los coeficientes o parámetros del modelo, y \\(\\epsilon=(\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n)\\) es el vector de errores aleatorios que convierte el modelo determinista (\\(\\beta_0 + \\beta_1 \\mathbf{X}\\)) en modelo estocástico. Los supuestos más habituales sobre el error son: tener esperanza cero, varianza constante y seguir una distribución normal.\nLa generalización a varias variables explicativas es inmediata: \\[ \\mathbf{Y} = \\beta_0 + \\beta_1 \\mathbf{X}_1 + \\ldots + \\beta_k \\mathbf{X}_k + \\epsilon, \\qquad \\epsilon \\sim N(0, \\sigma^2I).\\] A este modelo se le denomina modelo lineal de regresión lineal múltiple.\nUtilizando notación matricial: \\[\\mathbf{Y} = \\mathbf{X} \\beta + \\epsilon, , \\qquad \\epsilon \\sim N(0, \\sigma^2I) ;\\]\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1k} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2k} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{nk}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_n \\\\\n\\end{bmatrix},\\] donde \\(\\mathbf{Y}\\) vuelve a ser el vector de \\(n\\) respuestas, \\(\\mathbf{X}\\) es ahora la matriz \\(n\\times(k+1)\\) de variables explicativas, que contiene una columna de unos para incluir en el modelo el parámetro \\(\\beta_0\\) que no depende de las variables explicativas, \\(\\beta\\) es el vector de \\(k+1\\) parámetros del modelo y \\(\\epsilon\\) vuelve a ser el vector de los términos de error aleatorios, con distribución normal, esperanza cero y varianza constante, siendo \\(I\\) la matriz identidad.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#modelo-estadístico-de-regresión",
    "href": "Cap1-LM.html#modelo-estadístico-de-regresión",
    "title": "1  Modelos lineales",
    "section": "",
    "text": "En este contexto, cuando se habla de modelo lineal hay que distinguir entre modelo lineal en las variables y modelo lineal en los parámetros. Así:\n- \\(\\mathbf{Y}=\\beta_1 \\mathbf{X}_1 + \\beta_2 \\mathbf{X}_2 + \\epsilon\\) es un modelo lineal en las variables y en los parámetros.\n- \\(\\mathbf{Y}=\\beta_1 \\mathbf{X}_1 + \\beta_2 \\mathbf{X}_1^2  + \\epsilon\\) es un modelo lineal en los parámetros, pero no en la variable.\n- \\(\\mathbf{Y} = \\beta_1\\mathbf{X}_1^{\\beta_2}  + \\epsilon\\) es un modelo no lineal tanto en los parámetros como en la variable.\nEn este material modelo lineal se refiere a lineal en los parámetros, sin que ello genere ambigüedad.\n\n\n\n\nLos modelos que se puedan expresar en la forma matricial anterior son modelos lineales.\n\n\nPreguntas\nEn el modelo de regresión ¿tienen que ser todas las variables cuantitativas?\nRespuesta corta: No.\nDependiendo del tipo de variables (\\(\\mathbf{X}\\) e \\(\\mathbf{Y}\\)) se tienen distintos modelos…\n¿Y si hay más de una variable \\(\\mathbf{Y}\\)? Se habla de regresión múltiple multivariante, que se podrá ver en otra asignatura del grado.\n\n\n1.1.1 Objetivos de la regresión\n\nDescribir la estructura general entre la(s) variable(s) explicativa(s) y la respuesta, estimando y evaluando su efecto.\nGeneralmente, este proceso es iterativo, hasta seleccionar la(s) variable(s) del mejor modelo posible (como se ve en el Capítulo 5).\nPredecir observaciones futuras.\n\n¡Ambos objetivos pueden ser muy distintos! Bajo el prisma del “machine learning” suele relajarse la descripción estructural y evaluación de los efectos, dando toda la importancia a la mejor predicción posible.\n\n\n1.1.2 Supuestos del modelo de regresión\nLos supuestos en los que se basa el modelo de regresión expuesto son:\n\nLinealidad: la relación entre la(s) variable(s) explicativa(s) y la respuesta es lineal.\nHomocedasticidad (homogeneidad de varianzas): la varianza de la variable \\(\\mathbf{Y}\\) para cada valor de \\(\\mathbf{X}_i\\) (distribución condicionada) debe ser homogénea.\nNormalidad: tanto la variable \\(\\mathbf{Y}\\) como los valores de \\(\\mathbf{Y}\\) para cada valor de \\(\\mathbf{X}_i\\) deben seguir una distribución normal.\nIndependencia: cada observación de la variable \\(\\mathbf{Y}\\) debe ser independiente de las demás.\n\nLos supuestos anteriores se basan en las hipótesis de los errores:\n\nEsperanza nula: \\(E[\\epsilon]=0\\),\nVarianza constante: \\(\\text{Var}[\\epsilon]=\\sigma^2I\\),\nDistribución normal \\(\\epsilon \\sim N(0, \\sigma^2 I)\\),\nIndependencia entre errores: \\(E[\\epsilon_i\\epsilon_j]=0\\).\n\nSe puede encontrar más información sobre las también denominadas hipótesis básicas en Peña (2002) (apartados 5.2.1 y 5.2.2) y en Faraway (2004) (Capítulo 4).\nResiduos\nLos residuos, \\(u_i\\), son los errores observados para los datos y el modelo escogido (realizaciones de la variable aleatoria error \\(\\epsilon\\)). Recogen toda la información que la estructura del modelo no ha sido capaz de asimilar. Es muy importante el estudio de los residuos para comprobar las hipótesis y supuestos anteriores y dar con ello validez al uso del modelo, como se ve en la Sección 1.2.9 a nivel teórico y en los casos prácticos (Sección 1.4 y Sección 1.5).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#lm",
    "href": "Cap1-LM.html#lm",
    "title": "1  Modelos lineales",
    "section": "1.2 Modelo lineal de regresión simple",
    "text": "1.2 Modelo lineal de regresión simple\nEs habitual comenzar el estudio de los modelos lineales con el caso más sencillo, el de regresión lineal simple, para profundizar en los detalles de cada uno de los pasos del proceso de modelización, estimación, validación y predicción. Posteriormente se señalarán los aspectos que cambian al pasar a la regresión lineal múltiple, que es el caso más habitual en la práctica.\nPartiendo de los datos muestrales recogidos sobre el fenómeno de interés. ¿Cómo obtener el modelo de regresión lineal simple?\n\nEstructura: se deben identificar la variable respuesta (la que se pretende explicar) y la variable explicativa. Esta identificación define una forma estructural, que en este caso sencillo podría tener 2 formas:\n\n\n\\(E(\\mathbf{Y})=\\beta_0 + \\beta_1 \\mathbf{X}\\), la habitual (incluso diría “por defecto”) o\n\\(E(\\mathbf{Y})= \\beta_1 \\mathbf{X}\\) (proporcionalidad directa, sin término independiente).\n\n\nEstimación: con los valores muestrales recogidos \\((x_i, y_i)\\) se obtienen las estimaciones \\(\\hat{\\beta}_i\\) de los parámetros \\(\\beta_i\\) del modelo preestablecido.\nValidación: mediante el análisis de residuos se comprueban las hipótesis que validan el uso del modelo estimado.\nInterpretación/Inferencia: validado el modelo se interpretan las estimaciones de los parámetros y su utilidad práctica. Sólo tendrá sentido interpretar aquellos que sean significativos. También, mediante algún indicador, se dará cuenta de la bondad del modelo para explicar la variable respuesta.\nEn este proceso, puede verse conveniente cambiar el modelo (en el caso de regresión simple, considerar otra variable explicativa, o aplicar alguna transformación) para intentar obtener un modelo que explique mejor la respuesta, con mayor bondad, por lo que se volvería al paso 1.\nPredicción: se obtienen las predicciones necesarias u oportunas a partir del modelo considerado, teniendo cuidado con la extrapolación (y, en algunos casos, también con la interpolación).\n\n\n1.2.1 Estimación de los parámetros del modelo\nEn el caso más habitual de regresión simple: \\(E(\\mathbf{Y}) = \\beta_0 + \\beta_1\\mathbf{X},\\) se estiman los dos parámetros del modelo: \\(\\beta_0\\), la ordenada en el origen y \\(\\beta_1\\), la pendiente de la recta. Intuitivamente, se buscan las estimaciones \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que “expliquen” la relación lineal entre las dos variables, aquellas que generen las mejores predicciones posibles: \\[\\hat{\\mathbf{Y}} = \\hat{\\beta}_0 + \\hat{\\beta}_1\\mathbf{X}.\\]\n\n\n1.2.2 Método de mínimos cuadrados\nEl método por excelencia para obtener tales estimaciones es el método de mínimos cuadrados (1805 Gauss y Legendre). Consiste en minimizar los cuadrados de los residuos, \\(u_i = y_i - \\hat{y}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\), diferencia entre los valores observados y la predicción. Es decir, \\[\\min_{\\beta_0, \\beta_1} \\sum_i u_i^2 =\n\\min_{\\beta_0, \\beta_1} \\sum_i(y_i - \\beta_0 - \\beta_1 x_i)^2.\\]\nDerivando, e igualando a cero se tiene un sistema lineal de 2 ecuaciones con 2 incógnitas, denominado ecuaciones normales: \\[\\left.\\begin{align*}\n\\dfrac{\\partial \\sum_i u_i^2}{\\partial \\beta_0} &= 0 \\Rightarrow \\sum_{i} u_i = 0 \\\\\n\\dfrac{\\partial \\sum_i u_i^2}{\\partial \\beta_1} &= 0 \\Rightarrow \\sum_{i} u_i x_i = 0\n\\end{align*}\n\\right\\rbrace\\]\ncuya solución es:\n\\[\\begin{align*}\n\\hat{\\beta}_1^{MC} &= \\rho \\frac{S_y}{S_x}, \\\\\n\\hat{\\beta}_0^{MC} &= \\bar{y} - \\hat{\\beta}_1^{MC} \\bar{x},\n\\end{align*} \\] donde \\(S_y\\) y \\(S_x\\) son las desviaciones típicas muestrales de las variables \\(\\mathbf{Y}\\) y \\(\\mathbf{X}\\), y \\(\\rho\\) es el coeficiente de correlación lineal de Pearson: \\[\\rho=\\frac{S_{xy}}{S_x S_y}=\\dfrac{\\sum (x_i-\\bar x)(y_i-\\bar y )}{\\sqrt{\\sum (x_i-\\bar x)^2 \\sum (y_i-\\bar y)^2}}.\\]\nCorrelación\nEn la regresión lineal simple se espera que ambas variables estén correlacionadas, así, el modelo tendrá sentido práctico. El coeficiente de correlación lineal de Pearson, \\(\\rho\\), mide la fuerza de la relación lineal entre las dos variables. Se puede expresar en función de las medias y las desviaciones típicas de las variables:\n\\[\\rho=\\frac{1}{n}\\sum \\left( \\frac{x_i-\\bar{x}}{S_x} \\right) \\left(\\frac{y_i-\\bar{y}}{S_y} \\right)=\\frac{1}{n}\\sum z_x \\cdot z_y.\\]\nLos valores de este coeficiente se extienden de \\(-1\\) a \\(1\\), indicando ausencia de correlación cuanto más cercano sea a \\(0\\).\n\nHay que destacar el ejemplo ilustrativo del cuarteto de Anscombe, en el que 4 conjuntos de datos presentan el mismo valor del coeficiente de correlación lineal, pero su interpretación es muy distinta en cada uno de los 4 casos.\n\n\n1.2.2.1 Interpretación geométrica\nLos estimadores de mínimos cuadrados (MC) tienen una interpretación geométrica sencilla y gráficamente elocuente. Son aquellos que hacen que la recta de regresión simple minimice las distancias verticales (residuos) de toda la nube de puntos:\n\n\n\n\n\n\n\n\n\nAhora bien, también se puede representar el problema de MC en el espacio de las \\(n\\) observaciones (en lugar del espacio de 2 variables), lo que proporciona una interpretación geométrica interesante (véase Faraway (2004) apartado 2.3). La proyección ortogonal del vector \\(\\mathbf{Y}=(y_1, y_2, \\ldots, y_n)^\\top\\) sobre el plano definido por los vectores \\(\\mathbf{1}\\) y \\(\\mathbf{X}\\) genera el vector de residuos \\(\\mathbf{u}\\) perpendicular a ellos y a todos los vectores del plano. Además, aplicando el teorema de Pitágoras al triangulo rectángulo resultante de la proyección (Figuras 5.11 de Peña (2002) y 2.1 de Faraway (2004)) proporciona una forma alternativa de obtener la fórmula de la suma de cuadrados que se ve en la Sección 1.2.7.\n\n\n\n1.2.3 Método de máxima verosimilitud\nTambién se pueden estimar los parámetros del modelo mediante el método de máxima verosimilitud (MV), que consiste en maximizar la función de verosimilitud, o su logaritmo: \\(\\log L(\\beta_0, \\beta_1, \\sigma)\\). Suponiendo normalidad: \\[\\log L(\\beta_0, \\beta_1, \\sigma) = -\\dfrac{n}{2}\\log (2\\pi) - n \\log(\\sigma) -\\frac{\\sum_{i=1}^{n} u_i^2}{2\\sigma^2}\\] En Peña (2002), apartado 5.4.1, se puede ver el detalle de la obtención de los estimadores máximo verosímiles.\nBajo el supuesto de normalidad, los estimadores MC coinciden con los MV, es decir, los estimadores que minimizan la suma de cuadrados de los residuos, también maximizan la probabilidad de los datos observados. Basta observar la fórmula de la verosimilitud escrita arriba.\n\n\n1.2.4 Estimación de la varianza del modelo\nTambién es necesario estimar la varianza del modelo. Utilizando el método de máxima verosimilitud se llega a: \\[\\hat{\\sigma}^2_{MV}=\\dfrac{\\sum u_i^2 }{n},\\] es decir, es la varianza de los residuos (la dividida por \\(n\\)).\nAhora bien, los \\(n\\) residuos no son independientes, se pierden 2 grados de libertad al tener que estimar los dos parámetros de la recta de regresión. Por ello, se define el estimador denominado varianza residual: \\[\\hat S_R^2=\\dfrac{\\sum u_i^2 }{n-2}.\\] En la práctica se suele obtener su raíz cuadrada, \\(\\hat S_R\\), denominada error estándar residual.\n\n\n1.2.5 Propiedades de los estimadores\nEl estudio de las propiedades de los estimadores obtenidos resulta de interés para la posterior inferencia. Aquí se mencionan algunas propiedades. El lector interesado puede ampliar información, por ejemplo, en Peña (2002) (la Tabla 5.3, página 264, contiene un buen resumen)\nAntes de ello, la ecuación de la recta de regresión también se suele expresar en la forma punto-pendiente: \\[(y - \\bar{y}) = \\hat{\\beta}_1 (x - \\bar{x}), \\] de donde se deduce inmediatamente que el punto \\((\\bar x, \\bar y)\\), pertenece a la recta estimada. Tal punto se conoce como centroide, centro de gravedad de la nube de puntos.\nSobre \\(\\hat \\beta_1\\), la pendiente estimada de la recta. Esta pendiente se puede obtener como una media ponderada de pendientes de las rectas que pasan por cada punto y por el centroide. Tal ponderación está relacionada con el leverage (véase Sección 1.2.12) e implica que los puntos con valores de \\(x\\) más alejados a la media tienen más influencia sobre la estimación de \\(\\beta_1\\), más cuanto menos puntos sean. También se puede ver como una combinación lineal de valores de la variable respuesta, por lo que el estimador sigue una distribución normal, bajo el supuesto de normalidad de la variable respuesta, concretamente, \\[\\hat \\beta_1 \\sim N \\left( \\beta_1, \\dfrac{\\sigma^2}{nS_x^2} \\right) .\\] Por lo tanto, es insesgado (asumiendo la linealidad, independientemente de la normalidad) y su varianza disminuirá (aumentará la precisión de \\(\\hat\\beta_1\\)), bien al aumentar el tamaño de muestra, \\(n\\), o al aumentar la dispersión de los valores de \\(x\\), \\(S_x^2\\). Como \\(\\sigma^2\\) es desconocida, se considerará en su lugar la varianza residual \\(\\hat S_R^2\\).\nSobre \\(\\hat \\beta_0\\), mencionar que es el parámetro de la ordenada en el origen (intersección de la recta con el eje \\(y\\)) y su importancia se encuentra en un segundo plano respecto a \\(\\hat \\beta_1\\). También se puede ver como una combinación lineal de valores de la variable respuesta, por lo que seguirá una distribución normal (como la variable respuesta), concretamente, \\[\\hat \\beta_0 \\sim N \\left( \\beta_0, \\dfrac{\\sigma^2}{n} \\left( 1+ \\dfrac{\\bar x^2}{S_x^2}\\right) \\right).\\] De nuevo, este estimador es insesgado. Su varianza está multiplicada por \\(\\bar x^2\\), lo que supone mayor impacto cuando mayor sea el valor de \\(\\bar x\\).\nAdemás, ambos estimadores están correlacionados: \\[\\text{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) = -\\dfrac{\\bar{x}\\sigma^2}{nS_x^2}\\] Por lo tanto, cuando \\(\\bar{x}&gt;0\\), \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) están negativamente correlados.\nRespecto a la varianza residual, se tiene que \\(\\hat S_R^2\\) es un estimador insesgado de \\(\\sigma^2\\), dado que \\[\\dfrac{(n-2) \\hat S_R^2}{\\sigma^2} \\sim \\chi^2_{n-2}.\\]\n\n\n1.2.6 Contrastes de hipótesis\nConocidas las distribuciones en el muestro de los estimadores, podemos plantear contrastes de hipótesis sobre ellos. En una regresión lineal simple, realizar un contraste de hipótesis sobre el parámetro \\(\\beta_1\\) asociado a la variable explicativa, es equivalente al contraste de hipótesis sobre el coeficiente de correlación lineal de Pearson entre dicha variable y la respuesta, \\(\\rho\\): \\[\\left. \\begin{array}{ll}\nH_0: \\beta_1 = 0 \\\\\nH_1: \\beta_1 \\neq 0\n\\end{array} \\right\\rbrace\n\\qquad \\equiv  \\qquad\n\\left. \\begin{array}{ll}\nH_0: \\rho = 0 \\\\\nH_1: \\rho \\neq 0\n\\end{array} \\right\\rbrace\\]\nPara parejas de variables normales e incorreladas se cumple que: \\[\\rho\\sqrt{\\frac{n-2}{1-\\rho^2}} \\sim t_{n-2}\n\\qquad \\equiv \\qquad\n\\dfrac{\\hat \\beta_i - \\beta_i}{\\hat s(\\hat \\beta_i)} \\sim t_{n-2}\\] donde \\(\\hat s(\\hat \\beta_i)\\), es el estimador de la desviación típica del estimador. Esto también se cumple de forma aproximada si las variables son no normales y si los tamaños de muestra son “grandes” (o no demasiado pequeños). Y de aquí podemos determinar la significación de la estimación del parámetro \\(\\hat \\beta_1\\) (de una correlación). El p-valor nos dará la fuerza de dicha significación.\n\n\n1.2.7 Suma de Cuadrados en la regresión\nAl contraste de hipótesis anterior también puede llegarse desde otro punto de vista. Los residuos, distancias verticales entre cada uno de los puntos, \\((x_i,y_i)\\), y la recta de regresión, expresan el error aleatorio del modelo. ¿Hasta qué punto es más importante el efecto de la variable \\(\\mathbf{X}\\) sobre la variable \\(\\mathbf{Y}\\) que el error de los residuos?\nSi las variables \\(\\mathbf{X}\\) e \\(\\mathbf{Y}\\) no estuviesen relacionadas, no aportaría información sobre \\(\\mathbf{Y}\\) conocer los valores de \\(\\mathbf{X}\\). La mejor predicción que podríamos hacer sería predecir \\(\\mathbf{Y}\\) con su media, \\(\\bar{\\mathbf{Y}}\\), sin tener en cuenta el valor de \\(\\mathbf{X}\\). Este modelo (\\(\\beta_1 = 0\\)), el más sencillo, es el que vamos a intentar falsar.\nEl estudio de la variabilidad (información) de la variable respuesta nos aportará evidencias que nos permitan (o no) rechazar \\(H_0: \\beta_1 = 0\\) (falsarla) y asumir la hipótesis alternativa, \\(H_1 : \\beta_1 \\neq 0\\).\n\\[SC_{total}=SC_y=\\sum (y_i-\\bar{y})^2\\]\n\\(SC_{total}\\) es el numerador de la habitual varianza muestral aplicada a \\(y\\). Se puede calcular multiplicando dicha varianza por los grados de libertad \\(n-1\\).\nMatemáticamente se puede descomponerse en \\(SC_{regresion}\\) y \\(SC_{residual}\\).\n\\[SC_{total}=SC_{regresion}+SC_{residual}\\] \\[\\sum (y_i-\\bar{y})^2={\\sum (\\hat{y}_i-\\bar{y})^2}+\\sum (y_i-\\hat{y}_i)^2\\]\nEstudiamos los grados de libertad (gl) al calcular cada uno de los términos:\n\nPara \\(SC_{total}\\) “gastamos” \\(1\\) gl al dar la media, \\(\\bar{y}\\), por lo que tiene \\(n-1\\) gl.\nLos gl de los residuos son \\(n-2\\), necesitamos \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) para calcular \\(SC_{residual}\\).\n\nCon lo que nos queda \\(1\\) gl para la suma de cuadrados de la regresión \\(SC_{regresion}\\). “Gastamos” \\(1\\) gl con el parámetro extra que hemos estimado \\(\\hat{\\beta}_1\\), la pendiente.\n\nPor último, se promedia cada suma de cuadrados por sus respectivos grados de libertad (SCM: Suma de Cuadrados Media), en definitiva, calculamos varianzas.\nNunca seremos capaces de realizar predicciones perfectas (todos los modelos son falsos), pero estamos interesados en comparar el efecto de \\(\\mathbf{X}\\) sobre \\(\\mathbf{Y}\\) con el error aleatorio (residual):\n\\[\\frac{\\text{Efecto de X sobre Y}}{\\text{Error Aleatorio}}=\\frac{\\text{Varianza de la Regresión}}{\\text{Varianza Error}}=\\frac{SCM_{regresion}}{SCM_{error}}=F \\sim F_{1,n-2}\\]\nEsto se conoce en estadística como un Análisis de la Varianza, ANOVA (que se ve más en profundidad en el Capítulo 2).\nPara que podamos falsar \\(H_0\\), la Varianza de la Regresión debe ser mayor que la Varianza del Error, cuanto más grande mejor. Si ambas son próximas se concluye que \\(\\beta_1\\) puede ser 0.\nSe compara entonces el estadístico F obtenido de dividir las dos varianzas con una distribución F con los grados de libertad correspondientes, \\(1\\) en el numerador y \\(n-2\\) en el denominador. Calculando el p-valor se podrá rechazar (o no) la hipótesis nula, concluyendo (o no) que \\(\\beta_1\\) es significativamente distinto de 0.\nEs importante resaltar aquí el supuesto de linealidad. No rechazar \\(H_0\\) no implica necesariamente que \\(x\\) e \\(y\\) sean independientes, podrían estar relacionadas de forma no lineal, y la aproximación lineal podría no ser significativa.\nEn Peña (2002), Apartado 5.6.2, se descompone las desviaciones de los datos (usando notación matricial) en dos componentes ortogonales. Por el teorema de Pitágoras se llega a la descomposición de la variabilidad descrita aquí. Además, en el Apéndice 5B, se dan detalles sobre la “Deducción de las distribuciones de sumas de cuadrados”.\n\n\n1.2.8 Bondad de ajuste: Coeficiente de Determinación, \\(R^2\\)\nPor la descomposición mencionada anteriormente, \\(SC_{total}=SC_{regresion}+SC_{residual}\\), la proporción de variabilidad explicada por la regresión, respecto al total, es un indicador de la bondad de la regresión:\n\\[\\frac{SC_{regresion}}{SC_{total}} = R^2,\\]\nlo que se conoce como coeficiente de determinación, \\(R^2\\). Como proporción, puede tomar valores entre \\(0\\) y \\(1\\), evitando así la dependencia de las unidades de medida. Suele expresarse en porcentaje.\nLa siguiente figura permite ilustrar el concepto de bondad de ajuste.\n\n\n\n\n\n\n\n\n\nEn la gráfica de la izquierda se tiene un buen ajuste de la recta de regresión, que produce un \\(R^2\\) elevado. Mientras que en la gráfica de la derecha el \\(R^2\\) es próximo a cero, indicando que el conocimiento de los valores de \\(\\mathbf{X}\\) aporta casi nula información sobre los valores de variable \\(\\mathbf{Y}\\). Para más énfasis, en la imagen de la izquierda se muestra la variabilidad total de \\(\\mathbf{Y}\\) frente a la variabilidad que tiene \\(\\mathbf{Y}\\) para casi cualquier valor de \\(\\mathbf{X}\\).\nObtención del coeficiente\nEn el caso de la regresión lineal simple, \\(R^2\\) coincide con el cuadrado del coeficiente de correlación, \\(\\rho\\) (véase Peña (2002), apartado 5.7.1, donde se dan detalles de cómo llegar al coeficiente de correlación a partir del de determinación, o a partir de la varianza residual).\n\n\n1.2.9 Diagnosis\nLa diagnosis consiste en comprobar los supuestos básicos del modelo (Sección 1.1.2). Recordemos: linealidad, homocedasticidad, normalidad e independencia.\nEn la regresión simple, los supuestos de linealidad y homocedasticidad se pueden comprobar visualmente acudiendo al diagrama de dispersión. En la regresión múltiple, no es tan directo. Se acude directamente a los gráficos de diagnóstico, basados en los residuos del modelo, para comprobar las hipótesis. Con la ayuda de R será bastante sencillo obtener tales gráficos. Por lo que, en la práctica, se comprueban las hipótesis (y alguna característica más) en ellos. Ahora bien, el supuesto de independencia podría incumplirse por construcción, por ejemplo, si se tienen datos temporales.\nUn enfoque ligeramente distinto sobre la diagnosis se encuentra en Faraway (2004), que divide los problemas potenciales de la regresión estimada en 3 categorías:\n\nerror: se asume que los errores son independientes, tienen varianza constante y son normalmente distribuidos.\nmodelo: se asume que la parte estructural del modelo es correcta, es decir, que la relación entre las variables es lineal.\nobservaciones inusuales: en ocasiones unas pocas observaciones no se ajustan al modelo, o cambian/influyen demasiado en el modelo.\n\nAdemás indica que las técnicas gráficas de diagnóstico son más flexibles, pero mucho más difíciles de interpretar, que las de contrastes, que son sencillas y directas (no requieren de intuición y podrían parecer más precisas), pero no permiten una visualización general del problema. La diagnosis gráfica de los residuos permiten detectar problemas, revelar estructuras ocultas que es imposible vislumbrar con los contrastes, lo que da pistas para solucionar los problemas de la regresión, llevando la modelización a un proceso iterativo, con cierto aire artesanal. Por ello, predomina un enfoque gráfico para el diagnóstico, acudiendo a los contrastes como complemento para confirmar lo observado en los gráficos. En ocasiones los gráficos pueden ser ambiguos, pero al menos permiten verificar que no hay grandes desviaciones de los supuestos del modelo.\n\n1.2.9.1 Linealidad\nComo se ha comentado, el modelo asume una estructura lineal en la relación entre el predictor y la respuesta, que puede observarse fácilmente con el gráfico de dispersión entre ambas variables.\nAhora bien, en la práctica, cuando se estima el modelo con software, es fácil tener los residuos y comprobar esta hipótesis en el correspondiente gráfico de diagnóstico. Concretamente, a partir del gráfico de residuos frente a valores estimados por el modelo, para cada valor observado. En líneas generales, si la relación lineal es la correcta se observará aleatoriedad, valores dispersos entorno al 0 (verticalmente), sin tendencias ni otros patrones marcados (véase el gráfico del apartado siguiente de Homocedasticidad).\nEl contraste de linealidad es el de la tabla ANOVA, por lo que no se entrará aquí en más detalles de los ya vistos.\n\n\n1.2.9.2 Homocedasticidad\nLos residuos deben tener varianza constante, homocedasticidad, con respecto a los valores de la variable \\(x\\), en el caso de regresión simple (por lo que se puede comprobar en el gráfico de dispersión entre \\(x\\) e \\(y\\)). La falta de homocedasticidad invalida el uso de los estimadores MC/MV pues implica distinta precisión en las estimaciones.\nAnálisis gráfico\nEl gráfico de diagnóstico apropiado para observar la homocedasticidad o heterocedasticidad es el de residuos frente a valores estimados por el modelo (el mismo que para la linealidad, que es también válido para el caso de regresión múltiple). De nuevo se desea observar aleatoriedad, ausencia de patrones.\nEn el siguiente gráfico se pueden ver ejemplos simulados de distintas situaciones, que se interpretan de una forma clara. Pero la realidad supera la ficción… Se necesita cierta experiencia para no cometer equivocaciones al interpretar gráficos de residuos.\n\n\n\n\n\n\n\n\n\nLo deseable en este gráfico es encontrar una nube de puntos dispersos (y simétricos, mirando verticalmente) alrededor de 0, sin ningún patrón aparente (como en el primer gráfico). Si se observa un patrón, como una forma cónica (segundo gráfico) o una tendencia (como la no lineal del tercer gráfico), puede indicar problemas de heterocedasticidad (varianza no constante) o no linealidad en el modelo. Estas dos últimas situaciones dan pistas de las posibles acciones a tomar sobre los datos, como transformar la variable \\(x\\) o \\(y\\) para conseguir homocedasticidad, o incluir algún término no lineal en la variable \\(x\\) (manteniendo el modelo lineal en los parámetros). Por su parte, el primer gráfico permitiría validar gráficamente el supuesto de homocedasticidad.\nComo se ve en los casos prácticos (Sección 1.4 y Sección 1.5), se suele también comprobar la homocedasticidad en el gráfico de la raíz cuadrada del valor absoluto de los residuos estandarizados (véase Sección 1.2.12) frente a los valores estimados. Al tomar el valor absoluto se aumenta la resolución para detectar la falta de homocedasticidad. Ahora bien, no permite la comprobación de la nolinealidad.\nUna alternativa, más elaborada, es la propuesta de Faraway (2004): dibujar los residuos frente a cada una de las \\(x_i\\) -en caso de regresión múltiple-. Para todas la variables del conjunto de datos, tanto las incluidas en el modelo, como las no incluidas, mirando en estas últimas si existe alguna relación que indique la necesidad de incluirla en el modelo.\nContrastes\nLos contrastes para detectar heterocedasticidad, tienen una hipótesis nula clara: \\[H_0: \\sigma^2 = \\text{cte}, \\] Pero dependen de la hipótesis alternativa especificada. Así, el contraste puede detectar bien un tipo específico de heterocedasticidad, pero no tener potencia suficiente para otros.\nDe entre los distintos contrastes para comprobar la homocedasticidad destacan, el contraste de Bartlett y el de Levene, que evalúan la hipótesis nula de igualdad de varianzas entre \\(k\\) grupos. Los estadístico de contraste se pueden encontrar en la página web del NIST/SEMATECH e-Handbook of Statistical Methods, concretamente: contraste de Bartlett y contraste de Levene, respectivamente. El estadístico de Bartlett sigue aproximadamente una distribución \\(\\chi^2\\) con \\(k - 1\\) grados de libertad bajo \\(H_0\\). Mientras que el de Levene sigue aproximadamente una distribución \\(F\\) con \\(k-1\\) y \\(n-k\\) grados de libertad. En ambos casos, se rechazará la homocedasticidad si se obtienen p-valores pequeños, habitualmente inferiores a 0.05.\n\n\n1.2.9.3 Normalidad\nLos residuos también deben seguir una distribución normal, para justificar el uso de estimadores MC/MV.\nEl gráfico apropiado para evaluarlo es el denominado Q-Q plot (gráfico Cuantil-Cuantil): un diagrama de dispersión de los cuantiles de los residuos frente a los cuantiles de la distribución normal (con la misma media y desviación típica de los residuos). La interpretación, como diagrama de dispersión, es bien sencilla, los residuos siguen una distribución normal cuanto más se alineen los puntos del diagrama sobre una linea recta “guía” (y más diagonal sea dicha “guía”). Se incumple la normalidad generalmente por las colas, con puntos que se alejan ostensiblemente de la recta “guía”. Por ejemplo, es típico observar curvatura en forma de S, indicando que los residuos tiene colas más ligeras que la distribución normal, o curvatura en forma de U indicando que siguen otra distribución.\n\n\n\n\n\n\n\n\n\nContrastes\nPara completar la comprobación gráfica, se acude a los contrastes de normalidad. El más difundido es del de Shapiro-Wilk, basado precisamente en el gráfico Q-Q, y considerado uno de los que más potencia poseen para contrastar normalidad. \\[\\left. \\begin{array}{ll}\nH_0: \\text{los residuos provienen de una distribución normal} \\\\\nH_1: \\hspace{1.4cm}\\text{... no provienen...}\n\\end{array} \\right\\rbrace\\] Para más información sobre el test de Shapiro-Wilk se puede consultar también la web del NIST/SEMATECH e-Handbook of Statistical Methods: Shapiro-Wilk test. En este caso, el estadístico de contraste no sigue una distribución de probabilidad conocida y sus probabilidades se han calculado mediante simulaciones por el método de Monte Carlo.\n\n\n1.2.9.4 Independencia\nLa independencia es un supuesto que puede comprobarse con los gráficos de residuos, pero que puede incumplirse desde el planteamiento del problema, de la recogida de datos, etc. sin necesidad de llegar al análisis de residuos, en el que incluso podría no quedar reflejado el incumplimiento.\nPara comprobar la independencia temporal de los datos se visualizará el gráfico de residuos frente al orden en la toma de datos (si se dispone de ello), o, en su defecto, en el orden que se tengan los datos (que podrían haber sido ordenados, lo que impediría su correcto análisis).\nContrastes\nComo en los anteriores supuestos, se suele acudir a contrastes para completar el análisis de independencia. El más utilizado es el de Durbin-Watson, que comprueba la presencia de autocorrelación (relación temporal entre los residuos). \\[\\left. \\begin{array}{ll}\nH_0: \\text{los residuos no tienen correlación temporal} \\\\\nH_1: \\text{los residuos siguen un proceso autorregresivo de primer orden, } AR(1) \\end{array} \\right\\rbrace\\] Más información en Estadístico de Durbin–Watson.\n\n\n1.2.9.5 Soluciones\nComo se ha comentado, la comprobación de los supuestos mediante los gráficos de residuos, a la par que pueden conducir a rechazar uno o varios de ellos, pueden proporcionar pistas para su solución.\nLo primero que suele saltar a la vista en el análisis gráfico es la presencia de observaciones atípicas, que contribuyen a la falta de linealidad, de homocedasticidad, ambas… Lo apropiado es analizar el impacto de dichas observaciones, por ejemplo, comparando los modelos estimados con tales observaciones o sin ellas. Para ello, se han desarrollado medidas como el leverage y la distancia de Cook, para averiguar la influencia de las observaciones (sean atípicas o no) en la regresión (véase Sección 1.2.12). El resultado de este análisis puede llevar a detectar errores de medición, o que la definición funcional (como forma lineal) del problema no es la adecuada.\nOtra posible solución para obtener linealidad y/o homocedasticidad es realizar transformaciones en las variables, bien \\(y\\), bien \\(x\\), o ambas (véase Sección 1.2.13). Por ejemplo, la relación entre la variable \\(y\\) y la \\(x\\) podría ser exponencial, por lo que, tomando como respuesta \\(\\log(y)\\) se tendrá linealidad. Sobre la heterocedasticidad, en ocasiones se da por la dependencia de la varianza de \\(y\\) respecto de \\(x\\). Si se dividen las observaciones por la estructura que provoque \\(x\\) en la varianza, se tendrá un modelo homocedástico. En la práctica, el gráfico de residuos frente a valores estimados puede dar pistas sobre la transformación a realizar. Pero, se suelen intentar varias transformaciones, pues es un arte encontrar la transformación más adecuada (la intuición y la experiencia pueden ayudar).\nUn caso más complejo de intuir es la introducción de variables para conseguir linealidad, bien puede ser, términos polinomiales de la misma variable, lo que conduce a una regresión polinómica lineal (véase Sección 1.5.3), o bien, se pueden añadir otras variables, lo que lleva a un modelo lineal de regresión múltiple (véase Sección 1.3). Otras opciones de resolver los problemas por incumplimiento de los supuestos básicos pasan por acudir a regresión no paramétrica (que se escapa del alcance de este material) o, en el caso de la heterocedasticidad, aprovechar si se conoce la estructura de varianza para utilizar el método de mínimos cuadrados ponderados.\n\n\n\n1.2.10 Interpretación de la recta de regresión\nUna vez estimada y validada la recta de regresión, si es significativa, estadísticamente hablando, se puede pasar a interpretar los coeficientes/parámetros, si tienen un sentido práctico (y el análisis de residuos que se ve más adelante no invalida los supuestos en los que se basa).\nLa interpretación generalmente más importante es la de \\(\\hat \\beta_1\\) dado que recoge el efecto sobre la variable \\(y\\) de la variación de una unidad de la variable explicativa \\(x\\). Su interpretación debe hacerse acorde a las unidades en la que esté recogida la variable (no es lo mismo que, si es una medida de temperatura, se haya medido en ºC que en ºK, o si es de tiempo que se mida en segundos o en días). Así el impacto sobre la respuesta del cambio de \\(1\\) ºC (o de \\(1\\) s) será de una magnitud muy distinta al cambio de 100ºC (\\(1\\) hora). Es más, el valor de \\(\\hat \\beta_1\\) se podría aumentar o disminuir haciendo cambios de escala en las variables \\(y\\) y \\(x\\).\n¿Y qué pasa si en lugar de tener sólo una variable tenemos más variables? En el apartado de regresión múltiple (Sección 1.3), se ve el cambio que supone el tener varias variables explicativas, en la interpretación de las estimaciones de los parámetros.\nLa interpretación de la estimación \\(\\hat \\beta_0\\) es el del valor medio en ausencia del valor de la \\(x\\), que en ciertas ocasiones no pertenece al rango de variación de la variable \\(x\\) o puede no tener sentido (por ejemplo si la variable \\(x\\) recoge la edad de los individuos, puede que no tenga sentido la media a \\(0\\) años).\nSi la recta no fuese significativa, no implica que no haya relación entre \\(x\\) e \\(y\\), quizá la relación es no lineal (como se ve en Sección 1.2.9) o quizá el rango escogido para la \\(x\\) no es el idóneo para observar su influencia sobre la \\(y\\) (quizá es demasiado estrecho). Pero, como se señala en Peña (2002), para encontrar relaciones causales hay que acudir al diseño de experimentos (Capítulo 2), porque en un experimento se puede intentar controlar los valores de la variable \\(x\\) que se cree que influyen sobre la \\(y\\) y aleatorizar el resto de variables para “repartir” su impacto sobre la respuesta. Mientras que si los datos son observacionales sólo se puede deducir covariación, pero no causalidad, como que haya más criminalidad en las ciudades con más policías. Aquí la causa es una tercera variable, el tamaño de la ciudad. Reducir el número de policías ¡no causaría una reducción de la criminalidad!\n\n\n1.2.11 Predicción\nUno de los objetivos de la modelización es la posibilidad de poder predecir valores de la variable respuesta conocidos los valores de la variable explicativa.\nDichas predicciones pueden ser de dos tipos, predicciones medias, para la media de un conjunto de observaciones con el mismo valor del predictor, o predicciones individuales, para un sólo valor de la variable respuesta para un valor del predictor. Realmente, la predicción será la misma, la dada por la recta de regresión, pero la incertidumbre o precisión de dicha estimación, esto es, el intervalo de confianza, será distinto según se trate de la predicción de una media o un valor individual. Los intervalos de confianza para las predicciones individuales son más amplios que para los intervalos de confianza de la recta.\nPara todo ello es fundamental estudiar el error estándar de la pendiente y de la ordenada en el origen (en el caso de regresión lineal).\n\n1.2.11.1 Errores estándar de los estimadores\nDe las propiedades obtenidas anteriormente para los estimadores de los parámetros del modelo, se puede proporcionar con ellos intervalos de confianza sobre tales parámetros. Concretamente, como \\(\\hat \\beta_i\\) sigue una distribución normal \\[\\hat \\beta_i \\pm t_{gl,\\alpha/2} \\cdot \\hat s(\\hat \\beta_i)\\] donde \\(gl\\) son los grados de libertad, \\(\\alpha\\) es el nivel de significación escogido de antemano y \\(\\hat s(\\hat \\beta_i)\\), es el estimador de la desviación típica del estimador, también conocido como error estándar del estimador, que depende de la varianza residual.\nEl error estándar de la pendiente, \\(\\beta_1\\) es: \\[\\hat s (\\hat \\beta_1)=\\sqrt{\\dfrac{\\hat S_R^2}{nS_x^2}}=\\sqrt{\\frac{SC_{residual}/n-2}{SC_x}}\\] Y para la ordenada en el origen, \\(\\beta_0\\): \\[\\hat s (\\hat \\beta_0)=\\sqrt{\\frac{SC_{residual}}{n-2} \\left( \\dfrac{1}{n} + \\dfrac{ \\bar x^2}{SC_x}\\right)}\\]\nY podemos calcular intervalos de confianza tanto para la pendiente como para la ordenada en el origen.\nAhora bien, es más interesante poder calcular el error estándar y, con ello los intervalos de confianza, para las predicciones:\n\\[\\hat s(\\hat y) = \\sqrt{\\frac{SC_{residual}}{n-2} \\left( \\frac{1}{n}+\\frac{(x_i-\\bar{x})^2}{SC_x} \\right)}\\]\n\\[(\\hat \\beta_0 + \\hat \\beta_1\\cdot x_i) \\pm t_{n-2, \\alpha/2} \\cdot \\hat s (\\hat{y})\\]\nCon R será inmediato obtener intervalos de confianza para predicciones del modelo (tanto de la recta, como predicciones individuales). Algunos paquetes incluyen en los gráficos de regresión bandas de confianza para la respuesta media, \\(\\hat y\\), para cada valor individual \\(x_i\\), habitualmente del 95%, que significa que tenemos una confianza de ese 95% en que la verdadera recta de regresión está en esa región marcada.\n\n\n\n1.2.12 Observaciones influyentes\nSe sabe que, cuanto más alejado de su media esté el valor \\(x_i\\) observado más influencia tendrá sobre la pendiente de la recta de regresión. Sobre todo si no concuerda su pendiente respecto a la media, con la pendiente marcada por el resto de valores, y más si es un valor atípico.\nLeverage\nEl leverage (efecto palanca) es una medida necesaria para medir dicha influencia. En el caso de regresión lineal simple se puede calcular como: \\[h_i = \\dfrac{1}{n} + \\dfrac{(x_i - \\bar x)^2 }{\\sum_{j=1}^n(x_j - \\bar x)^2 }  \\] Observando la fórmula se deduce que el leverage toma valores entre \\(1/n\\) (cuando \\(x_i=\\bar x\\)) y \\(1\\) (cuando \\(x_i\\) esté muy alejado de \\(\\bar x\\), véase Peña (2002) para detalles sobre esta acotación superior). A mayor leverage mayor influencia sobre la pendiente de la recta de regresión.\nResiduos estandarizados\nUna observación con un valor de leverage próximo a \\(1\\) puede hacer que la recta de regresión pase por tal observación (además de pasar por el centroide, \\((\\bar x, \\bar y)\\)). Este hecho implica que en las observaciones con alto leverage, el residuo y su varianza son pequeños, mientras que en las observaciones cercanas a la media, el leverage es bajo y puede que su residuo y varianza sean más grandes. Por ello se suelen definir los residuos estandarizados: \\[r_i = \\dfrac{u_i}{\\hat S_R \\sqrt{1-h_i}}\\] que siguen una distribución normal tipificada, si las hipótesis del modelo son ciertas. Generalmente, los programas de software estadístico calculan estos residuos estandarizados y dibujan algunos gráficos de residuos con ellos.\nDistancia de Cook\nLa distancia de Cook es la medida de influencia utilizada en la práctica. Se basa en medir el cambio en la recta de regresión al eliminar la observación \\(i\\). Viene dada por: \\[D_i = \\dfrac{(\\hat y_i - \\hat y_{-i})^2}{2\\hat S_R^2 h_i}\\] donde:\n\n\\(\\hat y_i\\) es la estimación para la observación \\(i\\)-ésima, basada en todos los datos,\n\\(\\hat y_{-i}\\) es la estimación para la observación \\(i\\)-ésima, basada en todos los datos menos el \\(i\\)-ésimo.\n\nCon esta medida, un punto es influyente si \\(D_i&gt;1\\). Los puntos con alto leverage pueden ser influyentes, pero no lo son siempre, por lo comentado al principio, si no concuerda su pendiente respecto a la pendiente marcada por el resto serán influyentes, pero si concuerda, no lo serán.\nUn ejemplo aclarador podría ser el de dos variables claramente relacionadas. Pongamos por ejemplo, el PIB per cápita y la emisiones de CO2 de diferentes países. Se tendría una nube de puntos que indicase que a mayor PIB, mayores emisiones. Si el dato del PIB de uno de los países es muy grande (alejado de la media), y sus emisiones también son altas y acordes a la recta determinada por el resto de países, dicho país tendrá un alto leverage, pero su distancia de Cook no será mayor que 1. Por el contrario, si su valor de emisiones es muy distinto al que estimaría la recta determinada por el resto de países (sea dicho valor de emisiones mucho mayor o mucho menor), dicho país tendrá alto leverage y también alta distancia de Cook.\n\n\n1.2.13 Transformaciones\nComo se ha mencionado, puede ser útil transformar las variables para conseguir una relación lineal entre ellas, o conseguir homocedasticidad… Y así poder ajustarse con estas técnicas de modelos lineales. Por ejemplo si hay una relación exponencial entre las variables \\(\\mathbf{X}\\) e \\(\\mathbf{Y}\\), se puede aplicar el logaritmo a \\(\\mathbf{Y}\\) linealizando así el modelo. O si la relación es potencial, considerar una potencia de la variable \\(\\mathbf{X}\\): un modelo lineal de regresión no lineal. El tomar un modelo más complejo sólo tiene sentido si produce resultados significativos a la hora de explicar la relación. Ahora bien, hay que tener en cuenta que estas transformaciones pueden alterar la interpretación de los parámetros.\nEn la práctica, a la hora de transformar la variable respuesta, se acude a la familia de transformaciones Box-Cox, definida como: \\[\ny^{(\\lambda)} =\n\\begin{cases}\n\\frac{y^\\lambda - 1}{\\lambda}, & \\lambda \\ne 0 \\\\\n\\ln y, & \\lambda = 0\n\\end{cases}\n\\]\nY cuyos casos particulares se esquematizan en la siguiente tabla:\n\n\n\n\\(\\lambda\\)\nTransformación\nNomenclatura\n\n\n\n\n\\(1\\)\n\\(y\\)\nIdentidad\n\n\n\\(1/2\\)\n\\(\\sqrt{y}\\)\nRaíz cuadrada\n\n\n\\(0\\)\n\\(\\log y\\)\nLogaritmo natural\n\n\n\\(-1/2\\)\n\\(\\frac{1}{\\sqrt{y}}\\)\nInversa de la raíz cuadrada\n\n\n\\(-1\\)\n\\(\\frac{1}{y}\\)\nInversa\n\n\n\nEstas transformaciones pueden conseguir linealidad siempre que la relación entre las variables sea monótona. Un pequeño detalle es tener en cuenta que transformaciones como la raíz cuadrada o el logaritmo no funcionan si la variable respuesta \\(y_i\\) toma valores negativos, pero se puede subsanar considerando \\(y_i+\\delta\\) para evitarlo (aunque se pierde la interpretación directa de los resultados).\nEn ocasiones, no sólo es necesario la transformación de la variable respuesta, sino también de la explicativa. Y hay que tener en cuenta los efectos secundarios que pueden producir, por ejemplo, precaución con la interpretación de las estimaciones de los parámetros, sesgos en las predicciones, etc.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#sec-RLM",
    "href": "Cap1-LM.html#sec-RLM",
    "title": "1  Modelos lineales",
    "section": "1.3 Modelo lineal de regresión múltiple",
    "text": "1.3 Modelo lineal de regresión múltiple\nCuando se dispone de más variables que podrían explicar la respuesta, entonces podemos plantear un modelo múltiples variables explicativas.\nDesde el punto de vista estadístico, surgen unas cuantas preguntas:\n\n¿Qué variables incluir en el modelo?\n¿Están dichas variables explicativas correlacionadas entre ellas?\n¿O interaccionan entre sí?\n¿Hay curvatura en la respuesta?\n…\n\nPor ello, la regresión múltiple tiene más retos que la simple. Entre otros:\n\nEs muy habitual tener demasiadas variables explicativas. Hay que seleccionar cuidadosamente cuáles incluir. El Capítulo 5 se dedica a los métodos de selección de variables.\nTambién el tener pocas observaciones, valores de la variable respuesta, frente al número elevado de parámetros del modelo… más si hay interacciones, si no se dispone de todas las combinaciones posibles de las variables explicativas, etc. Puede hacer que el modelo sea inestimable (Sección 1.3.4).\nSi hay correlación entre las variable explicativas, entonces aportan información redundante. Se puede detectar (Sección 1.3.5) y tratar de solucionar (Capítulo 5).\nSi hay curvatura se debe acudir a modelos lineales, pero de regresión no lineal (Sección 1.5.3).\nLa mayor parte de los estudios son observacionales, no experimentales (Capítulo 2).\n…\n\nRecordemos, …Todos los modelos son falsos… Equivalentemente, el modelo perfecto y exacto no existe. Pero, algunos modelos son mejores que otros, y, en un modelo, la sencillez es un acierto (principio de parsimonia).\nTipos de modelos:\n\nSaturado: Mismo número de observaciones que de parámetros. Ajuste perfecto. Grados de libertad 0.\nMaximal: Contiene p variables y sus interacciones. Muchos de estos términos son despreciables. Grados de libertad \\(n-p-1\\).\nMinimal y Adecuado: Contiene las variables e interacciones significativas. Grados de libertad \\(n - p'-1\\).\nModelo Nulo: Un único parámetro, \\(\\beta_0 = \\bar{y}\\). Grados de libertad \\(n-1\\).\n\n\n1.3.1 Estimación MC\nEn Faraway (2004), apartado 2.4, y en Peña (2002), apartado 7.3.2, se puede ver la obtención del estimador MC para los parámetros del modelo de regresión lineal múltiple. Es la solución de las ecuaciones normales: \\[\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}^\\top \\mathbf{Y} \\quad \\Longrightarrow  \\quad \\hat{\\beta}=(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{Y}\\] siempre que \\(\\mathbf{X}^\\top \\mathbf{X}\\) sea invertible.\nSale a relucir la matriz \\(H=\\mathbf{X}(\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top\\), denominada matriz de proyecciones (hat-matrix, que es idempotente y simétrica), pues es la proyección ortogonal de \\(\\mathbf{Y}\\) en el espacio generado por las \\(\\mathbf{X}\\). Con esta matriz, se pueden expresar:\n\nlos valores predichos o estimados: \\(\\hat{\\mathbf{Y}}=H\\mathbf{Y}=\\mathbf{X}\\hat{\\beta}\\)\nlos residuos: \\(\\hat{\\epsilon}=\\mathbf{Y}-\\mathbf{X}\\hat{\\beta}=(I-H)\\mathbf{Y}\\)\nla suma de cuadrados residual (RSS por sus siglas en inglés): \\(\\hat{\\epsilon}^\\top\\hat{\\epsilon}=\\mathbf{Y}^\\top (I-H)\\mathbf{Y}\\)\n\nSe puede decir que el propósito del modelo es representar, de la mejor manera posible, la complejidad de la respuesta, dada en el espacio \\(n\\)-dimensional, en un espacio más pequeño, el \\(k\\)-dimensional de las variables. Si el modelo se ajusta bien, la estructura de los datos queda capturada en esas \\(k\\) dimensiones, dejando la variación aleatoria en los residuos que pertenecen a un espacio de dimensión \\(n-k\\).\n\n\n1.3.2 Bondad de ajuste\nEn el caso de regresión lineal múltiple la bondad de ajuste se puede medir con el coeficiente de determinación, \\(R^2\\), cuya fórmula es: \\[ R^2 = \\dfrac{\\text{Variabilidad explicada (VE)}}{\\text{Variabilidad total (VT)}}=\\dfrac{\\sum (\\hat y_i - \\bar y)^2 }{\\sum (y_i - \\bar y)^2 }\\] A partir de \\(R^2\\) se obtiene el coeficiente de correlación múltiple \\(R\\).\nEl \\(R^2\\) ajustado/corregido es una corrección para suavizar el comportamiento de \\(R^2\\) (que siempre aumenta al incluir en el modelo más variables explicativas). Consiste en considerar varianzas (medias de sumas de cuadrados, considerando sus respectivos grados de libertad, en lugar de sumas de cuadrados que se utilizan en \\(R^2\\)): \\[ R^2_{\\text{corregido}} = 1 - \\dfrac{\\text{Varianza residual}}{\\text{Varianza de $y$}} = R^2 - (1 - R^2) \\dfrac{k}{n+k-1}\\] Así, el \\(R^2\\) corregido de una regresión múltiple siempre será menor que el \\(R^2\\), incluso podría tomar valores negativos.\n\n\n1.3.3 Teorema de Gauss-Markov\nHay varias razones para usar el estimador MC de \\(\\beta\\).\n\nes el resultado de una proyección ortogonal sobre el espacio del modelo (interpretación geométrica).\nes también el estimador de máxima verosimilitud si los errores son independientes e idénticamente distribuidos siguiendo una normal.\nes el mejor estimador lineal insesgado (BLUE) según el teorema de Gauss-Markov.\n\n\nTeorema de Gauss-Markov\nSupongamos que \\(E[\\boldsymbol{\\varepsilon}] = 0\\), \\(\\text{Var}(\\boldsymbol{\\varepsilon}) = \\sigma^2 \\mathbf{I}\\) y que la parte estructural del modelo, \\(E[\\mathbf{y}] = \\mathbf{X} \\boldsymbol{\\beta}\\), es correcta.\nSea \\(\\mathbf{c}^\\top \\boldsymbol{\\beta}\\) una función estimable, entonces,\ndentro de la clase de todos los estimadores lineales insesgados de \\(\\mathbf{c}^\\top \\boldsymbol{\\beta}\\), el estimador de mínimos cuadrados tiene la varianza mínima y es único.\n\n\nUna función \\(\\mathbf{c}^\\top \\boldsymbol{\\beta}\\) es estimable, si, y solo si, existe una combinación lineal \\(\\mathbf{a}^\\top \\mathbf{y}\\) tal que \\(E[\\mathbf{a}^\\top \\mathbf{y}] = \\mathbf{c}^\\top \\boldsymbol{\\beta}\\).\n\nLa demostración puede encontrarse en Faraway (2004), apartado 2.6.\nLas funciones estimables incluyen predicciones de observaciones futuras, lo que explica por qué vale la pena considerarlas. Si la matriz \\(\\mathbf{X}\\) tiene rango completo, entonces todas las combinaciones lineales son estimables.\nConsideraciones adicionales\nEl teorema de Gauss-Markov recomienda usar mínimos cuadrados, salvo que haya una buena razón para no hacerlo. Como cuando los errores estén correlados y la varianza no sea constante, incumpliendo así los supuestos del teorema. En tal caso se deben usar mínimos cuadrados generalizados.\nSi los errores son no normales pero se comportan bien, típicamente con colas pesadas, puede que estimadores robustos, generalmente no lineales, funcionen mejor. O cuando se da multicolinealidad, se pueden preferir estimadores sesgados como la regresión ridge (véase Sección 5.4.1)\n\n\n1.3.4 Identificabilidad\nLa identificabilidad es un concepto clave en modelos estadísticos, especialmente en regresión. Un modelo es identificable si los parámetros del modelo pueden ser estimados de manera única a partir de los datos observados. En otras palabras, para cada conjunto de datos, hay una única solución para la estimación de los parámetros del modelo. Un modelo es no identificable si hay múltiples conjuntos de parámetros que pueden generar los mismos datos observados. Esto puede ocurrir por varias razones, como la presencia de variables redundantes o la falta de información suficiente en los datos para distinguir entre diferentes configuraciones de parámetros.\nSi \\(\\mathbf{X}^\\top \\mathbf{X}\\) no tiene rango completo, es decir, cuando sus columnas son linealmente dependientes, es singular y no puede invertirse, entonces habrá infinitas soluciones para las ecuaciones normales y el modelo será, al menos en parte, no identificable.\nLos paquetes estadísticos manejan la no identificabilidad de distintas formas. Algunos pueden devolver mensajes de error, y otros pueden ajustar el modelo porque los errores de redondeo eliminan la no identificabilidad exacta. En otros casos, se aplican restricciones, pero estas pueden ser diferentes de las que uno espera. Por defecto, R ajusta el modelo identificable más grande, eliminando variables en orden inverso al que aparecen en la fórmula del modelo.\n\n\n1.3.5 Multicolinealidad\nEn la regresión múltiple, hay que tener claro que la correlación se desea entre cada variable explicativa y la respuesta, y no entre ellas. De haber una alta correlación entre variables explicativas se presenta el problema denominado multicolinealidad. Aquellas variables que presentan multicolinealidad producen una aumento de la varianza del estimador, y, con ello, una peor precisión para detectar significatividad. Para resolverlo se puede acudir a la selección de variables (Capítulo 5), teniendo en cuenta medidas como el factor de inflación de la varianza (VIF) que vemos en este apartado. Ahora bien, conviene recordar que correlación no implica causalidad (ejemplos: delitos vs policías, limones vs accidentes,…).\nExisten distintas vías para detectar el grado de multicolinealidad existente. Por ejemplo, la matriz de correlaciones lineales y su determinante, el factor de inflación de la varianza, el número de condición, etc.\nNos centramos en el factor de inflación de la varianza (VIF). Su cálculo para cada parámetro \\(\\beta_i\\) se puede obtener al realizar una regresión (auxiliar) tomando como variable respuesta la variable asociada a dicho parámetro, \\(\\mathbf{X}_i\\), y como predictores el resto de variables explicativas: \\[\\mathbf{X}_i = \\alpha_0 + \\alpha_1 \\mathbf{X}_1 + \\ldots + \\alpha_{i-1}\\mathbf{X}_{i-1} + \\alpha_{i+1}\\mathbf{X}_{i+1} + \\ldots + \\alpha_k \\mathbf{X}_k.\\] Si el coeficiente de determinación de esta regresión auxiliar, \\(R_i^2\\), es alto, dicha variable \\(\\mathbf{X}_i\\) tiene una alta relación lineal con el resto de variables, tiene multicolinealidad. El factor de inflación de la varianza se define como: \\[\\text{VIF}(\\beta_i) = \\dfrac{1}{1 - R_i^2}\\] El VIF tomará valores entre 1 e \\(\\infty\\), indicando los valores cercanos a 1 ausencia de multicolinealidad. Si, por ejemplo, \\(R_i^2 = 0.9\\) (o \\(0.8\\)) se tendría VIF\\(=10\\) (o 5) valor que se toma en la práctica para alertar de alta multicolinealidad.\n\nEl lector interesado puede obtener más información en: Multicolinealidad. Detección, del interesante blog “Un rincón para R” de Román Salmerón (UGR).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#sec-LM-Airquality",
    "href": "Cap1-LM.html#sec-LM-Airquality",
    "title": "1  Modelos lineales",
    "section": "1.4 Caso práctico: airquality",
    "text": "1.4 Caso práctico: airquality\nEl primero de los casos prácticos de modelización lineal que se va a tratar en profundidad se basa en los datos airquality que contienen 153 medidas (de 6 variables) de calidad del aire en Nueva York. Entre otros, se estudia en Casero-Alonso y Durbán (2024), concretamente en: https://cdr-book.github.io/cap-lm.html#Casos. Aquí se presentan ejemplos ligeramente distintos y con algo más de detalle.\n\n1.4.1 Exploración de los datos\nAntes de comenzar el proceso de modelización lineal, es muy recomendable explorar los datos. El conjunto de datos airquality está disponible en la distribución base de R:\n\n?airquality #Para obtener más información sobre las variables, unidades, etc. \n\n\n#Primeras filas del data frame\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nComo se observa hay varios valores perdidos (NA: Not Available) entre los datos, lo que puede afectar a los resultados de la regresión. Su impacto debe estudiarse, pero sobrepasa el nivel de este curso.\nAdemás, al observar detenidamente los valores de la variable Day se puede inferir que los datos son temporales, lo que requiere un análisis específico (de Series Temporales, que también sobrepasa el alcance de este curso). El tener datos temporales hace que se incumpla, de partida, desde el plano teórico/conceptual, el supuesto de independencia (Sección 1.2.9.4).\nAun con esta situación (valores perdidos, datos temporalmente dependientes) se analizan aquí diversos modelos lineales. Eso sí, no tiene sentido intentar explicar la influencia lineal de la variable Day en el resto de variables, por ejemplo, por el hecho de que los días \\(1\\), \\(2\\), etc. de meses distintos no son homogéneos, etc.\nProcedemos a obtener resúmenes numéricos y gráficos:\n\n## Resúmenes numéricos de las variables\nsummary(airquality)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\n## Estructura (formato) del data frame\nstr(airquality)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n## Resumen gráfico de relaciones pareadas\npairs(airquality, upper.panel = panel.smooth)\n\n\n\n\n\n\n\n\nDel resumen numérico se obtiene que hay 2 variables Ozone y Solar.R que contienen NAs, principalmente la primera con 37 valores no disponibles. Estos resúmenes, junto con la visualización de la estructura (y la consulta de las unidades en las que están medidas) permiten determinar que las variables Ozone, Solar.R, Wind y Temp se pueden considerar cuantitativas continuas (obsérvense sus rangos) a los efectos de modelización lineal, aunque sólo Wind esté definida como num (y el resto como int). Sobre Temp, viendo el rango de sus valores (de 56 a 97), no parece que estén en ºC. ¿Y cómo considerar a la variable Month? El tratamiento más adecuado es como variable cualitativa, dado que, aunque vemos valores numéricos, de 5 a 9, no puede interpretarse como una variable continua en la que tenga sentido incrementar 1 unidad. Además, otorgarle valores 5 a 9 es un convenio para tratarlas por ordenador de una manera más cómoda, pero realmente sus valores son mayo, junio… Queda así más claro que no tiene sentido aumentar 1 unidad, por ejemplo, cuando estamos en el mes 12.\nDe los diagramas de dispersión, al incluir el argumento panel.smooth se pueden observar líneas de tendencias suavizadas de los datos. Se aprecia que casi ninguna de las relaciones entre las variables numéricas es lineal, sólo lo parece Wind frente a Temp. Los gráficos que involucran a la variable Month se aprecian distintos al resto, por los pocos valores de dicha variable, mientras que los que involucran a Day no reflejan lo mismo.\n\nAlternativas: Existen distintas funciones/paquetes más o menos sofisticados que realizan este análisis exploratorio de distintas maneras. El lector interesado puede explorar:\n- El paquete skimr y su función skim().\n- El paquete summarytools y su función dfSummary().\n- …\n\n\nPor ejemplo, con las siguientes funciones se pueden obtener gráficos complementarios al gráfico obtenido anteriormente con pairs().\n\n\n#Es necesario tener los paquetes instalados previamente\nlibrary(corrplot) \ncorrplot(cor(airquality, use = \"pairwise\"))\nlibrary(GGally)\nggpairs(airquality)\n#Para que aparezca los diagramas de dispersión \"arriba\"\nggpairs(airquality,\n         upper = list(continuous = wrap(\"points\", alpha = 0.7)),  \n         lower = list(continuous = wrap(\"cor\", size = 4)),  \n         diag = list(continuous = wrap(\"densityDiag\")))  \n\n\n\n1.4.2 lm() simple\nPara ilustrar la teoría de las secciones anteriores se va a realizar aquí una regresión lineal simple. La función lm() de R proporciona, a partir de los datos disponibles, la estimación de los parámetros del modelo que se especifique. También se pueden obtener, aplicando distintas funciones, la significación de dichas estimaciones, sus intervalos de confianza, predicciones, etc. Así como los gráficos de diagnóstico.\nModelización\nA diferencia del libro CDR, se considera para empezar un modelo de regresión simple. De entre los posibles modelos nos decantamos por intentar explicar la concentración de Ozono en función de la radiación solar: \\[Ozone = \\beta_0 + \\beta_1 Solar.R + \\epsilon\\]\n\nEl lector tiene aquí una buena tarea conceptual, la de ejercitarse en plantear modelizaciones lineales, que ¡tengan sentido práctico!\n¿Tiene sentido explicar/predecir Ozone en función de los valores de Solar.R observados? ¿Y al revés? ¿O explicar/predecir Day en función de Ozone? …\n\nEn R definiríamos así el modelo anterior:\n\nmodelo &lt;- Ozone ~ Solar.R\n\n\nLa sintaxis básica (regresión simple) es respuesta ~ explicativa.\nLa extensión a regresión múltiple es directa: resp ~ explica1 + explica2 indicaría un modelo con predictores explica1 y explica2, y así sucesivamente.\nEn el ejemplo de regresión múltiple se indican algunos “trucos/atajos” para definir modelos.\n\n\nPregunta\n¿Cómo obtener la regresión de proporcionalidad directa con R?\n\nEstimación\nEn R pueden obtenerse de varias maneras utilizando la función lm():\n\n#Opción 1: aprovechando la definición anterior del modelo\n#lm(modelo, data = airquality) \n#Opción 2: directa -&gt; lectura más clara\nlm(Ozone ~ Solar.R, \n   data = airquality)\n\n\nCall:\nlm(formula = Ozone ~ Solar.R, data = airquality)\n\nCoefficients:\n(Intercept)      Solar.R  \n    18.5987       0.1272  \n\n\nAquí se pueden ver las estimaciones para los dos parámetros obtenidas a partir de los datos (omitiendo las observaciones con valores NA).\n\nLa función lm() aplicada a un modelo simple y ~ x (o múltiple) requiere que los datos de las variables y y x estén en el Environment, o se especifique en data el conjunto de datos en el que están las dos variables.\n\nDiagrama de dispersión y recta estimada\nDado que se está analizando la relación entre dos variables, esta se puede visualizar fácilmente en un diagrama de dispersión, al que se puede añadir la recta estimada a partir de los datos.\n\npar(pty = \"s\") ## \"p\"lot \"ty\"pe \"s\"quare (recomendado para gráficos de dispersión)\nplot(airquality$Solar.R, airquality$Ozone)\nabline(a=18.5987, b=0.1272, col = \"red\") ## a = intercept, b = slope\n\n\n\n\n\n\n\n\nEn problemas de regresión múltiple con dos variables explicativas se puede obtener un diagrama de dispersión en 3 dimensiones y añadirle el plano de regresión, pero suele ser compleja su visualización. Con más variables es imposible obtener tales visualizaciones, diagramas de dispersión en \\(k\\) dimensiones e hiperplanos de regresión, lo que conduce a abordar el problema de regresión lineal múltiple mediante un proceso de abstracción.\nAnálisis de residuos\nValoremos la adecuación del modelo examinando los residuos. Primero guardamos el ajuste/estimación basado en los datos en un objeto, que denominamos rls, para su uso posterior:\n\nrls &lt;- lm(Ozone ~ Solar.R, data = airquality)\npar(mfrow = c(2, 2), #presenta los gráficos en formato 2x2\n    pty = \"s\",\n    mex = 0.66,\n    cex = 0.75)\nplot(rls)\n\n\n\n\n\n\n\n\n\nEl primero de los 4 gráficos de diagnóstico (Residuals vs Fitted) refleja heterocedasticidad (varianza no constante, forma de embudo), tal y como ya se podía apreciar en el diagrama de dispersión de Solar.R frente a Ozone que se obtuvo anteriormente. En este gráfico de aquí, conforme aumenta el valor de los valores estimados (fitted), la dispersión de los residuos se hace más grande, salvo en la parte final. Este gráfico también sirve para comprobar si se cumple (o no) la linealidad, aquí, la linea suavizada incluida es bastante plana, salvo, de nuevo, en la parte final.\nEl gráfico que mejor refleja esa heterocedasticidad es el tercero (Scale-Location), donde la línea roja de tendencia dista de la horizontalidad (que reflejaría homocedasticidad) teniendo una pendiente positiva (ligera, pero apreciable).\nRespecto a la normalidad, a la vista del segundo gráfico (Q-Q Residuals), se observa una clara desviación de la linea recta punteada (que marcaría el ajuste perfecto a la distribución normal), sobre todo en la parte superior derecha. Para reforzar la impresión de este gráfico se puede acudir a otra visualización y a un contraste. Lo más apropiado es un histograma y el contraste de Shapiro-Wilk:\n\nhist(residuals(rls))\n\n\n\n\n\n\n\nshapiro.test(residuals(rls))\n\n\n  Shapiro-Wilk normality test\n\ndata:  residuals(rls)\nW = 0.91418, p-value = 2.516e-06\n\n\nEl histograma refleja una clara asimetría, que dista de la simetría de la distribución normal. Y el p-valor del contraste lleva claramente a rechazar la normalidad.\nEl último de los 4 gráficos de residuos (Residuals vs Leverage) señala las observaciones 117 y 62 como los valores con más influencia en los resultados de la regresión. Pero en este último gráfico no presentan un valor grande de la distancia de Cook, que sirve para medir esa influencia. De hecho, no aparecen en el gráfico ni las lineas discontinuas que marcan los límites para considerar un valor grande de la distancia, \\(D_i &gt; 0.5\\) y \\(D_i&gt;1\\), y por lo tanto una posible observación influyente.\n\nLos valores de leverage se pueden obtener usando la función hatvalues().\n\n\nla función plot() aplicada a un objeto de tipo lm en realidad calcula 6 gráficos, que se pueden obtener así:\n\n\nplot(rls, which = 1:6)\n\n\nEn resumen, es claro que hay dos observaciones que pueden influir en los resultados de la regresión, pero también es claro que los problemas observados (heterocedasticidad, falta de normalidad…) no provienen sólo de esos dos datos.\n\nPregunta\nRepita el análisis manteniendo Ozone como variable respuesta/dependiente, cambiando la variable explicativa Solar.R por Wind o Temp. ¿Aprecia algún cambio?\n\n\nAlternativas: Existen distintas funciones/paquetes más o menos sofisticados que realizan este análisis de residuos de distintas maneras. El lector interesado puede explorar:\n- El paquete ggfortify y su función autoplot().\n- El paquete performance y su función check_model().\n- …\n\n\n#Es necesario tener los paquetes instalados previamente\nlibrary(\"ggfortify\")\nautoplot(rls) +\n  theme_minimal()\nlibrary(performance) \nlibrary(see)\ncheck_model(rls)\n\nInterpretación e Inferencia\nSupongamos que fuese todo correcto. El siguiente paso sería el de averiguar la significación de los parámetros, interpretarlos y obtener alguna predicción de interés. Aplicamos la función summary() a nuestro objeto rls:\n\nsummary(rls)\n\n\nCall:\nlm(formula = Ozone ~ Solar.R, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.292 -21.361  -8.864  16.373 119.136 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 18.59873    6.74790   2.756 0.006856 ** \nSolar.R      0.12717    0.03278   3.880 0.000179 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.33 on 109 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.1213,    Adjusted R-squared:  0.1133 \nF-statistic: 15.05 on 1 and 109 DF,  p-value: 0.0001793\n\n\nEn esta salida obtenemos, además de las estimaciones, \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) (columna Estimate), mucha información para su interpretación. En la columna Pr(&gt;|t|) se pueden ver los p-valores de cada parámetro. El del parámetro asociado a la variable Solar.R es muy pequeño, pudiéndose concluir, en caso de que el modelo fuese válido, que la variable Solar.R influye significativamente en la variable Ozone. De la misma manera, el parámetro \\(\\beta_0\\) ((Intercept), intersección/ordenada en el origen) es significativo, lo que indica que el modelo no pasa por el origen de coordenadas. Los p-valores obtenidos provienen, de la estimación del estadístico \\(t\\) que se encuentra en la columna t value, que a su vez se basa en la mencionada estimación de cada parámetro (columna Estimate) y el error estándar (columna Std. Error) (véase Sección 1.2.6). A partir de estos dos valores y la distribución \\(t\\) correspondiente, se pueden obtener los intervalos de confianza “a mano” (paso a paso con R, aplicando las fórmulas de la Sección 1.2.11), pero se dispone de la función confint():\n\nconfint(rls)\n\n                 2.5 %     97.5 %\n(Intercept) 5.22460110 31.9728544\nSolar.R     0.06220373  0.1921268\n\n\nSi el modelo fuese válido se pasaría a la interpretación de estos parámetros… Primero, el signo de \\(\\hat{\\beta}_1\\), al ser positivo, indica que conforme aumenta Solar.R también aumenta Ozone. Esta conclusión es clara aunque el modelo no sea válido. Ahora bien, no sólo se obtiene esa relación positiva, sino la magnitud de dicha relación. Así, el cambio de 1 unidad en Solar.R implicaría un cambio medio de aproximadamente 0.1272 unidades en Ozone, con un valor medio de 18.5987 unidades de Ozone en ausencia de radiación solar (¡Ojo! Con los datos disponibles, el mínimo valor de Solar.R es 7, según la tabla del resumen numérico, por lo que estamos hablando de una extrapolación que necesitaría del conocimiento de un experto en la materia para dilucidar su apropiada y/o oportuna interpretación).\nBondad de ajuste\nEn la salida anterior de la función summary() también se pueden observar resultados que ayudan a proporcionar la bondad de ajuste de la regresión simple planteada. El valor que resume la bondad de ajuste es el Multiple R-squared, aunque, como se ha mencionado en Sección 1.3.2, para comparar entre modelos conviene hacerlo con el Adjusted R-squared que arroja un valor de\n\nrls.summ &lt;- summary(rls)  #guardamos el objeto generado con summary()\n#con names() listamos distintos componentes generados con summary() \nnames(rls.summ)           #también se puede consultar con ?summary.lm \n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\"  \"na.action\"    \n\nsummary(rls)$adj.r.squared\n\n[1] 0.1132809\n\n\nEste 11.3% se puede considerar “pobre”. La variable Solar.R explica “pobremente” el Ozone. No obstante, el p-valor global del modelo que se muestra en la última linea de la salida de summary() es significativo, indicando que la recta de regresión es significativa, esto es, que es mejor que proporcionar sólo la media de Ozone para cualquier valor de Solar.R que sería el modelo básico.\n\nPregunta\n¿Sabe decir porqué el p-valor de la recta de regresión coincide con el del parámetro asociado a Solar.R?\n\nTambién se puede observar el valor del Residual standard error y sus grados de libertad, \\(n-2\\) (véase Sección 1.2.4). Aunque, en este caso, hay que tener en cuenta el mensaje que aparece en la salida: 42 observations deleted due to missingness, que indica que no se han considerado aquellas observaciones en las que cualquier variable tiene un NA. Así, los grados de libertad quedan en \\(109\\).\nPredicción\nA pesar de que el modelo no es idóneo para explicar el fenómeno, puede que la regresión lineal sirva para el propósito de predecir valores. Pasamos a ilustrar como se obtendrían con R predicciones a partir de la recta estimada:\n\npredict(rls, data.frame(Solar.R = c(10, 100, 300)),\n        interval = \"confidence\")\n\n       fit       lwr      upr\n1 19.87038  7.076164 32.66460\n2 31.31525 23.247138 39.38337\n3 56.74831 47.222078 66.27454\n\npredict(rls, data.frame(Solar.R = c(10, 100, 300)),\n        interval = \"prediction\")\n\n       fit        lwr       upr\n1 19.87038 -43.537910  83.27867\n2 31.31525 -31.310729  93.94124\n3 56.74831  -6.082164 119.57878\n\n\nCon el argumento interval = \"confidence\" obtenemos intervalos de confianza (para valores medios), con interval = \"prediction\" obtenemos intervalos de predicción (para valores individuales) para la predicción de Ozone dados valores de Solar.R. Concretamente para 3 valores: 10, 100 y 300, todos ellos en el rango de valores observados de la variable. La estimación media (fit) coincide en ambos casos, diferenciándose en la amplitud de los intervalos, mucho mayores para los intervalos de predicción, dado que, para el mismo valor de Solar.R, una observación individual puede alejarse mucho más de la media, que una media de varias observaciones .\n\n\n1.4.3 lm() múltiple\nModelización y estimación\nSe aborda aquí una regresión lineal múltiple ligeramente distinta a la del libro CDR. Directamente en formato de R:\n\nrlm &lt;- lm(Ozone ~ Solar.R + Wind + Temp + Month, data = airquality) \n## Equivalentemente\n#rlm &lt;- lm(Ozone ~ . - Day, data = airquality)\n\n\nTrucos/atajos: Para no escribir todas las variables, se hace uso de .: lm(y ~ ., data) indica una regresión con y como variable respuesta, y el resto de variables de data como predictores lineales.\nAdemás, se ha usado - para quitar la variable indicada.\n\nAnálisis de residuos\n\npar(mfrow = c(2, 2), #presenta los gráficos en formato 2x2\n    pty = \"s\",\n    mex = 0.66,\n    cex = 0.75)\nplot(rlm)\n\n\n\n\n\n\n\n\n\nPregunta\n¿Se ha mejorado respecto a la regresión simple?\n\nNótese que ahora, en el 4º gráfico (Residuals vs Leverage) sí que aparece la línea punteada del valor 0.5 de la distancia de Cook, que no llega a alcanzar ninguno de los datos.\nInterpretación\n\nsummary(rlm)\n\n\nCall:\nlm(formula = Ozone ~ Solar.R + Wind + Temp + Month, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.870 -13.968  -2.671   9.553  97.918 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -58.05384   22.97114  -2.527   0.0130 *  \nSolar.R       0.04960    0.02346   2.114   0.0368 *  \nWind         -3.31651    0.64579  -5.136 1.29e-06 ***\nTemp          1.87087    0.27363   6.837 5.34e-10 ***\nMonth        -2.99163    1.51592  -1.973   0.0510 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.9 on 106 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6199,    Adjusted R-squared:  0.6055 \nF-statistic: 43.21 on 4 and 106 DF,  p-value: &lt; 2.2e-16\n\n\nSi del análisis de residuos queda validado el modelo, se pueden interpretar los parámetros del modelo estimado, ¿qué explican/aportan?, o ¿cómo influyen?, cada uno a la respuesta (siempre en términos medios). De la salida anterior se desprende que, de las 4 variables consideradas, la más significativa es Temp, seguida de Wind (ambas con 3 asteriscos), y también es significativa Solar.R, pero a otro nivel (sólo un asterisco, por un valor inferior al 5% pero superior al 1%, cuando en la regresión simple tenía un p-valor más pequeño/significativo, 2 asteriscos). Por último, Month es significativa pero a un nivel del 10% de significación. También el término independiente es significativo.\nSobre la interpretación de los parámetros, se debe tener en cuenta las unidades en las que están recogidas cada una de las variables. Como se ha mencionado, cualquier cambio de escala influirá en dichas estimaciones, y generalmente en sus Std. Error (salvo traslaciones de las variables).\nAdemás, la interpretación de las estimaciones (magnitud) como cambios en la variable respuesta tiene ahora un cambio sustancial. El cambio en la variable respuesta de 1 unidad de una de las variables explicativas está condicionado a que el resto de variables explicativas no cambien, lo que en el contexto de la Economía se denomina ceteris paribus, y que puede que sea imposible de cumplirse. Por ejemplo, el parámetro asociado a Solar.R es, para este modelo de regresión lineal múltiple, 0.0496 menos de la mitad de magnitud que en el caso de regresión simple. Ese sería el cambio medio en las unidades de Ozone para cambios de 1 unidad en Solar.R siempre que Wind, Temp y Month tengan los mismos valores, o, dicho de otro modo, para otro día qué tuviese los mismos valores de Wind, Temp y Month, y cambiase 1 unidad Solar.R. Si es que es factible el cambio de Solar.R con los mismo valores del resto de variables.\nOtra lectura que se debe hacer sobre la interpretación de la magnitud del parámetro estimado es que, al ser independiente del resto de valores de las variables, es, a su vez, para cualquier combinación de dichos valores. Por ejemplo, para valores de Wind altos, medios o bajos, combinado con valores de Temp altos, medios o bajos, o para cualquier Month (entre 5 y 9, que es el rango observado).\n\nPregunta\n¿Cómo interpretaría el valor negativo del término independiente si se trata de la concentración de ozono en partes por billón?\n\nBondad de ajuste\nLa bondad del ajuste de este modelo, ha mejorado mucho comparada con la regresión simple anterior, pasando ahora a un Adjusted R-squared de 0.606.\nSi nuestro interés sólo fuese predecir valores de Ozone, este modelo podría servirnos.\nPredicción\n¿Cómo se pueden obtener predicciones con un modelo de regresión múltiple? Se deben proporcionar valores a cada una de las variables del modelo:\n\nnuevas.observ &lt;- data.frame(Solar.R = c(110, 110),\n                            Wind    = c(8, 20), \n                            Temp    = c(72, 85), \n                            Month   = c(6, 6))\npredict(rlm, newdata = nuevas.observ,\n        interval = \"confidence\" )\n\n       fit       lwr      upr\n1 37.62288 30.382618 44.86315\n2 22.14613  4.912182 39.38008\n\n\n\n1.4.3.1 Predictores cualitativos\nEl uso de predictores cualitativos debe tratarse de forma diferencial en R. Deben definirse como factor, (factor()). Así, al estimar R los parámetros del modelo donde se incluya dicha variable, generará automáticamente variables ficticias, variables dummys. La función contrasts() permite conocer la codificación que por defecto usa R para dichas variables ficticias (aunque se puede modificar). Para ilustrarlo, redefinimos la variable Month en el conjunto de datos airquality\n\nairquality$Month &lt;- factor (airquality$Month) \ncontrasts(airquality$Month)\n\n  6 7 8 9\n5 0 0 0 0\n6 1 0 0 0\n7 0 1 0 0\n8 0 0 1 0\n9 0 0 0 1\n\n\nLa salida indica que el valor 5 de Month se toma como valor de referencia (en su fila aparecen todo ceros), y se generan 4 variables dummy, una para cada uno de los meses restantes (del 6 al 9).\n\nEn R las variables definidas como factor toma como referencia la primera categoría al ordenar los valores de la variable, bien alfabéticamente (a, b, c…) o bien numéricamente de menor a mayor, aunque se puede especificar otro orden (véase Sección 1.5.1).\n\nAjustamos de nuevo el modelo de regresión múltiple.\n\nrlm.cualit &lt;- lm(Ozone ~ . - Day, \n                 data = airquality)\nsummary(rlm.cualit)\n\n\nCall:\nlm(formula = Ozone ~ . - Day, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.344 -13.495  -3.165  10.399  92.689 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -74.23481   26.10184  -2.844  0.00537 ** \nSolar.R       0.05222    0.02367   2.206  0.02957 *  \nWind         -3.10872    0.66009  -4.710 7.78e-06 ***\nTemp          1.87511    0.34073   5.503 2.74e-07 ***\nMonth6      -14.75895    9.12269  -1.618  0.10876    \nMonth7       -8.74861    7.82906  -1.117  0.26640    \nMonth8       -4.19654    8.14693  -0.515  0.60758    \nMonth9      -15.96728    6.65561  -2.399  0.01823 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.72 on 103 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6369,    Adjusted R-squared:  0.6122 \nF-statistic: 25.81 on 7 and 103 DF,  p-value: &lt; 2.2e-16\n\n\nSe puede observar que se muestran parámetros para las 4 variables dummy de los 5 valores de Month. Como se ha indicado, el primer valor de la variable (el 5) lo toma como referencia (se podría cambiar si se quiere otro mes de referencia) y genera 4 variables dicotómicas que reflejan el cambio de la categoría de referencia a cada una de ellas. Matemáticamente el modelo que se estima es: \\[\\begin{align*}\nOzone = &\\beta_0 + \\beta_1 Solar.R + \\beta_2 Wind + \\beta_3 Temp + \\\\\n&\\beta_6 Month6 +  \\beta_7 Month7 +  \\beta_8 Month8 +  \\beta_9 Month9 + \\epsilon  \n\\end{align*}\\] Así, se entiende ahora mejor la visualización de contrasts() obtenida anteriormente. Cuando Month6, Month7… toman todos el valor 0 la estimación que se obtiene es del mes de mayo. Si la variable dummy Month6 toma el valor 1, y las otras 3 toman el valor 0, se obtiene la diferencia media de Ozone respecto al mes de mayo, ceteris paribus. Por lo tanto, que el parámetro asociado a Month6 sea negativo implica que, para cualquier combinación fija de valores del resto de variables, en junio disminuye la media de Ozone respecto a mayo (aunque justo este parámetro/cambio ¡no es significativo!).\n\nPregunta\n¿Como se interpreta ahora el valor del Intercept?\n¿Y cómo se explica el valor negativo de Month6 a la vista del resumen gráfico que se obtuvo con pairs()?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#sec-LM-Boston",
    "href": "Cap1-LM.html#sec-LM-Boston",
    "title": "1  Modelos lineales",
    "section": "1.5 Caso práctico: Boston",
    "text": "1.5 Caso práctico: Boston\nEl segundo de los casos prácticos de modelización lineal que se va a tratar en profundidad utiliza los datos Boston, recogidos en varios paquetes, por ejemplo, en ISLR2 asociado con el libro: https://www.statlearning.com. Los datos recogen valores relacionados con viviendas para 506 distritos censales de Boston. El estudio de la modelización lineal se puede encontrar en: https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch3-linreg-lab.html y el Rscript asociado en: https://hastie.su.domains/ISLR2/Labs/R_Labs/Ch3-linreg-lab.R. De nuevo, aquí se presentan ejemplos ligeramente distintos y con algo más de detalle.\n\n1.5.1 Exploración de los datos\n\nlibrary(ISLR2)\n#?Boston #Para obtener más información sobre las variables, unidades, etc. \nhead(Boston)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94 33.4\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33 36.2\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21 28.7\n\n\nAhora no parece haber NAs (en el resumen numérico se va a ver que no hay). No obstante, se puede comprobar con:\n\nsum(is.na(Boston))\n\n[1] 0\n\n\nEstructura y resúmenes\n\nstr(Boston)\n\n'data.frame':   506 obs. of  13 variables:\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n\nsummary(Boston)\n\n      crim                zn             indus            chas        \n Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000  \n 1st Qu.: 0.08205   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000  \n Median : 0.25651   Median :  0.00   Median : 9.69   Median :0.00000  \n Mean   : 3.61352   Mean   : 11.36   Mean   :11.14   Mean   :0.06917  \n 3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000  \n Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000  \n      nox               rm             age              dis        \n Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130  \n 1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02   1st Qu.: 2.100  \n Median :0.5380   Median :6.208   Median : 77.50   Median : 3.207  \n Mean   :0.5547   Mean   :6.285   Mean   : 68.57   Mean   : 3.795  \n 3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08   3rd Qu.: 5.188  \n Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127  \n      rad              tax           ptratio          lstat      \n Min.   : 1.000   Min.   :187.0   Min.   :12.60   Min.   : 1.73  \n 1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40   1st Qu.: 6.95  \n Median : 5.000   Median :330.0   Median :19.05   Median :11.36  \n Mean   : 9.549   Mean   :408.2   Mean   :18.46   Mean   :12.65  \n 3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20   3rd Qu.:16.95  \n Max.   :24.000   Max.   :711.0   Max.   :22.00   Max.   :37.97  \n      medv      \n Min.   : 5.00  \n 1st Qu.:17.02  \n Median :21.20  \n Mean   :22.53  \n 3rd Qu.:25.00  \n Max.   :50.00  \n\n\nA la vista de la estructura y el resumen numérico, se pueden considerar continuas casi todas las variables (internamente están definidas en R como double o int, para manejar números reales con formato de coma flotante de doble precisión que requieren más espacio en memoria o enteros, respectivamente). Llama la atención el resumen de las variables zn, chas y rad. La variable zn está definida internamente como double porque contiene algunos valores con decimales (y mirando su definición es una proporción). Ahora bien, su mediana es 0, por lo que al menos la mitad de los 506 valores son 0, de hecho el 0 aparece 372 veces. Esto, sin duda tendrá un impacto al considerarlo en los modelos. Por su parte las variables chas y rad tienen formato int, pero acudiendo a su definición, chas es dicotómica (lo que también da sentido que su mediana sea 0 y su media se 0.0692), mientras que rad es un índice, que toma valores entre 1 y 24. Se dejan zn y rad con valores numéricos, pero, para su apropiado manejo en los modelos, es oportuno convertir chas en variable factor de R:\n\nBoston$chas &lt;- factor(Boston$chas)\ncontrasts(Boston$chas)\n\n  1\n0 0\n1 1\n\n\nComo se puede ver con la función contrasts() en los casos de variable dicotómica sólo se genera una variable dummy, quedando por defecto el valor 0 como valor de referencia. Rizando el rizo, vamos a cambiar la definición del valor de referencia para luego observar su impacto en la modelización.\n\nBoston$chas &lt;- factor(Boston$chas, \n                      levels = c(1, 0))\ncontrasts(Boston$chas)\n\n  0\n1 0\n0 1\n\n\n\npairs(Boston, upper.panel = NULL,\n      lower.panel = panel.smooth)\n\n\n\n\n\n\n\n\nA la vista del resumen gráfico, diagramas de dispersión de pares de variables, casi todas las relaciones entre pares de variables parecen no lineales. Como se puede apreciar, la dicotomía de la variable chas genera diagramas de dispersión muy distintos al resto. Y las variables rad y tax llaman la atención por presentar dos grupos de valores alejados, especialmente rad. También, la variable nox presenta dos grupos de valores pero con menor separación entre grupos.\n\n\n1.5.2 lm() múltiple\nModelización y estimación\nLa variable de interés a modelizar es medv (“median value”: valor mediano de las casas ocupadas por sus propietarios, en $1000s). En el libro ISLR2 utilizan las otras 12 variables explicativas como predictores. Aquí se consideran sólo unas cuantas (ojo con lo que recogen cada una de las variables… ¿age?):\n\nrlm.Boston &lt;- lm(medv ~ lstat + rm + age + tax + chas, #chas definida como factor\n                 data = Boston)\nsummary(rlm.Boston)\n\n\nCall:\nlm(formula = medv ~ lstat + rm + age + tax + chas, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.358  -3.345  -1.124   1.930  30.073 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.323355   3.270507   1.322    0.187    \nlstat       -0.588092   0.055117 -10.670  &lt; 2e-16 ***\nrm           4.954530   0.440829  11.239  &lt; 2e-16 ***\nage          0.014624   0.011379   1.285    0.199    \ntax         -0.007009   0.001756  -3.990 7.58e-05 ***\nchas0       -3.898106   0.955381  -4.080 5.24e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.372 on 500 degrees of freedom\nMultiple R-squared:  0.6622,    Adjusted R-squared:  0.6588 \nF-statistic:   196 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\n\nSe han unificado la modelización y estimación, como se hace generalmente. Posteriormente se realizará el análisis de residuos que permitirá dar validez al modelo considerado.\nEn este caso, los parámetros asociados a las variables salen significativos, excepto uno. Este hecho, conduce a eliminar dicha variable del modelo, para obtener uno con sólo variables influyentes sobre medv.\nLa consecuencia de que algunos parámetros no sean significativos también se puede apreciar en sus intervalos de confianza… Los no significativos incluyen el 0.\n\nconfint(rlm.Boston)\n\n                   2.5 %       97.5 %\n(Intercept) -2.102274869 10.748985289\nlstat       -0.696380539 -0.479802930\nrm           4.088424014  5.820635797\nage         -0.007732991  0.036981489\ntax         -0.010459310 -0.003557925\nchas0       -5.775162051 -2.021050701\n\n\n\n1.5.2.1 Quitando predictores\nComo ilustración del proceso iterativo (manual) de modelización, pasamos a estimar un nuevo modelo en el que eliminamos la variable no significativa que se obtuvo en el modelo anterior rlm.Boston, esto es quitando age:\n\nrlm.BostonModif &lt;- lm(medv ~ lstat + rm + tax + chas, data = Boston)\n#Alternativamente, usando la función `update()`.\n#rlm.BostonModif &lt;- update(rlm.Boston, ~ . - age)\nsummary(rlm.BostonModif)\n\n\nCall:\nlm(formula = medv ~ lstat + rm + tax + chas, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.484  -3.400  -1.158   1.909  30.849 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.154797   3.270001   1.271 0.204468    \nlstat       -0.554277   0.048462 -11.437  &lt; 2e-16 ***\nrm           5.060589   0.433317  11.679  &lt; 2e-16 ***\ntax         -0.006412   0.001695  -3.783 0.000174 ***\nchas0       -4.076908   0.945811  -4.310 1.96e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.376 on 501 degrees of freedom\nMultiple R-squared:  0.6611,    Adjusted R-squared:  0.6584 \nF-statistic: 244.3 on 4 and 501 DF,  p-value: &lt; 2.2e-16\n\n\nAhora el modelo sale con todos los parámetros significativos, con un Adjusted R-squared ligeramente peor. No obstante el \\(R^2\\) ajustado es razonablemente bueno si el objetivo es predecir valores de Ozone.\n\nPregunta\n¿Cómo quitar varios predictores simultáneamente?\n¿Cómo hacer este proceso automáticamente? Véase el Capítulo 5\n\n\nPregunta\nObtenga este último modelo con la variable chas original (sin convertirla en factor) ¿Qué diferencias observa en la estimación del parámetro asociado a dicha variable?\n\nAnálisis de residuos\nValoremos la adecuación del último modelo examinando sus residuos.\n\npar(mfrow = c(2, 2), #presenta los gráficos en formato 2x2\n    pty = \"s\",\n    mex = 0.66,\n    cex = 0.75)\nplot(rlm.BostonModif)\n\n\n\n\n\n\n\n\n\nLos gráficos 1 y 3 (Residuals vs Fitted y Scale-Location) reflejan heterocedasticidad y no linealidad.\nEn el segundo gráfico (Q-Q Residuals), se observa una clara desviación de la normalidad (marcada por la linea de puntos). Veamos el histograma:\n\n\nhist(residuals(rlm.BostonModif))\n\n\n\n\n\n\n\n\nEl histograma refleja asimetría por lo que claramente rechazamos la normalidad.\n\nPor último en los 4 gráficos de residuos quedan señaladas las observaciones 369 y 373, junto con la 372 que aparece en 3 de los gráficos. Ahora bien, no tienen una distancia de Cook elevada para considerarlas influyentes. De nuevo, parece claro que los problemas comentados (heterocedasticidad, falta de normalidad…) no provienen sólo de esas observaciones.\n\n\n\n\n1.5.3 Extensiones/transformaciones de la regresión múltiple\nEl modelo lineal de regresión lineal múltiple permite considerar “constructos” de variables, es decir, considerar productos de dos variables, términos no lineales, etc. El tratamiento de esos casos con R es similar a lo visto hasta ahora, con el inconveniente de enfrentarse a su interpretación. Así, entre los “constructos” más habituales se encuentran:\n\nTérminos de interacción: La sintaxis lstat:age le dice a R que incluya un término de interacción entre lstat y age. La sintaxis lstat * age incluye como predictores simultáneamente lstat, age y la interacción lstat:age, es una abreviatura de lstat + age + lstat:age.\n\n\nsummary(lm(medv ~ lstat * age, data = Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\n\nPregunta\n¿Cómo se interpreta que la interacción salga significativa?\n\n\nTransformaciones polinómicas de los predictores: La sintaxis I(lstat^2) introduce en el modelo el predictor cuadrático de lstat.\n\n\nrlm.Boston2 &lt;- lm(medv ~ lstat + I(lstat^2), data = Boston)\nsummary(rlm.Boston2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nEl p-valor cercano a cero asociado con el término cuadrático sugiere que, su inclusión, conduce a un modelo mejorado. Pero, obviamente, introduce un problema claro de multicolinealidad, que se puede obviar si el interés en el modelo es por su valor predictivo en lugar de explicativo.\n\nPregunta\nLa función I() es necesaria para el objetivo que se persigue. ¿Qué consecuencias acarrea no aplicar la función I() en la definición del modelo?\n\nObviamente se pueden incluir términos cúbicos, etc.: I(variable^3)…, varios a la vez, etc. Pero para un ajuste polinómico, conviene acudir a la función poly(), que, por defecto, ortogonaliza los predictores.\n\nrlm.Boston6 &lt;- lm(medv ~ poly(lstat, 6), data = Boston)\nsummary(rlm.Boston6)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 6), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.7317  -3.1571  -0.6941   2.0756  26.8994 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2317  97.252  &lt; 2e-16 ***\npoly(lstat, 6)1 -152.4595     5.2119 -29.252  &lt; 2e-16 ***\npoly(lstat, 6)2   64.2272     5.2119  12.323  &lt; 2e-16 ***\npoly(lstat, 6)3  -27.0511     5.2119  -5.190 3.06e-07 ***\npoly(lstat, 6)4   25.4517     5.2119   4.883 1.41e-06 ***\npoly(lstat, 6)5  -19.2524     5.2119  -3.694 0.000245 ***\npoly(lstat, 6)6    6.5088     5.2119   1.249 0.212313    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.212 on 499 degrees of freedom\nMultiple R-squared:  0.6827,    Adjusted R-squared:  0.6789 \nF-statistic: 178.9 on 6 and 499 DF,  p-value: &lt; 2.2e-16\n\n\nLa interpretación de esta salida sugiere que el modelo polinómico de orden 5 conduce al mejor ajuste del modelo con un polinomio de la variable lstat.\n\nTransformaciones logarítmicas: también se pueden aplicar otro tipo de transformaciones sobre los predictores, siempre que sean apropiadas y oportunas.\n\n\nsummary(lm(medv ~ log(rm), data = Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n1.5.4 Comparación de modelos mediante anova()\nCon la función anova() se pueden comparar modelos “jerárquicos”. En los casos expuestos anteriormente, se puede cuantificar hasta qué punto el ajuste cuadrático es superior al ajuste lineal, o que el ajuste de orden 6 no es superior al de orden 5.\n\nrlm.Boston1 &lt;- lm(medv ~ lstat, data = Boston)\nanova(rlm.Boston1, rlm.Boston2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nrlm.Boston5 &lt;- lm(medv ~ poly(lstat, 5), data = Boston)\nanova(rlm.Boston5, rlm.Boston6)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ poly(lstat, 5)\nModel 2: medv ~ poly(lstat, 6)\n  Res.Df   RSS Df Sum of Sq      F Pr(&gt;F)\n1    500 13597                           \n2    499 13555  1    42.364 1.5596 0.2123\n\n\nEn la primera tabla ANOVA se puede ver, en la etiquetada como línea 2, el estadístico F, que recoge la razón de variabilidad explicada por el Model 2 frente al Model 1. Su elevado valor, junto con un p-valor prácticamente nulo, llevan a considerar el modelo cuadrático muy superior al simple. Al lector observador no le debe sorprender este resultado a la vista del diagrama de dispersión entre medv y lstat, que presenta una clara no linealidad.\nPor su parte, la segunda tabla ANOVA muestra un resultado bien distinto, con un discreto valor del estadístico F que conduce a rechazar que el polinomio de orden 6 explique mejor la respuesta que el de orden 5. Además se puede comprobar que este p-valor coincide con el p-valor asociado al término de orden 6 estimado por la regresión.\n\nPregunta ¿Qué conclusiones obtiene al comparar los modelos rlm.Boston y rlm.BostonModif?\n\n\n\n1.5.5 Multicolinealidad: vif()\nEl cálculo del factor de inflación de la varianza (VIF) que ayuda a detectar la multicolinealidad (véase Sección 1.3.5) de cada parámetro se puede obtener con la función vif(), del paquete car:\n\nlibrary(car)\n\nCargando paquete requerido: carData\n\nvif(rlm.Boston)\n\n   lstat       rm      age      tax     chas \n2.710812 1.678750 1.795414 1.533240 1.030405 \n\n\nPara estos datos, la mayoría de los VIF son bajos o moderados.\n\nRegla de decisión práctica: No preocuparse de la multicolinealidad si vif &lt; 5, incluso vif &lt; 10.\n\n\nPregunta\n¿Cómo salen los VIF de la regresión múltiple con los datos airquality?\n\nVeamos un ejemplo claro de colinealidad:\n\nvif(rlm.Boston2)\n\n     lstat I(lstat^2) \n  12.93657   12.93657 \n\n\n\nPregunta\n¿Qué interpretación tiene que los VIF sean iguales?\n\nVamos a generar un caso con alta colinealidad:\n\nset.seed(pi)\nBoston$sum &lt;- Boston$lstat/2 + Boston$rm + rnorm(length(Boston$lstat))\nrlm.BostonColine &lt;- lm(medv ~ lstat + rm + tax + chas + sum, \n                       data = Boston)\nvif(rlm.BostonColine)\n\n    lstat        rm       tax      chas       sum \n13.189150  1.988231  1.430875  1.008564 10.209699 \n\n\n\nPregunta\n¿Qué interpretación tienen ahora los VIF?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#resumen",
    "href": "Cap1-LM.html#resumen",
    "title": "1  Modelos lineales",
    "section": "1.6 Resumen",
    "text": "1.6 Resumen\nEste capítulo se dedica a los modelos lineales de regresión que permiten explicar una variable cuantitativa (respuesta) en función de una o más variables explicativas. Se estudia tanto el caso más sencillo, la regresión lineal simple (una sola variable explicativa), como su generalización, la regresión múltiple (varias variables explicativas). El capítulo se completa con dos casos prácticos de modelización, utilizando R y los conjuntos de datos airquality y Boston.\nLos modelos de regresión se basan en una estructura algebraica, lineal en los parámetros, más un término de error aleatorio que recoge la variabilidad no explicada por la estructura.\nPara estimar los parámetros del modelo se utilizan estimadores de mínimos cuadrados, que buscan minimizar la suma de los cuadrados de los residuos y que, bajo normalidad, coinciden con los obtenidos por el método de máxima verosimilitud. En el documento se proporcionan las fórmulas y el procedimiento para su obtención.\nEl modelo se sustenta en unos supuestos que se han verificar. El análisis de residuos es clave para validar (o no) tales supuestos. Se utilizan gráficos como el de residuos frente a valores ajustados para homocedasticidad y linealidad y el Q-Q plot para normalidad. También se aplican contrastes como Shapiro-Wilk, Durbin-Watson, Bartlett y Levene. Algunas observaciones pueden tener gran impacto en el modelo. Mediante la medición de su leverage y su distancia de Cook se evalúa cuánto cambia el modelo al eliminar tal observación.\nGracias al conocimiento de las distribuciones en el muestreo de los estimadores, se pueden realizar contrastes de hipótesis y construir intervalos de confianza, que permiten interpretar el modelo.\nLa calidad del modelo se evalúa con el coeficiente de determinación \\(R^2\\), que mide la proporción de variabilidad explicada por el modelo. En regresión simple, coincide con el cuadrado del coeficiente de correlación. En regresión múltiple, se utiliza el \\(R^2\\) corregido que penaliza los modelos por el número de variables.\nCuando los supuestos no se cumplen, pueden aplicarse transformaciones a las variables. La familia Box-Cox ofrece opciones como logaritmo, raíz cuadrada o inversa. Estas transformaciones pueden mejorar la linealidad o la homocedasticidad, aunque afectan la interpretación de los parámetros.\nAl plantear regresiones lineales múltiples, se introducen retos como la selección de variables, la multicolinealidad y la interpretabilidad. Se usan herramientas como el VIF para detectar redundancia entre variables y técnicas como ANOVA para comparar modelos.\nEn los casos prácticos se muestra cómo definir modelos con lm(), interpretar los resultados obtenidos con summary(), obtener intervalos con confint() y realizar predicciones con predict(). También se exploran gráficos de diagnóstico y medidas de influencia.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#bibliografía",
    "href": "Cap1-LM.html#bibliografía",
    "title": "1  Modelos lineales",
    "section": "Bibliografía",
    "text": "Bibliografía\n\n\n\n\nCasero-Alonso, Víctor, y María Durbán. 2024. «Modelización lineal». En Fundamentos de Ciencia de Datos con R. McGraw Hill. https://cdr-book.github.io/cap-lm.html.\n\n\nFaraway, Julian J. 2004. Linear Models with R. Chapman &amp; Hall/CRC.\n\n\nFernández-Avilés, Gema, y José-María Montero. 2024. Fundamentos de Ciencia de Datos con R. McGraw Hill. https://cdr-book.github.io/index.html.\n\n\nPeña, Daniel. 2002. Regresión y diseño de experimentos. Alianza Editorial.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap3-GLM.html",
    "href": "Cap3-GLM.html",
    "title": "3  Modelos lineales generalizados",
    "section": "",
    "text": "En preparación",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales generalizados</span>"
    ]
  },
  {
    "objectID": "Cap4-Superv.html",
    "href": "Cap4-Superv.html",
    "title": "4  Análisis de supervivencia o fiabilidad",
    "section": "",
    "text": "En preparación",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Análisis de supervivencia o fiabilidad</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html",
    "href": "Cap5-Seleccion.html",
    "title": "5  Selección de variables",
    "section": "",
    "text": "5.1 Métodos de selección\n(version “2025-09-24”)\nCuando en un problema de regresión se dispone de un elevado número de variables explicativas, es posible que algunas de ellas no estén correlacionadas con la variable respuesta o que su relación no resulte estadísticamente significativa. También puede surgir el problema de la multicolinealidad, con una o varias variables explicativas que aportan información redundante respecto a otras explicativas (véase Sección 1.3.5). En estos casos, resulta deseable identificar un subconjunto de variables que expliquen adecuadamente la respuesta y/o que permitan generar mejores predicciones que el modelo completo. Estas circunstancias justifican la necesidad de realizar una selección apropiada de variables.\nLa tarea de generar y evaluar todos los modelos posibles que combinan subconjuntos de variables puede ser ingente: con \\(k\\) variables, el número de modelos posibles es \\(2^k\\). Por ello, en la práctica se recurre a seleccionar un conjunto reducido de modelos candidatos, entre los cuales se elige el “mejor”. Esta elección puede basarse en los enfoques explicativo o predictivo, como se ha mencionado. Aquí nos centramos en el enfoque predictivo, que es el más habitual en la práctica, pues es el utilizado en “Machine Learning”. Es por ello, que aquí no se presta atención al análisis de residuos para validar el modelo, dado que la validación viene dada por la calidad de las predicciones que realice el modelo obtenido.\nEsta selección de variables es especialmente útil cuando el número de variables es mayor que el número de observaciones (\\(k &gt; n\\)), situación común en algunos campos científicos, como la genómica, el tratamiento de imágenes médicas, etc. Es más, con más variables que observaciones es imposible estimar de forma única todos los parámetros del modelo completo.\nDos buenas referencias para este capítulo son Durbán (2024) y James et al. (2013). También se puede encontrar información en Peña (2002) y Faraway (2004).\nSe han desarrollado distintos procedimientos de selección, automática, de variables explicativas, que permitan obtener un modelo de regresión óptimo (desde alguna perspectiva).\nUno de los métodos más utilizados, por su simplicidad y eficacia, es la regresión paso a paso (stepwise regression). Este método permite construir el modelo de forma progresiva, evaluando el impacto de cada variable. Se basa en añadir, o eliminar, al modelo variables explicativas en función de su contribución estadística al modelo, evaluada mediante algún criterio (que se ve en el siguiente apartado). Este método surge como una alternativa computacionalmente menos costosa al método de selección del mejor subconjunto (Best Subset Selection). Este último método evalúa (exhaustivamente) todas los combinaciones posibles de modelos con un número fijo de variables (por ejemplo, todos los modelos con 4 variables, de las \\(k\\) variables disponibles, son \\({k \\choose 4}\\), o con 5 variables son \\({k \\choose 5}\\), etc., ).\nOtro conjunto de métodos, ampliamente utilizados, son los métodos de regularización, que penalizan la complejidad del modelo para evitar el sobreajuste. Dicho de otro modo, reducen el impacto de las variables menos relevantes. En este capítulo se estudian las regresiones Ridge y Lasso.\nLos métodos anteriores suelen combinarse con una técnica de evaluación muy extendida en Machine Learning: la validación cruzada. Esta consiste en obtener los modelos de los métodos anteriores, evaluando su rendimiento predictivo mediante procedimientos como la validación cruzada k-fold. Se basa en la minimización del error de predicción sobre un subconjunto de los datos reservado exclusivamente para validación, mientras que el resto se utiliza para el ajuste del modelo (enfoque de datos de entrenamiento y validación).\nOtra opción muy habitual es reducir la dimensión del problema, la dimensión de las variables explicativas. No es estrictamente un método de selección, sino que el objetivo es proyectar las \\(k\\) variables explicativas en un subespacio de dimensión más pequeña (mediante componentes principales: combinaciones lineales de las variables explicativas originales).\nPor último hay que mencionar los métodos basados en árboles. Pertenecen a este grupo las técnicas de Árboles de decisión, Random Forests, y Gradient Boosting, entre otros. Hay que comentar que estos no son métodos de regresión lineal, aunque ayudan a identificar variables relevantes, pues suelen proporcionar medidas de la importancia de las variables, lo que permite al usuario elegir qué variables seleccionar.\nDe los citados, en el capítulo se ven los 3 primeros grupos.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html#métodos-de-selección",
    "href": "Cap5-Seleccion.html#métodos-de-selección",
    "title": "5  Selección de variables",
    "section": "",
    "text": "La selección automática de variables mediante cualquiera de los procedimientos descritos debe realizarse siempre considerando el contexto y la lógica del problema. Es posible que el método seleccione variables cuya influencia en la respuesta carezca de sentido práctico. Esto puede deberse, entre otras razones, a la realización de numerosos contrastes de significación. A medida que aumenta el número de contrastes, también lo hace la probabilidad de obtener resultados estadísticamente significativos por azar. Por ejemplo, con un nivel de significación del 5%, se espera, en promedio, que 5 de cada 100 contrastes conduzcan al rechazo de la hipótesis nula, incluso si esta es cierta.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html#sec-Criterios",
    "href": "Cap5-Seleccion.html#sec-Criterios",
    "title": "5  Selección de variables",
    "section": "5.2 Criterios de selección",
    "text": "5.2 Criterios de selección\nPara elegir qué modelo es “mejor” de entre los candidatos, se acude a criterios de selección, que generalmente pueden estar basados, bien, en la bondad del ajuste del modelo a los datos (tienden a escoger modelos sobreajustados, con más parámetros de los necesarios); o bien, en la capacidad predictiva del modelo. En la práctica se utilizan los segundos, y principalmente:\n\nAIC (Akaike Information Criterion)\nBIC (Bayesian Information Criterion)\nC\\(_p\\) de Mallows\n\nEstos 3 criterios (y otros) se pueden expresarse en una forma general, que refleja un balance entre la bondad de ajuste del modelo (medida por la varianza residual) y su complejidad (basada en el número de parámetros). Concretamente:\n\\[\\text{Criterio} = n \\cdot \\log(\\hat{\\sigma}^2) + \\lambda(p)\\]\nDonde:\n\n\\(n\\): número de observaciones\n\n\\(\\hat{\\sigma}^2\\): estimación MV de la varianza residual del modelo completo\n\\(\\lambda(p)\\): penalización por complejidad del modelo, que depende de \\(p\\), el número de parámetros a estimar (incluyendo la constante), y que varía según el criterio:\n\nPara AIC: \\(\\lambda(p) = 2p\\)\nPara BIC: \\(\\lambda(p) = \\log(n)p\\)\nPara C\\(_p\\): se reestructura como penalización sobre \\(\\text{RSS}_p\\), la suma de cuadrados de los residuos del modelo con \\(p\\) parámetros: \\[C_p = \\frac{\\text{RSS}_p}{\\hat{\\sigma}^2} - n + 2p\\]\n\n\n\nUna deducción muy detallada del estadístico C\\(_p\\) de Mallows puede encontrarse en Peña (2002) (apartado 11.3.2 y Apéndice 11A). Donde se demuestra que minimizar el criterio C\\(_p\\) es equivalente a minizar el criterio AIC.\n\nCon la penalización basada en el número de parámetros se busca combatir modelos sobreajustados. El criterio BIC penaliza más la complejidad que AIC cuando el tamaño muestral es “grande” (basta \\(n&gt;7\\) dado que \\(\\log(8) &gt; 2\\)), por lo que BIC es un método más parsimonioso, tiende a proporcionar modelos más simples (más cuanto mayor sea \\(n\\)).\nEstos criterios son medidas relativas, se utilizan para comparaciones entre modelos, que podrían ser todos “malos”. El modelo preferido es el que tenga menor valor del criterio.\n\n¡Ojo! Los criterios indicados no permiten una comparación válida entre modelos cuya variable respuesta difiere.\n\nTambién existen otros criterios basados en medidas de bondad del ajuste del modelo, tales como el coeficiente de determinación corregido (R\\(^2\\)), su versión corregida (R\\(^2\\) corregido) o la varianza residual. Sin embargo, estos criterios únicamente permiten comparaciones, en igualdad de condiciones, entre modelos que poseen el mismo número de parámetros, por lo que su uso debe realizarse con cautela.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html#sec-Stepwise",
    "href": "Cap5-Seleccion.html#sec-Stepwise",
    "title": "5  Selección de variables",
    "section": "5.3 Selección paso a paso",
    "text": "5.3 Selección paso a paso\nEl método de regresión paso a paso tiene dos variantes:\n\nSelección hacia adelante (forward): se parte del modelo nulo —que solo incluye la constante, sin variables explicativas— y se van incorporando variables explicativas, una a una. En cada paso la que más “mejore” el modelo previo y hasta, o bien un modelo maximal propuesto (con aquellas variables que se consideren oportunas), o bien el modelo completo (que incluye todas las variables).\nEn formato algoritmo\nSiendo \\(k\\) el número máximo de variables a considerar, bien sea del modelo maximal o del completo.\nPaso 1. Sea \\(M_0\\) el modelo nulo.\nPaso 2. Para \\(i = 0, 1, \\dots, k - 1\\) , de los \\(k - i\\) modelos que se obtienen al añadir una variable explicativa adicional a las ya incluidas en \\(M_i\\), seleccionamos el mejor, y lo denotamos por \\(M_{i+1}\\).\nAquí “mejor” es el modelo que produce el mayor incremento de variabilidad explicada, \\(R^2\\), al añadir sólo una de las variables que todavía no han entrado al modelo. En las salidas de software se puede reconocer dicha variable como la de mayor valor del estadístico \\(t\\) (de las restantes variables).\nPaso 3. El proceso finaliza escogiendo uno de los \\(k\\) modelos (\\(M_0, \\ldots, M_k\\)) según uno de los criterios mencionados: AIC, BIC, etc.\nLa ventaja computacional es que, cuanto mayor es el número de variables disponibles, \\(k\\), el número de modelos a evaluar, \\(1 + \\frac{k(k + 1)}{2}\\), es menor que en la selección del mejor subconjunto, \\(2^k\\). Pero tiene una desventaja, no garantiza el mejor modelo posible de todos (por ejemplo, si existe multicolinealidad).\nSelección hacia atrás (backward): se parte del modelo maximal (o el modelo completo), y se van eliminando, una a una, la variable que menos pérdida supongan para el modelo. La de menor valor del estadístico \\(t\\) asociado (que no sea significativa). El proceso finaliza eligiendo de entre los \\(k\\) modelos, el de menor AIC, o BIC,… etc.\nComo la selección hacia adelante evalua \\(1 + \\frac{k(k + 1)}{2}\\) modelos (ventaja computacional frente a otros métodos), pero no garantiza encontrar el mejor modelo. Además, se añade la limitación (que puede ser importante) de necesitar un número de observaciones mayor que el número de variables \\(n&gt;k\\), de modo que el modelo completo (o, en su caso, el maximal considerado) pueda ajustarse. En contraste, la selección hacia adelante sí puede utilizarse incluso cuando \\(n &lt; p\\), lo que le otorga una ventaja competitiva, pues le convierte en el único método viable cuando \\(k\\) es muy grande.\n\nAlgunos paquetes de software incluyen también la selección bidireccional, combinación de ambos enfoques, permitiendo tanto la inclusión como la eliminación de variables en cada iteración, según su contribución al modelo. Obviamente es el procedimiento más flexible y suele ofrecer mejores resultados en la práctica.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html#selección-por-regularización",
    "href": "Cap5-Seleccion.html#selección-por-regularización",
    "title": "5  Selección de variables",
    "section": "5.4 Selección por regularización",
    "text": "5.4 Selección por regularización\nLos métodos de selección basados en regularización (denominados en inglés Shrinkage -contracción-) se basan en modificar el procedimiento de estimación de mínimos cuadrados de los parámetros añadiendo un término de penalización sobre la magnitud de los parámetros. Los parámetros estimados se reducen/contraen hacia cero (“regularizan”) en comparación con las estimaciones por mínimos cuadrados, provocando una disminución en la varianza de los parámetros estimados del modelo. Buscan mejorar la capacidad predictiva del modelo y controlar el sobreajuste de los mismos. Por el contrario, pueden no seleccionar las variables de forma explícita como sí hacen los métodos anteriores. Eso sí, en contextos de alta dimensión (\\(k &gt;n\\)) no se puede aplicar directamente mínimos cuadrados porque la matriz de diseño no tiene rango completo.\nEn este capítulo vamos a ver los dos más importantes: Regresión Ridge y Regresión Lasso.\n\n5.4.1 Regresión Ridge\nEn este método se añade el término de penalización, suma de los cuadrados de los parámetros (es decir aplica una especie de norma \\(\\ell_2\\), le falta la raíz cuadrada para ser la norma \\(\\ell_2\\)). Puede no llegar a realizar una selección efectiva de variables, es decir, no llegar a eliminar las variables menos relevantes, pero reduce su impacto (al reducir las estimaciones de los parámetros).\nSu objetivo es minimizar la siguiente función: \\[\\text{RSS} + \\lambda \\sum_{j=1}^{k} \\beta_j^2 = \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{k} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{k} \\beta_j^2\\]\ndonde\n\nRSS es la suma de cuadrados de los residuos,\n\\(\\lambda\\) es el parámetro de penalización, un parámetro ajustable/ elegible, que controla la intensidad de la penalización. Si \\(\\lambda = 0\\) no hay penalización y los estimadores son los de mínimos cuadrados.\n\\(\\beta_j\\) son los parámetros del modelo.\n\n\nA la vista de la fórmula, el procedimiento no es invariante a la escala de los predictores, a diferencia del estimador de mínimos cuadrados ordinarios. Por ello, es necesario estandarizar las variables para aplicar regresión Ridge.\n\n\n\n5.4.2 Regresión Lasso\nEl método Lasso (Least Absolute Shrinkage and Selection Operator) penaliza mediante la suma de los valores absolutos de los parámetros (es decir considera una norma \\(\\ell_1\\)): \\[\\text{RSS} + \\lambda \\sum_{j=1}^{k} |\\beta_j| \\] Tiene la propiedad de forzar algunos parámetros a ser exactamente cero, cuando el parámetro \\(\\lambda\\) es suficientemente grande, lo que implica una selección efectiva de variables, mejorando la predicción, y aumentando la interpretabilidad del modelo.\nElección de \\(\\lambda\\)\nLa eficacia de la selección mediante Ridge o Lasso depende de la elección adecuada del parámetro \\(\\lambda\\). En la práctica se utiliza validación cruzada (véase Sección 5.5) para determinar el valor óptimo que minimiza el error de predicción.\n\n\n5.4.3 Comparativa\nAunque ninguno de los dos métodos domina universalmente al otro, en determinados contextos se prefiere uno u otro. La regresión Lasso es preferible cuando se espera que únicamente un pequeño subconjunto de predictores sea relevante, mientras que la regresión Ridge es útil cuando todos los predictores (o la mayoría) contribuyen. Así, la regresión Ridge permite, “colateralmente”, ayudar con el problema de la multicolinealidad de variables explicativas, regularizando sus estimaciones.\nComo penalizaciones de tipo norma \\(\\ell_2\\) o norma \\(\\ell_1\\), se tiene una interpretación geométrica de las restricciones que se imponen. Para Ridge, la restricción es una región esférica, mientras que para Lasso es una región en forma de rombo, favoreciendo soluciones sparse (dispersas). En James et al. (2013) se puede encontrar la explicación detallada, y una figura ilustrativa muy elocuente:\n\n\nEl lector interesado puede buscar información sobre otros métodos de regularización: Elastic Net (que es una combinación de Ridge y Lasso, especialmente útil cuando se sabe que hay muchas variables correlacionadas), Group Lasso, Sparse Group Lasso,etc.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html#sec-CV",
    "href": "Cap5-Seleccion.html#sec-CV",
    "title": "5  Selección de variables",
    "section": "5.5 Validación cruzada",
    "text": "5.5 Validación cruzada\nEl procedimiento de selección de variables por validación cruzada (cross-validation) es una técnica de remuestreo ampliamente utilizada en “Machine Learning”, con un claro enfoque predictivo de los modelos, en lugar de explicativo (como en el Capítulo 1). Su objetivo principal es evaluar la capacidad de generalización de un modelo, es decir, cómo se comporta al predecir datos no utilizados durante el ajuste.\nLa idea básica consiste en particionar (aleatoriamente) el conjunto de datos en dos subconjuntos disjuntos: uno para el entrenamiento (train), donde se ajusta el modelo, y otro para la validación (test), donde se evalúa su rendimiento. Esta partición suele ser desbalanceada, reservando típicamente un 80% o 90% de los datos para entrenamiento, y el restante 20% o 10% para validación (evaluación de la capacidad predictiva del modelo). En la práctica, la tasa de error de entrenamiento, calculada a partir de las observaciones utilizadas en el ajuste, tiende a subestimar la tasa de error del conjunto de validación. Especialmente si el conjunto de entrenamiento tiene pocas observaciones, lo que puede llevar a invalidar la generalización del modelo obtenido por entrenamiento. Por otro lado, interesa que la tasa de error sea lo menor posible en dicho conjunto de validación. Pero recordemos que la partición es aleatoria, por lo que dicha tasa de error puede variar con otra partición. Por lo que el resultado debe basarse en un consenso claro tras los resultados de varias particiones.\nPues bien, el procedimiento de validación cruzada viene a aportar una solución para la búsqueda de ese consenso. La partición a realizar se hace de forma cruzada, dividiendo (aleatoriamente) el conjunto total de datos en \\(k\\) subconjuntos (en inglés se les denomina folds) con el mismo tamaño (o aproximadamente). En cada iteración, se selecciona uno de los subconjuntos como conjunto de validación y se utilizan los \\(k-1\\) restantes para entrenamiento. Este proceso se repite \\(k\\) veces, de modo que cada observación se utiliza una vez para validar y \\(k-1\\) veces para entrenar. Se dice por ello que el método es eficiente al utilizar todos los datos tanto para entrenamiento como para validación. Cada modelo ajustado (fijado el número de variables) se evalúa mediante una métrica de error, como el Error Cuadrático Medio (MSE: Mean Squared Error) para problemas de regresión, que mide la precisión de las predicciones. Finalmente, se calcula el promedio de los \\(k\\) MSEs (o la métrica elegida) para seleccionar el modelo que minimiza el error de predicción.\nElección del número de folds\nHabitualmente se utiliza \\(k = 5\\) (5-fold cross-validation) ó \\(k=10\\), pues ofrecen un buen equilibrio entre sesgo y varianza del modelo a generalizar. Otros valores de \\(k\\), tienden a producir estimaciones con alta varianza (cuando \\(k\\) es pequeño) o se vuelven computacionalmente costosos (cuanto mayor es \\(k\\)), aunque con menor sesgo.\nEstimación final\nUna vez identificado mediante validación cruzada el modelo óptimo, de entre los distintos modelos con distinto número de variables, se ajusta sobre el conjunto completo de datos disponibles. Esto permite obtener estimaciones más precisas de los parámetros, ya que el mejor modelo de \\(p\\) variables en el conjunto completo puede diferir de los modelos obtenidos en cada subconjunto de entrenamiento.\n\n5.5.1 Leave-One-Out CV\nLa estimación LOOCV (validación cruzada dejando uno fuera) es un caso particular de validación cruzada en el que, como su nombre indica, el número de folds coincide con el número total de observaciones. Esto lleva a un coste computacional considerable si, \\(n\\), el número de observaciones es grande, al tener que entrenar \\(n\\) modelos. Ahora bien, para modelos lineales estimados mediante mínimos cuadrados existe un fórmula que ahorra tiempo de cálculo del LOOCV, pues se puede obtener al ajustar un sólo modelo: \\[ \\text{LOOCV} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i}{1 - h_i} \\right)^2\\] donde:\n\n\\(\\hat{y}_i\\) es el valor estimado para la observación \\(i\\) en el modelo original, y\n\\(h_i\\) es el leverage que para el caso de regresión simple es (véase Sección 1.2.12): \\[h_i = \\dfrac{1}{n}+\\dfrac{(x_i − \\bar{x})^2}{\\sum_{j=1}^n(x_{j} − \\bar{x})^2}\\]\n\nComo se puede apreciar, la fórmula es la del MSE ordinario salvo la “normalización” de cada \\(i\\)-ésimo residuo considerado su leverage \\(h_i\\), que recoge lo que influye cada observación en el ajuste del modelo.\n\nRecordemos que el leverage varía entre \\(1/n\\) y \\(1\\), y así, las observaciones con alto leverage aumentan el valor del LOCCV.\n\nEl LOOCV no tiende a sobreestimar la tasa de error de validación y tiene, en general, menos sesgo, que el enfoque de un sólo conjunto de entrenamiento y validación. Además, por construcción, su resultado es siempre el mismo para el conjunto de datos considerado (no hay aleatoriedad).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html#caso-práctico-boston",
    "href": "Cap5-Seleccion.html#caso-práctico-boston",
    "title": "5  Selección de variables",
    "section": "5.6 Caso práctico: Boston",
    "text": "5.6 Caso práctico: Boston\nSobre el conjunto de datos Boston, ya trabajado en otros capítulos (véase Sección 1.5, para el análisis exploratorio), se van a aplicar los siguientes métodos de selección de variables:\n\nSelección stepwise (incluyendo selección del mejor subconjunto de variables)\nSelección mediante validación cruzada\nRegresión Ridge\nRegresión Lasso\n\nEl propósito es obtener un modelo de predicción para medv seleccionando las variables que influyan en ella, utilizando los distintos métodos.\n\n5.6.1 Selección stepwise\nComo se ha señalado en la Sección 5.3, los métodos de selección de variables stepwise proporcionan el “mejor” modelo, obtenido por adición (forward) o eliminación (backward) de las variables explicativas.\nEn R existen distintas funciones que permiten aplicar estos procedimientos:\n\npaquete base de R, función step().\npaquete MASS, función stepAIC().\npaquete leaps, función regsubsets().\npaquete StepReg, función stepwise().\n\nAlgunas funciones, como step() o stepAIC() utilizan el criterio de información de Akaike (AIC) para la selección del modelo. En otras funciones se puede elegir otro criterio. También algunas permiten la selección bidireccional. En los libros James et al. (2013) y Fernández-Avilés y Montero (2024) se puede encontrar ejemplos de utilización de la función regsubsets().\n\n5.6.1.1 step()\nCon esta función se obtiene la aplicación más simple y directa de este método:\n\nlibrary(ISLR2)\n# redefinimos la variable `chas` al ser dicotómica\nBoston$chas &lt;- factor(Boston$chas)\nrlm &lt;- lm(medv ~ . , data = Boston)\nstep.rlm &lt;- step(rlm)\n\nStart:  AIC=1599.85\nmedv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + \n    tax + ptratio + lstat\n\n          Df Sum of Sq   RSS    AIC\n- indus    1      1.08 11350 1597.9\n- age      1      1.69 11351 1597.9\n&lt;none&gt;                 11349 1599.8\n- chas     1    245.31 11595 1608.7\n- tax      1    256.28 11606 1609.2\n- zn       1    263.59 11613 1609.5\n- crim     1    311.49 11661 1611.6\n- rad      1    430.71 11780 1616.7\n- nox      1    546.10 11896 1621.6\n- ptratio  1   1157.70 12507 1647.0\n- dis      1   1258.52 12608 1651.1\n- rm       1   1744.36 13094 1670.2\n- lstat    1   2733.54 14083 1707.0\n\nStep:  AIC=1597.9\nmedv ~ crim + zn + chas + nox + rm + age + dis + rad + tax + \n    ptratio + lstat\n\n          Df Sum of Sq   RSS    AIC\n- age      1      1.69 11352 1596.0\n&lt;none&gt;                 11350 1597.9\n- chas     1    251.21 11602 1607.0\n- zn       1    262.99 11614 1607.5\n- tax      1    299.68 11650 1609.1\n- crim     1    313.07 11664 1609.7\n- rad      1    453.61 11804 1615.7\n- nox      1    574.23 11925 1620.9\n- ptratio  1   1168.01 12518 1645.5\n- dis      1   1333.19 12684 1652.1\n- rm       1   1750.50 13101 1668.5\n- lstat    1   2743.21 14094 1705.4\n\nStep:  AIC=1595.98\nmedv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + \n    lstat\n\n          Df Sum of Sq   RSS    AIC\n&lt;none&gt;                 11352 1596.0\n- chas     1    254.21 11606 1605.2\n- zn       1    261.75 11614 1605.5\n- tax      1    298.57 11651 1607.1\n- crim     1    313.27 11666 1607.8\n- rad      1    452.16 11804 1613.7\n- nox      1    601.74 11954 1620.1\n- ptratio  1   1168.51 12521 1643.5\n- dis      1   1496.35 12848 1656.6\n- rm       1   1848.38 13201 1670.3\n- lstat    1   3043.23 14395 1714.2\n\n\nComo se puede ver al consultar la ayuda de la función, por defecto muestra todos los pasos dados para la obtención del mejor modelo (trace=1) y realiza la selección en ambas direcciones (\"both\").\nLa comparativa de modelos hasta llegar al “mejor” se guarda en el value $anova y el modelo ajustado final, junto con su bondad de ajuste, etc. se puede obtener con la conocida función summary():\n\nstep.rlm$anova\n\n     Step Df Deviance Resid. Df Resid. Dev      AIC\n1         NA       NA       493   11349.42 1599.855\n2 - indus  1 1.081197       494   11350.50 1597.903\n3   - age  1 1.686474       495   11352.19 1595.978\n\nsummary(step.rlm)\n\n\nCall:\nlm(formula = medv ~ crim + zn + chas + nox + rm + dis + rad + \n    tax + ptratio + lstat, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1814  -2.7625  -0.6243   1.8448  26.3920 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.451747   4.903283   8.454 3.18e-16 ***\ncrim         -0.121665   0.032919  -3.696 0.000244 ***\nzn            0.046191   0.013673   3.378 0.000787 ***\nchas1         2.871873   0.862591   3.329 0.000935 ***\nnox         -18.262427   3.565247  -5.122 4.33e-07 ***\nrm            3.672957   0.409127   8.978  &lt; 2e-16 ***\ndis          -1.515951   0.187675  -8.078 5.08e-15 ***\nrad           0.283932   0.063945   4.440 1.11e-05 ***\ntax          -0.012292   0.003407  -3.608 0.000340 ***\nptratio      -0.930961   0.130423  -7.138 3.39e-12 ***\nlstat        -0.546509   0.047442 -11.519  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.789 on 495 degrees of freedom\nMultiple R-squared:  0.7342,    Adjusted R-squared:  0.7289 \nF-statistic: 136.8 on 10 and 495 DF,  p-value: &lt; 2.2e-16\n\n\n\nPregunta\n¿Obtiene el mismo “mejor” modelo final utilizando el metodo hacia atrás? ¿Y utilizando el método hacia adelante? ¿A qué conclusión llega con ello?\nNota: Para aplicar el método hacia adelante debe definir bien el scope.\n\n\nLa citada función MASS::stepAIC() es una implementación más sofisticada que step() de este método, que permite un mayor rango de objetos (como glm que se ve en el Capítulo 3). Más información/ejemplos en: https://fhernanb.github.io/libro_regresion/selec.html#función-stepaic\n\n\n\n5.6.1.2 StepReg::stepwise()\nEl paquete StepReg es considerado por muchos usuarios el más completo y flexible para selección stepwise, dado que se puede aplicar a distintos tipos de modelos (todos los que se ven en esta asignatura, lm, glm… y más). Ofrece una gran variedad de criterios de selección, más de los vistos en la Sección 5.2. Por último, implementa las direcciones habituales (\"forward\", \"backward\", \"bidirection\") y la selección del mejor subconjunto (\"subset\", al igual que hace la función leaps::regsubsets(), que permiten seleccionar el mejor modelo de 1, 2, 3, … variables).\n\nMás información en la ayuda de la función ?stepwise o, mucho mejor, en su vignette: https://cran.r-project.org/web/packages/StepReg/vignettes/StepReg.html que incluso contiene las fórmulas y referencias bibliográficas de los criterios seleccionables (AIC, BIC, etc.)\n\nAplicando esta función a nuestros caso práctico obtenemos:\nNota: ¡Ojo! Es posible que algunas de las funciones tarden un tiempo en completar su ejecución.\n\nlibrary(StepReg)\nBoston.Step &lt;- stepwise(formula = medv ~ .,\n                        data = Boston,\n                        type = \"linear\",\n                        strategy = c(\"forward\", \"backward\"),\n                        metric = c(\"AIC\", \"BIC\"))\n# Ejecutando Boston.Step se muestran los 4 ajustes obtenidos\n# Mostramos aquí sólo el ajuste \"extendido\" de uno de ellos\nsummary(Boston.Step$forward$BIC)\n\n\nCall:\nlm(formula = medv ~ 1 + lstat + rm + ptratio + dis + nox + chas + \n    zn + crim + rad + tax, data = data, weights = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1814  -2.7625  -0.6243   1.8448  26.3920 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.451747   4.903283   8.454 3.18e-16 ***\nlstat        -0.546509   0.047442 -11.519  &lt; 2e-16 ***\nrm            3.672957   0.409127   8.978  &lt; 2e-16 ***\nptratio      -0.930961   0.130423  -7.138 3.39e-12 ***\ndis          -1.515951   0.187675  -8.078 5.08e-15 ***\nnox         -18.262427   3.565247  -5.122 4.33e-07 ***\nchas1         2.871873   0.862591   3.329 0.000935 ***\nzn            0.046191   0.013673   3.378 0.000787 ***\ncrim         -0.121665   0.032919  -3.696 0.000244 ***\nrad           0.283932   0.063945   4.440 1.11e-05 ***\ntax          -0.012292   0.003407  -3.608 0.000340 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.789 on 495 degrees of freedom\nMultiple R-squared:  0.7342,    Adjusted R-squared:  0.7289 \nF-statistic: 136.8 on 10 and 495 DF,  p-value: &lt; 2.2e-16\n\nplot(Boston.Step, \n     strategy = \"forward\", \n     process = \"overview\") #Alternativa: process = \"details\" \n\n\n\n\n\n\n\n\nEn este ejemplo se obtiene el mismo modelo final (y por tanto las mismas estimaciones), aplicando tanto paso a paso hacia adelante, como hacia atrás, y tanto con el criterio de selección AIC como con el BIC. Obviamente, esto no ocurre siempre (véase el ejemplo de la sección Selección del mejor subconjunto del libro CDR).\nVeamos ahora un ejemplo de aplicación de \"subset\":\n\nBoston.Subset &lt;- stepwise(formula = medv ~ .,\n                          data = Boston,\n                          type = \"linear\",\n                          strategy = c(\"subset\"),\n                          metric = \"AIC\")\nBoston.Subset\n\n$subset\n$subset$AIC\n\nCall:\nlm(formula = medv ~ 1 + crim + zn + chas + nox + rm + dis + rad + \n    tax + ptratio + lstat, data = data, weights = NULL)\n\nCoefficients:\n(Intercept)         crim           zn        chas1          nox           rm  \n   41.45175     -0.12166      0.04619      2.87187    -18.26243      3.67296  \n        dis          rad          tax      ptratio        lstat  \n   -1.51595      0.28393     -0.01229     -0.93096     -0.54651  \n\n\nSe obtiene el mismo “mejor” modelo que el obtenido por stepwise. Ahora bien, internamente se han generado los mejores modelos para cada número fijo de “variables”. En la mencionada vignette de la función se puede encontrar el siguiente código que permite visualizarlo:\n\nplot_list &lt;- setNames(\n  lapply(c(\"subset\"),\n         function(i) {\n           setNames(lapply(c(\"details\", \"overview\"),\n                           function(j) {\n                             plot(Boston.Subset, \n                                  strategy = i, \n                                  process = j)\n                             }),\n                    c(\"details\", \"overview\")\n                    )}),\n  c(\"subset\"))\ncowplot::plot_grid(plotlist = plot_list$subset,\n                   ncol = 1,\n                   rel_heights = c(2, 1))\n\n\n\n\n\n\n\n\nAsí, el mejor modelo con 2 “variables” (más bien parámetros) es el que contiene el término independiente 1 y la variable lstat; el mejor de 3 “variables” incluye las 2 anteriores y rm, etc. Así, el mejor modelo con hasta 5 “variables” es (en formato R) 1 + lstat + rm + ptratio + dis.\n\nPregunta\n¿Cómo obtener ahora los parámetros del mejor modelo ajustado de \\(k\\) variables?\n\n\n\n\n5.6.2 Selección mediante validación cruzada\nCambiamos el orden respecto a la teoría, para aplicar \\(k\\)-fold CV para la selección de variables. De nuevo, en R existen distintos paquetes y funciones para obtenerla:\n\npaquete boot, función cv.glm().\npaquete caret, función train(), junto con trainControl().\npaquete mlr, función resample().\n…\n\nAquí utilizamos la función cv.glm() del paquete boot, que se usa también en el Capítulo 3.\nNota: Al haber un componente aleatorio en la partición de los subconjuntos de entrenamiento y validación, es importante utilizar la función set.seed() que establece la semilla de aleatorización que permite al lector la reproducibilidad de resultados.\nAdemás, es posible que algunas de las funciones tarden un tiempo en completar su ejecución.\n\n# Establecemos semilla de aleatorización\nset.seed(pi^2)\n# Ajustamos un modelo de regresión lineal completo\n# con la función glm() que se ve en detalle en el Capítulo 3\n# y que es necesaria para la función cv.glm\nmodelo.completo &lt;- glm(medv ~ ., \n                       family = gaussian,\n                       data = Boston)\nsummary(modelo.completo)\n\n\nCall:\nglm(formula = medv ~ ., family = gaussian, data = Boston)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.617270   4.936039   8.431 3.79e-16 ***\ncrim         -0.121389   0.033000  -3.678 0.000261 ***\nzn            0.046963   0.013879   3.384 0.000772 ***\nindus         0.013468   0.062145   0.217 0.828520    \nchas1         2.839993   0.870007   3.264 0.001173 ** \nnox         -18.758022   3.851355  -4.870 1.50e-06 ***\nrm            3.658119   0.420246   8.705  &lt; 2e-16 ***\nage           0.003611   0.013329   0.271 0.786595    \ndis          -1.490754   0.201623  -7.394 6.17e-13 ***\nrad           0.289405   0.066908   4.325 1.84e-05 ***\ntax          -0.012682   0.003801  -3.337 0.000912 ***\nptratio      -0.937533   0.132206  -7.091 4.63e-12 ***\nlstat        -0.552019   0.050659 -10.897  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 23.02113)\n\n    Null deviance: 42716  on 505  degrees of freedom\nResidual deviance: 11349  on 493  degrees of freedom\nAIC: 3037.8\n\nNumber of Fisher Scoring iterations: 2\n\n# procedimiento de validación cruzada\nlibrary(boot)\ncv.10fold &lt;- cv.glm(Boston, modelo.completo, K = 10)\n# Estimación del error de predicción\ncv.10fold$delta\n\n[1] 23.79801 23.72486\n\n\nComo “resumen” del proceso de \\(10\\)-fold validación cruzada obtenemos los valores delta, estimaciones del promedio de error de predicción, la primera es la estimación estándar y la segunda es una versión con corrección de sesgo. Estos 2 datos por sí solos no aportan información. Los comparamos con los de una validación cruzada de \\(2\\)-fold y con el MSE del modelo nulo (al que siempre se puede acudir, por no depender/contener variables explicativas).\n\nset.seed(pi^2)\n# 2-fold CV\ncv.2fold &lt;- cv.glm(Boston, modelo.completo, K = 2)\n# Estimación del error de predicción\ncv.2fold$delta\n\n[1] 27.05146 25.34112\n\n# MSE modelo nulo\nvar(Boston$medv)\n\n[1] 84.58672\n\n\nLos valores de delta para \\(2\\)-fold CV son peores que para \\(10\\)-fold CV, pero mucho mejores que el MSE del modelo nulo con los datos completos.\n\nPregunta\n¿Porqué un \\(2\\)-fold CV proporciona peores errores que un \\(10\\)-fold CV? Realice un \\(15\\)-fold CV y compruebe si mejora el MSE del \\(10\\)-fold CV. Pruebe con distintas semillas de aleatorización para comprobar la dependencia de los resultados respecto a la semilla proporcionada.\n\nProbemos a realizar otra \\(10\\)-fold CV ajustando otro modelo, para intentar comprobar si baja el error de predicción.\n\nset.seed(pi^2)\n# Ajustamos el modelo de regresión obtenido con stepwise()\n# de nuevo con la función glm()\nformula.step &lt;- Boston.Step$forward$AIC$call$formula\nmodelo.step &lt;- glm(formula.step, \n                   family = gaussian, \n                   data = Boston)\nsummary(modelo.step)\n\n\nCall:\nglm(formula = formula.step, family = gaussian, data = Boston)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.451747   4.903283   8.454 3.18e-16 ***\nlstat        -0.546509   0.047442 -11.519  &lt; 2e-16 ***\nrm            3.672957   0.409127   8.978  &lt; 2e-16 ***\nptratio      -0.930961   0.130423  -7.138 3.39e-12 ***\ndis          -1.515951   0.187675  -8.078 5.08e-15 ***\nnox         -18.262427   3.565247  -5.122 4.33e-07 ***\nchas1         2.871873   0.862591   3.329 0.000935 ***\nzn            0.046191   0.013673   3.378 0.000787 ***\ncrim         -0.121665   0.032919  -3.696 0.000244 ***\nrad           0.283932   0.063945   4.440 1.11e-05 ***\ntax          -0.012292   0.003407  -3.608 0.000340 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 22.93371)\n\n    Null deviance: 42716  on 505  degrees of freedom\nResidual deviance: 11352  on 495  degrees of freedom\nAIC: 3033.9\n\nNumber of Fisher Scoring iterations: 2\n\n# Validación cruzada\ncv.10fold &lt;- cv.glm(Boston, modelo.step, K = 10)\ncv.10fold$delta\n\n[1] 23.55353 23.49375\n\n\nComo se puede ver, el error de predicción baja ligeramente respecto al modelo completo.\n\nPregunta\n¿Empeora el error de predicción al eliminar del modelo la variable tax, que presenta una alta multicolinealidad? ¿Hay alguna otra variable con alta multicolinealidad? Si es así, calcule el error de predicción eliminándolas.\n\nEstimación LOOCV\nCon la función cv.glm() también se puede obtener la estimación LOOCV, aunque lamentablemente no tiene implementada la fórmula “abreviada” para LOOCV, por lo que el coste computacional puede ser excesivo.\n\nset.seed(pi)\n# LOOCV\ncv.LOOCV &lt;- cv.glm(Boston, modelo.completo)\ncv.LOOCV$delta\n\n[1] 24.15953 24.15777\n\n\nOtros ejemplos\nEn los libros James et al. (2013) y Fernández-Avilés y Montero (2024) se pueden encontrar ejemplos en los que se genera código para la realización “a mano” de particiones, para el cálculo del promedio de los errores de predicción, etc. utilizando la función regsubsets() (mencionada en el apartado anterior, Sección 5.6.1).\n\n\n5.6.3 Regresión Ridge\nPara ajustar un modelo de regresión Ridge (también para Lasso) utilizamos la función glmnet() del paquete glmnet.\nLos argumentos a proporcionar a la función glmnet() difieren de lo visto hasta ahora, en lugar de proporcionarle una fórmula de tipo y ~ x1 + x2, se le debe proporcionar el vector de respuestas y la matriz de variables explicativas. Además, por defecto, la función estandariza las variables para que estén en la misma escala (elemento clave en las regresiones Ridge y Lasso como se ha indicado). También glmnet() incluye el argumento alpha que determina el tipo de modelo a ajustar. Cuando se establece alpha = 0, se ajusta un modelo de regresión Ridge; en cambio, si se estable alpha = 1, se ajusta un modelo de regresión Lasso (y cualquier valor intermedio entre 0 y 1 equivale a una combinación lineal entre Ridge y Lasso, esto es, a aplicar una elastic net) Por último, el argumento lambda permite introducir los valores deseados del parámetro de penalización (por defecto toma 100 valores).\n\nObservación: una restricción de la función glmnet() es que sólo puede manejar entradas numéricas, por lo que las variables cualitativas deben convertirse en variables dummys, por ejemplo, mediante la función factor().\n\nComenzamos ajustando un modelo de regresión Ridge como primer paso.\n\nlibrary(glmnet)\n\nCargando paquete requerido: Matrix\n\n\nLoaded glmnet 4.1-10\n\nx &lt;- as.matrix(Boston[, -13])  # Explórese la función `model.matrix()`\ny &lt;- Boston$medv               \nmod.Ridge &lt;- glmnet(x, y, alpha = 0)\n\nLa mejor manera de mostrar la información recogida en el objeto mod.Ridge (y recomendada por los creadores del paquete) es utilizando las funciones plot(), print(), coef() y predict():\n\nplot(mod.Ridge, xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\nCada curva corresponde a una variable (etiquetadas en este caso), en función del logaritmo de \\(\\lambda\\), el parámetro de penalización. A mayor penalización, los parámetros estimados tienden a cero, siendo la variable 5 la que más tarda en tender hacia cero. A menor penalización, las estimaciones de los parámetros se asemejan a las de mínimos cuadrados (técnicamente cuando \\(\\lambda = 0\\)). En el eje horizontal superior se indican los grados de libertad del modelo, número de parámetros distintos de cero (12 en todos los casos).\nEjecutando print(mod.Ridge) se obtiene una salida con 100 valores, los que toma lambda por defecto (calculados en función del número de variables y observaciones, y de alpha, entre otros), aunque glmnet() se detendría antes de tiempo si % de deviance no cambia lo suficiente de una lambda a otro. Aquí sólo mostramos los primeros y los últimos.\n\n\n  Df %Dev Lambda\n1 12 0.00   6778\n2 12 0.76   6176\n3 12 0.83   5627\n4 12 0.91   5127\n5 12 1.00   4672\n6 12 1.10   4257\n\n\n    Df  %Dev Lambda\n95  12 72.30 1.0790\n96  12 72.42 0.9833\n97  12 72.53 0.8960\n98  12 72.63 0.8164\n99  12 72.72 0.7438\n100 12 72.80 0.6778\n\n\nEn esta salida se muestran los grados de libertad, el % de deviance explicada (véase Capítulo 3) y el valor de \\(\\lambda\\). Se aprecia que los grados de libertad siempre son 12, la regresión Ridge, como se ha comentado, no realiza selección efectiva de variables. El % de deviance y Lambda son inversamente proporcionales, conforme disminuye lambda, la penalización, aumenta el % de deviance explicada.\nLa función coef() permite obtener las estimaciones de los parámetros para todos los modelos de regresión Ridge ajustados (los 100!!) o para un valor concreto de \\(\\lambda\\) de los predeterminados (aunque hay que especificarlos como s). Aquí se opta por mostrar los del \\(\\lambda\\) más pequeño, 0.6778, y del más grande, 6778, de la salida de print(), para observar los distintos valores de las estimaciones de los parámetros.\n\ncoef(mod.Ridge, s = c(0.7, 6778))\n\n13 x 2 sparse Matrix of class \"dgCMatrix\"\n                 s=   0.7      s=6778.0\n(Intercept)  32.580969595  2.253281e+01\ncrim         -0.099950913 -4.193841e-37\nzn            0.032571142  1.435758e-37\nindus        -0.044924593 -6.550405e-37\nchas          3.040795956  6.410260e-36\nnox         -12.592691149 -3.425864e-35\nrm            3.902180496  9.194049e-36\nage          -0.001979554 -1.244068e-37\ndis          -1.117830946  1.102639e-36\nrad           0.137192219 -4.071671e-37\ntax          -0.006115460 -2.582636e-38\nptratio      -0.840383674 -2.178965e-36\nlstat        -0.492502012 -9.596458e-37\n\n\nPara comparación, obtengamos también sus normas \\(\\ell_2\\):\n\nsqrt(sum(coef(mod.Ridge, s = 0.7)^2))\n\n[1] 35.31004\n\nsqrt(sum(coef(mod.Ridge, s = 6778)^2))\n\n[1] 22.53281\n\n\nComo era esperable, un \\(\\lambda\\) mayor implica un menor valor de norma \\(\\ell_2\\).\n\nEl lector interesado puede explorar el funcionamiento de la función predict(), que permite obtener predicciones para cualquier valor de \\(\\lambda\\), y más detalles del paquete glmnet en https://glmnet.stanford.edu/articles/glmnet.html#linear-regression-family-gaussian-default\n\n\n5.6.3.1 Elección de \\(\\lambda\\) por CV\nElegir el mejor valor del parámetro de penalización, \\(\\lambda\\), a mano, de entre los 100 modelos obtenidos con la función glmnet(), puede ser tedioso. En la práctica se utiliza la función cv.glmnet() que permite seleccionar automáticamente el mejor valor de \\(\\lambda\\) por validación cruzada. De forma predeterminada, la función realiza una \\(10\\)-fold CV (ajustable con el argumento nfolds). Como todos los procedimientos de validación cruzada, para reproducibilidad de resultados, se debe establecer una semilla de aleatorización.\n\nset.seed(pi)\n# Ajuste modelo Ridge por CV\nmod.cv.Ridge &lt;- cv.glmnet(x, y,\n                          type.measure = \"mse\",\n                          alpha = 0)\n#Resumen de la CV\nprint(mod.cv.Ridge)\n\n\nCall:  cv.glmnet(x = x, y = y, type.measure = \"mse\", alpha = 0) \n\nMeasure: Mean-Squared Error \n\n    Lambda Index Measure    SE Nonzero\nmin  0.678   100   25.57 4.194      12\n1se  5.248    78   29.59 5.586      12\n\n\nEl resumen proporciona el valor mínimo (óptimo) de \\(\\lambda\\), y el mayor valor tal que su error se encuentra a 1 error estándar del mínimo, 1se. Utilizar el valor del 1se ayuda a reducir el sobreajuste, es decir ayuda a seleccionar menos variables, pero en regresión Ridge ya se sabe que no es efectivo (en el caso práctico de regresión Lasso sí que se ilustra la reducción).\nLa función plot() aplicada a un objeto de tipo cv.glmnet genera un gráfico con las medias del MSE para los k-folds (puntos rojos), para cada valor de \\(\\lambda\\) considerado, junto con las barras verticales de \\(\\pm\\) 1se (la amplitud será tanto menor cuanto mayor sea el número de folds). Además señala con líneas punteadas los 2 valores de \\(\\lambda\\) del resumen anterior, tanto el del mínimo, como el 1se.\n\n# Gráfico \nplot(mod.cv.Ridge)\n\n\n\n\n\n\n\n\nEl inconveniente es que los \\(\\lambda\\) que aparecen en el resumen no están en la escala logarítmica del gráfico, lo que suele confundir:\n\n# Mejor lambda por CV\nmod.cv.Ridge$lambda.min; log(mod.cv.Ridge$lambda.min)\n\n[1] 0.6777654\n\n\n[1] -0.3889541\n\nmod.cv.Ridge$lambda.1se; log(mod.cv.Ridge$lambda.1se)\n\n[1] 5.247691\n\n\n[1] 1.657788\n\n\n.\nPor último, obtenemos las estimaciones de los parámetros del modelo para el mejor \\(\\lambda\\):\n\n# Estimaciones del modelo con el mejor lambda\ncoef(mod.cv.Ridge, s = \"lambda.min\")\n\n13 x 1 sparse Matrix of class \"dgCMatrix\"\n               lambda.min\n(Intercept)  32.751149447\ncrim         -0.100336258\nzn            0.032827923\nindus        -0.044165780\nchas          3.039078301\nnox         -12.715946236\nrm            3.899711111\nage          -0.001878906\ndis          -1.126495302\nrad           0.139516293\ntax          -0.006197994\nptratio      -0.842522564\nlstat        -0.494019020\n\n\nDe nuevo, ninguna de las estimaciones de los parámetros es cero, ¡la regresión Ridge no realiza una selección efectiva de variables!\n\nPregunta\n¿Se obtiene menor MSE con el modelo obtenido por stepwise o con el obtenido por validación cruzada?\nAyuda:\n\n\npred.Ridge &lt;- predict(mod.cv.Ridge, s = \"lambda.min\", newx = x)\nmean((pred.Ridge - y)^2)\n\n\n\n\n5.6.4 Lasso\nLa idea de aplicar regresión Lasso es que puede producir un modelo más reducido (parsimonioso, sparse) y por lo tanto, más interpretable que la regresión Ridge.\nPara ajustar un modelo Lasso utilizamos, como anteriormente, la función glmnet(), cambiando el argumento a alpha = 1.\n\nmod.lasso &lt;- glmnet(x, y, alpha = 1)\nplot(mod.lasso, xvar = \"norm\", label = TRUE) \n\n\n\n\n\n\n\nplot(mod.lasso, xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\nEn este caso, se presentan dos gráficos que varían en los valores del eje X. En el primero se dibujan los coeficientes respecto de la norma \\(\\ell_1\\), mientras que en el segundo se dibujan respecto al menos logaritmo de \\(\\lambda\\). En ambos cada curva corresponde a una variable, y mirando el segundo, por comparación con el mostrado en la regresión Ridge, ahora no es la variable 5 la última que tiende a cero, sino la variable 12 la última que se hace efectivamente cero. Pues, como se puede observar, las curvas alcanzan el cero y se mantienen en él, mostrando como el modelo Lasso (basado en la norma \\(\\ell_1\\)) sí que realiza una selección efectiva de variables. Además, ahora en el eje horizontal superior sí que cambian los grados de libertad del modelo, siendo 0 el último valor, indicando que no queda ninguna variable explicativa (ni término independiente en el modelo). Y para el valor \\(-\\log(\\lambda)=0\\) se tienen 4 grados de libertad, por lo que para \\(\\lambda = e^1\\) sólo quedan 4 variables con parámetro no nulo en el modelo Lasso. Obtengamos los parámetros para este caso:\n\ncoef(mod.lasso, s = exp(1))\n\n13 x 1 sparse Matrix of class \"dgCMatrix\"\n            s=2.718282\n(Intercept) 12.9983574\ncrim         .        \nzn           .        \nindus        .        \nchas         .        \nnox          .        \nrm           2.6290850\nage          .        \ndis          .        \nrad          .        \ntax          .        \nptratio     -0.1056136\nlstat       -0.3982620\n\n\nDonde se ve qué cuatro variables quedan en este modelo Lasso, y sus correspondientes parámetros estimados.\nAl igual que en la regresión Ridge, con print() se obtiene la tabla asociada al gráfico anterior. En ella se pueden ver los grados de libertad y el % de deviance explicada para cada valor de \\(\\lambda\\). Aquí se muestran de nuevo los primeros valores y los últimos:\n\n\n  Df  %Dev Lambda\n1  0  0.00  6.778\n2  1  9.24  6.176\n3  2 17.38  5.627\n4  2 25.27  5.127\n5  2 31.82  4.672\n6  2 37.26  4.257\n\n\n   Df  %Dev   Lambda\n72 12 73.42 0.009170\n73 12 73.42 0.008356\n74 12 73.42 0.007614\n75 12 73.43 0.006937\n76 12 73.43 0.006321\n77 12 73.43 0.005759\n\n\n\nRecuérdese que los valores de \\(\\lambda\\) los calcula la función glmnet() a partir del número de observaciones y variables, y de alpha, entre otros. Y que, en este caso, no llega a mostrar los 100 valores de \\(\\lambda\\) al no haber ganancia del % de devianza de un \\(\\lambda\\) a otro (técnicamente si el cambio fraccional en la devianza es inferior a \\(10^{-5}\\) o se alcanza el 99.9% de devianza explicada).\n\nComo en la regresión Ridge, obtenemos los parámetros estimados para los dos casos extremos y calculamos su norma \\(\\ell_1\\):\n\ncoef(mod.lasso, s = c(0.005759, 6.778))\n\n13 x 2 sparse Matrix of class \"dgCMatrix\"\n               s=0.005759 s=6.778000\n(Intercept)  41.068890376   22.53281\ncrim         -0.119348086    .      \nzn            0.045699188    .      \nindus         0.004560661    .      \nchas          2.850798996    .      \nnox         -18.305459374    .      \nrm            3.676675041    .      \nage           0.002574195    .      \ndis          -1.480645547    .      \nrad           0.275267178    .      \ntax          -0.011962505    .      \nptratio      -0.930411914    .      \nlstat        -0.549746736    .      \n\nsqrt(sum(abs(coef(mod.lasso, s = 0.005759))))\n\n[1] 8.325986\n\nsqrt(sum(abs(coef(mod.lasso, s = 6.778))))\n\n[1] 4.746873\n\n\n\n5.6.4.1 Elección de \\(\\lambda\\) por CV\nProcedemos a obtener por CV el mejor valor para \\(\\lambda\\) para el modelo Lasso. Teniendo la precaución de cambiar type.measure para indicar la métrica MAE, la apropiada para la norma \\(\\ell_1\\)\n\nset.seed(pi)\n# Ajuste modelo Ridge por CV\nmod.cv.lasso &lt;- cv.glmnet(x, y,\n                          type.measure = \"mae\",\n                          alpha = 1)\n# Mejor lambda\nmod.cv.lasso$lambda.min; log(mod.cv.lasso$lambda.min)\n\n[1] 0.07792655\n\n\n[1] -2.551989\n\n# Gráfico \nplot(mod.cv.lasso)\n\n\n\n\n\n\n\n# Estimaciones del modelo con el mejor lambda\ncoef(mod.cv.lasso, s = c(mod.cv.lasso$lambda.min, \n                         mod.cv.lasso$lambda.1se))\n\n13 x 2 sparse Matrix of class \"dgCMatrix\"\n             s=0.07792655  s=0.50091754\n(Intercept)  35.817147782 17.2337940423\ncrim         -0.093523880 -0.0251874063\nzn            0.034046511  .           \nindus        -0.002138392  .           \nchas          2.764345929  1.6396306845\nnox         -15.215905652 -0.0015426372\nrm            3.856744686  4.1455562003\nage           .            .           \ndis          -1.249210756 -0.0704054097\nrad           0.157219959  .           \ntax          -0.006877068 -0.0005616005\nptratio      -0.886616520 -0.7338252586\nlstat        -0.544438423 -0.5323802145\n\n\nAhora, para el mejor \\(\\lambda\\) obtenido por CV sí se obtiene una estimación de parámetro con valor \\(0\\), el asociado a la variable age (que ya habíamos visto que era poco significativa). En este caso, la selección de variables es mínima (ya conocíamos que la mayoría de variables sí que influye significativamente en la respuesta). Por ello se incluye también la estimación de parámetros para \\(\\lambda=\\)1se, que realiza una selección más efectiva de variables, concretamente, elimina 4 de ellas.\nAdemás, cambian las estimaciones respecto a los correspondientes modelos lineales. Por ejemplo:\n\nsummary(lm(medv ~ . - age, \n           data = Boston))\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1851  -2.7330  -0.6116   1.8555  26.3838 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.525128   4.919684   8.441 3.52e-16 ***\ncrim         -0.121426   0.032969  -3.683 0.000256 ***\nzn            0.046512   0.013766   3.379 0.000785 ***\nindus         0.013451   0.062086   0.217 0.828577    \nchas1         2.852773   0.867912   3.287 0.001085 ** \nnox         -18.485070   3.713714  -4.978 8.91e-07 ***\nrm            3.681070   0.411230   8.951  &lt; 2e-16 ***\ndis          -1.506777   0.192570  -7.825 3.12e-14 ***\nrad           0.287940   0.066627   4.322 1.87e-05 ***\ntax          -0.012653   0.003796  -3.333 0.000923 ***\nptratio      -0.934649   0.131653  -7.099 4.39e-12 ***\nlstat        -0.547409   0.047669 -11.483  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.794 on 494 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7284 \nF-statistic: 124.1 on 11 and 494 DF,  p-value: &lt; 2.2e-16\n\n\n\nPregunta\n¿Qué modelo es mejor para predecir: regresión Ridge o Lasso?\n\n\n\n\n5.6.5 Epílogo\nEn el material asociado al libro James et al. (2013) se pueden encontrar ejemplos de estas técnicas de selección de variables utilizando distintas funciones de R y el conjunto de datos Hitters (véase https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch6-varselect-lab.html). También en Durbán (2024). En Faraway (2004) hay un ejemplo de métodos stepwise utilizando R basado en el conjunto de datos statedata, y otro de regresión Ridge, basado en el conjunto de datos meatspec y la función lm.ridge() del paquete MASS.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html#resumen",
    "href": "Cap5-Seleccion.html#resumen",
    "title": "5  Selección de variables",
    "section": "5.7 Resumen",
    "text": "5.7 Resumen\nEl presente capítulo aborda la problemática de la selección de un subconjunto óptimo de variables explicativas en modelos de regresión, especialmente en contextos de alta dimensionalidad, con numerosas variables (incluso cuando se tienen más variables que observaciones, \\(k &gt; n\\), como ocurre en áreas como la genómica o el análisis de imágenes médicas). Esta situación plantea desafíos como la multicolinealidad, la irrelevancia estadística de algunas variables y la imposibilidad de estimar de forma única todos los parámetros del modelo completo.\nSe presentan distintos métodos de selección automática de variables, con énfasis en el enfoque predictivo, propio del aprendizaje automático: (a) selección paso a paso (stepwise) incluyendo la selección del mejor subconjunto; (b) métodos de regularización, centrándose en las regresiones Ridge y Lasso y (c) validación cruzada k-fold, incluida LOOCV. Se advierte sobre los riesgos de interpretar resultados estadísticamente significativos sin considerar el contexto del problema (el sentido práctico de las variables), especialmente cuando se realizan múltiples contrastes.\nLos métodos stepwise incluyen las variantes hacia adelante, hacia atrás y bidireccional. La selección de variables se basa en criterios/métricas como AIC, BIC o C\\(_p\\) de Mallows, que equilibran la bondad de ajuste y la complejidad/parsimonia del modelo. Por su parte, los métodos de regularización, se basan en penalizar la magnitud de los parámetros para evitar el sobreajuste, siendo útiles para reducir la multicolinealidad y/o para eliminar variables irrelevantes. Por último, la validación cruzada, basándose en el remuestreo, selecciona el modelo óptimo evaluando la capacidad predictiva de los distintos modelos.\nEl capítulo incluye un caso práctico basado en el conjunto de datos Boston, aplicando distintas funciones de R que tienen implementados los métodos de selección indicados arriba, y comparando sus resultados en términos de error de predicción.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html#bibliografía",
    "href": "Cap5-Seleccion.html#bibliografía",
    "title": "5  Selección de variables",
    "section": "Bibliografía",
    "text": "Bibliografía\n\n\n\n\nDurbán, María. 2024. «Modelos sparse y métodos penalizados de regresión». En Fundamentos de Ciencia de Datos con R. McGraw Hill. https://cdr-book.github.io/cap-lm.html.\n\n\nFaraway, Julian J. 2004. Linear Models with R. Chapman &amp; Hall/CRC.\n\n\nFernández-Avilés, Gema, y José-María Montero. 2024. Fundamentos de Ciencia de Datos con R. McGraw Hill. https://cdr-book.github.io/index.html.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, y Robert Tibshirani. 2013. An introduction to statistical learning: with applications in R. 2nd ed. Vol. 103. Springer. https://www.statlearning.com/.\n\n\nPeña, Daniel. 2002. Regresión y diseño de experimentos. Alianza Editorial.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  }
]