[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estadística II: modelos",
    "section": "",
    "text": "Prefacio\nEste “Quarto Book” contiene el material preparado por el profesor para la asignatura Estadística II impartida en el tercer curso del Grado en Matemáticas de la Universidad de Castilla-La Mancha. Se trata de la tercera asignatura de “Estadística” en el plan de estudios, precedida por Elementos de Probabilidad y Estadística y Estadística I, por lo que se asume que el estudiante posee conocimientos previos en probabilidad, estadística y manejo del software R.\nEl objetivo principal de esta asignatura es introducir al estudiante en el manejo de distintos tipos de modelos estadísticos. En concreto, se abordarán:\nCada uno de estos modelos podría constituir una asignatura completa. Por ello, para ajustarlos al formato cuatrimestral, se ha optado por tratar cada tema con distinto nivel de profundidad, buscando el difícil balance entre el rigor teórico y la aplicación práctica de estos modelos mediante el software estadístico R.\nEl contenido se basa en la experiencia docente del profesor y en diversas referencias bibliográficas, entre las que destacan:\nAunque este material podría haber incluido otros modelos (como regresión ordinal, modelos GAM,…), técnicas (árboles de decisión, bootstrap,…) o enfoques (machine learning, diseño óptimo de experimentos,…), su incorporación excedería el alcance de una asignatura cuatrimestral. No obstante, muchos de estos contenidos están presentes en la bibliografía citada, a la que el estudiante/lector puede acudir para ampliar conocimientos.\nEste material no está exento de errores o erratas.\nSe agradece cualquier comunicación al respecto para su corrección.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#software-r",
    "href": "index.html#software-r",
    "title": "Estadística II: modelos",
    "section": "Software: R",
    "text": "Software: R\nLa parte práctica de la asignatura se desarrollará utilizando el lenguaje de programación R (https://www.r-project.org/), en combinación con la interfaz RStudio.\n\n\n\n\n\n\nSe recomienda disponer de las versiones más recientes de ambos programas:\n\nDescarga de R: https://cloud.r-project.org/\n\nDescarga de RStudio: https://posit.co/downloads/\n\n\n\n\nManuales básicos\n\nR Para principiantes, E. Paradis.\n\nIntroducción a R\n\nMás documentación en: https://cran.r-project.org/other-docs.html\n\nBlog recomendado\n\nR Bloggers\n\nAyuda\nVéase la página de inicio de la ayuda en RStudio.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#agradecimientos",
    "href": "index.html#agradecimientos",
    "title": "Estadística II: modelos",
    "section": "Agradecimientos",
    "text": "Agradecimientos\nAunque el contenido es fruto de la experiencia docente del profesor, deseo expresar mi agradecimiento a mi compañero y amigo, Dr. D. Licesio J. Rodríguez-Aragón (UCLM), por compartir conmigo sus valiosos materiales y comentarios, fundamentales en la elaboración de este libro.\nTambién agradezco a los compañeros del Área de Estadística del Departamento de Matemáticas de la UCLM, en especial al Dr. D. Mariano Amo Salas, por sus aportaciones, especialmente en el apartado de análisis de supervivencia; al Dr. D. Sergio Pozuelo Campos y a la Dra. Da. Irene García Camacha por su apoyo y sugerencias en la docencia del Grado en Matemáticas.\nTambién debo mencionar al Dr. D. Juan Manuel Rodríguez Díaz (USAL), compañero de grupo de investigación, por sus materiales sobre modelos lineales y diseño de experimentos. Finalmente, agradezco a los autores de las referencias biblográficas que se citan (destaco a Román Salmerón Gómez (rnoremlas?)), así como a los desarrolladores de los paquetes de R utilizados, por su trabajo y dedicación.\n\n\n\n\nFaraway, Julian J. 2004. Linear Models with R. Chapman &amp; Hall/CRC.\n\n\nFernández-Avilés, Gema, y José-María Montero. 2024. Fundamentos de Ciencia de Datos con R. McGraw Hill. https://cdr-book.github.io/index.html.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, y Robert Tibshirani. 2013. An introduction to statistical learning: with applications in R. Second. Vol. 103. Springer. https://www.statlearning.com/.\n\n\nJr., Frank E. Harrell. 2015. Regression Modeling Strategies: With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis. 2nd ed. Springer Series en Statistics. Springer. https://doi.org/10.1007/978-3-319-19425-7.\n\n\nPeña, Daniel. 2002. Regresión y diseño de experimentos. Alianza Editorial.\n\n\nRivas López, María Jesús, y Jesús López Fidalgo. 2000. Análisis de supervivencia. Vol. 10. Cuadernos de estadística. Editorial La Muralla.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "Cap1-LM.html",
    "href": "Cap1-LM.html",
    "title": "1  Modelos lineales",
    "section": "",
    "text": "1.1 Modelo estadístico de regresión\nEn este tema se estudiarán los denominados modelos lineales. El caso paradigmático es la regresión lineal simple, caso particular del modelo de regresión lineal múltiple. Los modelos de diseño de experimentos (que estudiaremos en el Capítulo 2) también son modelos lineales. En ambos modelos, la denominada variable respuesta debe ser cuantitativa/numérica continua, a diferencia del modelo lineal generalizado (que estudiaremos en el Capítulo 3).\nUna buena referencia para este tema es el libro “Regresión y diseño de experimentos”, Peña (2002), concretamente la segunda parte, capítulos 5 a 10. Otro libro de referencia con un buen contenido matemático y práctico es “Linear Models with R”, Faraway (2004), capítulos 1 a 7. Otra referencia, con un enfoque más aplicado, es el capítulo 15 “Modelización lineal”, Casero-Alonso y Durbán (2024), https://cdr-book.github.io/cap-lm.html del libro “Fundamentos de Ciencia de Datos con R”, Fernández-Avilés y Montero (2024), que denominaremos en el texto como CDR.\nParafraseando a G.E.P. Box …Todos los modelos son falsos. Pero algunos son útiles. Es imposible describir la realidad de forma exacta mediante un modelo, pero puede ser útil utilizar un modelo aproximado, basado en los “datos”, que permita entender y explicar el fenómeno o experimento de interés. En matemáticas un modelo es una relación matemática, no necesariamente algebraica, que permite entender el fenómeno. En estadística, a los modelos matemáticos se les añade un término de error, para incluir lo que la parte estructual (algebraica) no es capaz de explicar Este término de error se desea aleatorio, estocástico, y tiene el papel de “cajón de sastre”.\nEl modelo lineal de regresión lineal simple permite modelizar (de forma aproximada) el comportamiento de una variable cuantitativa, denominada respuesta o dependiente, denotada por \\(Y\\), mediante una función lineal de otra variable cuantitativa, denominada explicativa o predictora, \\(X\\), que se supone está correlacionada con ella (correlación no implica causalidad). La forma habitual de expresar el modelo es: \\[ Y = \\beta_0 + \\beta_1 X + \\epsilon,\\]\ndonde \\(Y = (y_1, y_2, \\ldots, y_n)^\\top\\) es el vector de las \\(n\\) observaciones de la variable respuesta, \\(X = (x_1, x_2, \\ldots, x_n)^\\top\\) es el vector de las \\(n\\) observaciones de la variable explicativa, \\(\\beta_i \\ (i=0 \\ ó \\ 1)\\) son los coeficientes o parámetros del modelo, y \\(\\epsilon=(\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n)\\) es el vector de errores aleatorio que convierte el modelo determinista (\\(\\beta_0 + \\beta_1 X\\)) en modelo estocástico. Los supuestos más habituales sobre el error son:\nLa generalización a varias variables explicativas es inmediata: \\[ Y = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_k X_k + \\epsilon.\\] A este modelo se le denomina modelo lineal de regresión lineal múltiple.\nUtilizando notación matricial: \\[Y = X \\beta + \\epsilon ;\\]\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1k} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2k} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{nk}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\vdots \\\\\n\\epsilon_n \\\\\n\\end{bmatrix}.\\] donde \\(Y\\) vuelve a ser el vector de \\(n\\) respuestas, \\(X\\) es ahora la matriz \\(n\\times(k+1)\\) de variables explicativas, que contiene una columna de unos para incluir en el modelo el parámetro \\(\\beta_0\\) que no depende de las variables explicativas, \\(\\beta\\) el vector de \\(k+1\\) parámetros del modelo y \\(\\epsilon\\) vuelve a ser el vector de los términos de error aleatorios, \\(\\epsilon \\sim N(0, \\sigma^2I)\\).\nPregunta\nEn el modelo de regresión ¿tienen que ser todas las variables cuantitativas?\nRespuesta corta: No.\nDependiendo del tipo de variables (\\(X\\) e \\(Y\\)) se tendrán distintos modelos…\n¿Si hay más de una variable \\(Y\\)? Se habla de regresión múltiple multivariante, que se podrá ver en otra asignatura del grado.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#modelo-estadístico-de-regresión",
    "href": "Cap1-LM.html#modelo-estadístico-de-regresión",
    "title": "1  Modelos lineales",
    "section": "",
    "text": "tener media cero,\nvarianza constante y\nseguir una distribución normal.\n\n\n\nEn este contexto, cuando se habla de modelo lineal hay que distinguir entre modelo lineal en las variables y modelo lineal en los parámetros. Así:\n- \\(Y=\\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\) es un modelo lineal en las variables y en los parámetros.\n- \\(Y=\\beta_1 X_1 + \\beta_2 X_1^2  + \\epsilon\\) es un modelo lineal en los parámetros, pero no en la variable.\n- \\(Y = \\beta_1X_1^{\\beta_2}  + \\epsilon\\) es un modelo no lineal tanto en los parámetros como en la variable.\n\n\n\n\nLos modelos que se puedan expresar en forma matricial son lineales.\n\n\n\n\n1.1.1 Objetivos de la regresión\n\nDescribir la estructura general entre la(s) variable(s) explicativa(s) y la respuesta, estimando y evaluando su efecto.\nEste proceso suele ser iterativo, hasta seleccionar la(s) variable(s) del mejor modelo posible.\nPredecir observaciones futuras.\n\nAmbos objetivos pueden ser muy distintos!!! Bajo el prisma del “machine learning” suele relajarse la descripción estructural y evaluación de los efectos dando toda la importancia a la mejor predicción posible.\n\n\n1.1.2 Supuestos del modelo de regresión\nLos supuestos en los que se basa el modelo de regresión expuesto son:\n\nNormalidad: no solo la variable \\(Y\\) debe seguir una distribución normal, sino los valores de \\(Y\\) para cada valor de \\(X_i\\) (distribución condicionada) también deben seguir una distribución normal.\nLinealidad: relación lineal entre cada una de las variables explicativas y la respuesta.\nHomocedasticidad (homogeneidad de varianzas): la varianza de la variable \\(Y\\) para cada valor de \\(X_i\\) debe ser homogénea.\nIndependencia: cada observación de la variable \\(Y\\) debe ser independiente de las demás.\n\nLos supuestos anteriores se basan en las hipótesis de los errores:\n\nesperanza nula: \\(E[\\epsilon]=0\\),\nvarianza constante: \\(Var[\\epsilon]=\\sigma^2I\\),\ndistribución normal \\(\\epsilon \\sim N(0, \\sigma^2 I)\\)\nindependencia entre errores: \\(E[\\epsilon_i\\epsilon_j]=0\\)\n\nSe puede encontrar más información sobre las también denominadas hipótesis básicas en Peña (2002) (capítulo 5, apartados 5.2.1 Hipótesis básicas y 5.2.2 Comentarios a las hipótesis) y en Faraway (2004)…\nResiduos\nLos residuos, \\(u_i\\), son los errores observados para los datos y el modelo escogido (realizaciones de la variable aleatoria error \\(\\epsilon\\)). Recogen toda la información que la estructura del modelo no ha sido capaz de asimilar. Es muy importante el estudio de los residuos para comprobar las hipótesis y supuestos anteriores y dar con ello validez al uso del modelo, como veremos en la Sección 1.2.12 a nivel teórico y en los casos prácticos (Sección 1.5 y Sección 1.6.2).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#lm",
    "href": "Cap1-LM.html#lm",
    "title": "1  Modelos lineales",
    "section": "1.2 Modelo lineal de regresión simple",
    "text": "1.2 Modelo lineal de regresión simple\nEs habitual comenzar el estudio de los modelos lineales con el caso más sencillo de regresión lineal simple para profundizar en los detalles de cada uno de los pasos del proceso de modelización, estimación, validación y predicción. Posteriormente se señalarán los aspectos que cambian al pasar a la regresión lineal múltiple, que es el caso más habitual en la práctica.\nPartiendo de los datos muestrales recogidos sobre el fenómeno de interés. ¿Cómo obtener el modelo de regresión lineal simple?\nREVISAR EL ORDEN\n\nEstructura: se deben identificar la variable respuesta (la que se pretende explicar) y la variable explicativa. Esta identificación define una forma estructural, que en este caso sencillo podría tener 2 formas:\n\n\n\\(E(Y)=\\beta_0 + \\beta_1 X\\), la habitual (incluso diría “por defecto”) o\n\\(E(Y)= \\beta_1 X\\) (proporcionalidad directa, sin término independiente).\n\n\nEstimación: con los valores muestrales recogidos \\((x_i, y_i)\\) se obtendrán estimaciones \\(\\hat{\\beta}_i\\) de los parámetros \\(\\beta_i\\) del modelo preestablecido.\nValidación: mediante el análisis de residuos se comprobarán las hipótesis que validan el uso del modelo.\nInterpretación/Inferencia: validado el modelo se interpretarán los coeficientes, valorando previamente su significación y su utilidad práctica. También se dará cuenta de la bondad del modelo para explicar la variable respuesta, mediante algún indicador.\nLlegados a este punto puede verse conveniente cambiar el modelo (en este caso considerar otra variable explicativa, o aplicar alguna transformación) para intentar obtener un modelo que explique mejor la respuesta, con mayor bondad, por lo que se volvería al paso 1.\nPredicción: se obtendrán las predicciones necesarias u oportunas a partir del modelo considerado, teniendo cuidado con la extrapolación (y en algunos casos, también con la interpolación).\n\n\n1.2.1 Estimación de los parámetros del modelo\nEn el caso más habitual de regresión simple: \\[Y = \\beta_0 + \\beta_1X + \\epsilon,\\] se estimarán los dos parámetros del modelo: \\(\\beta_0\\), la ordenada en el origen y \\(\\beta_1\\), la pendiente de la recta. Intuitivamente, se buscan las estimaciones \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que “expliquen” de la mejor manera posible la relación entre las dos variables, aquellas que generen las mejores predicciones posibles: \\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X\\]\n\n\n1.2.2 Método de mínimos cuadrados\nEl método por excelencia para obtener tales estimaciones es el método de mínimos cuadrados (1805 Gauss y Legendre). Consiste en minimizar los cuadrados de los denominados residuos, diferencia entre los valores observados y la predicción: \\(u_i = y_i - \\hat{y}_i = y_i - \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\).\nEs decir, \\[\\min_{\\hat{\\beta}_0, \\hat{\\beta}_1} \\sum u_i^2 =\n\\min_{\\hat{\\beta}_0, \\hat{\\beta}_1} \\sum(y_i-\\hat{y_i})^2 =\n\\min_{\\hat{\\beta}_0, \\hat{\\beta}_1} \\sum(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2.\\]\nDerivando, e igualando a cero se tiene un sistema lineal de 2 ecuaciones con 2 incógnitas, denominado ecuaciones normales: \\[\\left.\\begin{align*}\n\\dfrac{\\partial \\sum u_i^2}{\\partial \\beta_0} &= 0 \\Rightarrow \\sum_{i} u_i = 0 \\\\\n\\dfrac{\\partial \\sum u_i^2}{\\partial \\beta_1} &= 0 \\Rightarrow \\sum_{i} u_i x_i = 0\n\\end{align*}\n\\right\\rbrace\\]\ncuya solución es:\n\\[\\begin{align*}\n\\hat{\\beta}_1^{MC} &= \\rho \\frac{S_Y}{S_X}, \\\\\n\\hat{\\beta}_0^{MC} &= \\bar{y} - \\hat{\\beta}_1^{MC} \\bar{x},\n\\end{align*} \\] donde \\(\\rho\\) es el coeficiente de correlación lineal de Pearson y \\(S_Y\\) y \\(S_X\\) son las desviaciones típicas muestrales de las variables \\(Y\\) y \\(X\\).\nCorrelación\nEn la regresión lineal simple se espera que ambas variables estén correlacionadas, así, el modelo tendrá sentido práctico. El coeficiente de correlación lineal de Pearson mide la fuerza de la relación lineal entre las dos variables. Se puede expresar en función de las medias y las desviaciones típicas de las variables:\n\\[\\rho=\\frac{1}{n-1}\\sum(\\frac{x_i-\\bar{x}}{s_x})(\\frac{y_i-\\bar{y}}{s_y})=\\frac{1}{n-1}\\sum z_x \\cdot z_y=\\frac{s_{xy}}{s_x s_y}=\\frac{SP_{xy}}{\\sqrt{SC_x\\cdot SC_y}}\\]\nLos valores de este coeficiente se extienden de \\(-1\\) a \\(1\\), indicando ausencia de correlación cuanto más cercano a \\(0\\).\n\nHay que destacar el ejemplo ilustrativo del cuarteto de Anscombe, en el que 4 conjuntos de datos presentan el mismo valor del coeficiente de correlación lineal, pero su interpretación es muy distinta en cada uno de los 4 casos.\n\n\n\n1.2.3 Interpretación geométrica\nLos estimadores de mínimos cuadrados (MC) tienen una interpretación geométrica sencilla y gráficamente elocuente. Son aquellos que hacen que la recta de regresión simple minimice las distancias verticales (residuos) de toda la nube de puntos (Véase la Figura 5.7 de Peña (2002), apartado 5.4.4).\nAhora bien, también se puede representar el problema de MC en el espacio de las \\(n\\) observaciones (en lugar del espacio de 2 variables), lo que proporciona una interpretación geométrica interesante (véase Faraway (2004) apartado 2.3). La proyección ortogonal del vector \\(\\mathbf{Y}^\\top=(y_1, y_2, \\ldots, y_n)\\) sobre el plano definido por los vectores \\(\\mathbf{1}\\) y \\(\\mathbf{X}\\) genera el vector de residuos \\(\\mathbf{u}\\) perpendicular a ellos y a todos los vectores del plano. Además, aplicando el teorema de Pitágoras al triangulo rectángulo resultante de la proyección (Figuras 5.11 de Faraway (2004) y … de Peña (2002)) proporciona una forma alternativa de obtener la fórmula de la suma de cuadrados que veremos más adelante.\n\n\n1.2.4 Método de máxima verosimilitud\nTambién se pueden estimar los parámetros del modelo mediante el método de máxima verosimilitud (MV), que consiste en maximizar la función de verosimilitud \\(L(\\beta)\\). Suponiendo normalidad: \\[L(\\beta) = \\log (2\\pi)^{-n/2} \\sigma^{-n} \\exp\\left[-\\frac{\\sum_{i=1}^{n} u_i^2}{2\\sigma^2}\\right]\\]\nBajo dicho supuesto de normalidad, los estimadores MC coinciden con los MV. Por lo que los estimadores que minimizan la suma de cuadrados de los residuos, también maximizan la probabilidad de los datos observados.\nEn Peña (2002), apartado 5.4.1, se puede ver el detalle de la obtención de la estimación de los parámetros del modelo de regresión simple utilizando el método de máxima verosimilitud.\n\n\n1.2.5 Estimación de la varianza del modelo\n(5.4.3 Peña: varianza residual)\nEn 5.4.1 se obtiene también el estimador por MV de \\(\\sigma^2\\) que resulta ser la varianza de los residuos: \\[\\hat{\\sigma}^2=\\dfrac{\\sum e_i^2 }{n}.\\] En 5.4.3 se habla de la varianza residual, argumentando que los \\(n\\) residuos no son independientes, dadas las dos ecuaciones de restricción entre los residuos. Así los grados de libertad en este caso son \\(n-2\\) y por ello se define la varianza residual como \\[\\hat{S}_R^2=\\dfrac{\\sum e_i^2 }{n-2},\\] que …(propiedades en 5.5.4)\n\n\n1.2.6 Propiedades de los estimadores\n(5.5 de Peña, véase Tabla 5.3 página 264, aunque no aclara mucho.\nLa que sí que parece interesante la figura 5.15 que tiene 3 muestras al azar de la población y sus estadísticos asociados)\nPropiedades sobre \\(\\beta_0\\), \\(\\beta_1\\) y \\(\\sigma^2\\).\n(JUAN) 1. La ecuación de la recta de regresión también se suele expresar en la forma punto-pendiente: \\[(y - \\bar{y}) = \\hat{\\beta}_1 (x - \\bar{x}) \\] 1. Del punto anterior se deduce inmediatamente que el punto \\((\\bar{x}, \\bar{y})\\) pertenece a la recta estimada. 1. Los puntos con valores de \\(x\\) más alejados a la media tienen más influencia sobre la estimación de \\(\\beta_1\\), más cuanto menos puntos sean. 1. $_1 (_1, ) $ 1. $_0 (_0, (1 + ) ) $ 1. \\(Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) = -\\dfrac{\\bar{x}\\sigma^2}{nS_x^2}\\)\nPor lo tanto, cuando \\(\\bar{x}&gt;0\\), \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) estarán negativamente correlados.\n\n\n1.2.7 Contrastes de hipótesis\nEn una regresión lineal simple, realizar un contraste de hipótesis sobre el parámetro \\(\\beta_1\\) asociado a la variable explicativa, es equivalente al contraste de hipótesis sobre el coeficiente de correlación lineal de Pearson entre dicha variable y la respuesta, \\(\\rho\\): \\[\\left. \\begin{array}{ll}\nH_0: \\beta_1 = 0 \\\\\nH_1: \\beta_1 \\neq 0\n\\end{array} \\right\\rbrace\n\\qquad \\equiv  \\qquad\n\\left. \\begin{array}{ll}\nH_0: \\rho = 0 \\\\\nH_1: \\rho \\neq 0\n\\end{array} \\right\\rbrace\\]\nPodemos determinar la significación del coeficiente \\(\\beta_1\\) (de una correlación). El p-valor nos dará la fuerza de dicha significación. Para parejas de variables normales e incorreladas se cumple que: \\[\\rho\\sqrt{\\frac{n-2}{1-\\rho^2}} \\sim t_{n-2}.\\] Esto también se cumple de forma aproximada si las variables son no normales y si los tamaños de muestra son “grandes” (o no demasiado pequeños).\n(Apartado 5.6 de Peña, Inferencia parámetros) Habla de que la pdf de los estimadores obtenida en 5.5 se utiliza para construir estadísticos “t” que proporcionen IC y HT sobre los parámetros del modelo de regresión.\n(5.6.1 Fundamentos) Interesante!!! … Proporciona la expresión general (5.20) del IC para cualquier parámetro beta de centralización a partir de su estimador (si sigue una distribución normal)… Y en la tabla 5.4 aparecen los estadísticos (“pivote”) para IC y para HT, que son distintos para beta0 y beta1 !!!\n(5.6.2 El contraste de regresión)\nAunque “presentado” en 5.6.1 aquí lo relacionan con el ANOVA… Se trata de comparar la variabilidad/varianza residual (para cada \\(x_i\\) fijo) con la variabilidad/varianza de \\(y\\)… Si ambas son próximas concluiremos que \\(\\beta_1\\) puede ser 0!! Mientras que si la residual es mucho que menor que la de \\(y\\) concluiremos que \\(\\beta_1\\) es significativamente distinto de 0.\nEn la expresión (5.23) descompone las desviaciones de los datos (notación matricial) en dos componentes ortogonales. Por el teorema de Pitágoras se llega a la descomposición de la variabilidad (que aquí vemos en el siguiente apartado). Hace referencia a que ya salen estos resultados en el capítulo 2 (DoE) Introduce VT, VNE y VE como “variación” Total, no explicada y explicada… y en el apéndice 5B demuestra la distribución que siguen. (Interpretación página 271) Dice que es importante resaltar el supuesto de linealidad… Concluyendo que “aceptar H0” (recta horizontal) no implica que \\(x\\) e \\(y\\) sean independientes y lo ilustra con la figura 5.17 (típica de relación cuadrática entre x e y… -NO son independientes-, con recta de regresión estimada horizontal, x e y son linealmente independientes!!!)\n(Último párrafo pagina 272) “Estrictamente sólo podemos concluir a partir del contraste de regresión que las variable son independientes cuando tienen conjuntamente una distribución normal, ya que entre variables conjuntamente normales sólo son posibles relaciones lineales”.\n(Peña *5.7.2 Inferencias acerca del coeficiente de correlación (lineal),\n“La distribución en el muestreo de \\(r\\) es complicada. Para CH o IC… transformación de Fisher…” )\n\n\n1.2.8 Suma de Cuadrados en la regresión\nLos residuos, distancias verticales entre cada uno de los puntos, \\((x_i,y_i)\\), y la recta de regresión, \\(\\hat{y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\), expresan el error aleatorio del modelo. ¿Hasta qué punto es más importante el efecto de la variable \\(X\\) sobre la variable \\(Y\\) que el error de los residuos?\nLa recta de regresión siempre pasa por el centro de los datos \\((\\bar{X},\\bar{Y})\\), ese punto se conoce como centroide. Es el centro de gravedad de la nube de puntos.\nSi las variables \\(X\\) e \\(Y\\) no estuviesen relacionadas, no aportaría información sobre \\(Y\\) conocer los valores de \\(X\\). La mejor predicción que podríamos hacer sería predecir \\(Y\\) con su media, \\(\\bar{Y}\\), sin tener en cuenta el valor de \\(X\\). Este modelo, el mas sencillo, es el que vamos a intentar falsar.\n\\[\\begin{align*}\nH_0\\!:&\\ \\beta_1 = 0 \\\\\nH_1\\!:&\\ \\beta_1 \\neq 0\n\\end{align*}\\]\nEl estudio de la variabilidad (información) de la variable respuesta nos aportará evidencias que nos permitan rechazar \\(H_0\\) (falsarla).\n\\[SC_{total}=SC_y=\\sum (y_i-\\bar{y})^2\\]\n\\(SC_{total}\\) es el numerador de la habitual varianza muestral. Se puede calcular multiplicando dicha varianza por los grados de libertad \\(n-1\\).\nMatemáticamente se puede descomponerse en \\(SC_{regresion}\\) y \\(SC_{residual\\).\n\\[SC_{total}=SC_{regresion}+SC_{residual}\\] \\[\\sum (y-\\bar{y})^2={\\sum (\\hat{y}-\\bar{y})^2}+\\sum (y-\\hat{y})^2\\]\nEstudiamos los grados de libertad (gl) al calcular cada uno de los términos. Para \\(SC_{total}\\) “gastamos” \\(1\\) gl al dar la media, \\(\\bar{y}\\), por lo que tiene \\(n-1\\) gl. Los gl de los residuos son \\(n-2\\), necesitamos \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) para calcular \\(SC_{residuos}\\).\nCon lo que nos queda \\(1\\) gl para la suma de cuadrados de la regresión \\(SC_{regresion}\\). “Gastamos” \\(1\\) gl con el parámetro extra que hemos estimado \\(\\hat{\\beta}_1\\), la pendiente.\nPor último, se promedia cada suma de cuadrados por sus respectivos grados de libertad (SCM: Suma de Cuadrados Media), en definitiva, calculamos varianzas.\nNunca seremos capaces de realizar predicciones perfectas, todos los modelos son falsos, pero estamos interesados en comparar el Efecto de \\(X\\) sobre \\(Y\\) con el Error Aleatorio (residual):\n\\[\\frac{\\text{Efecto de X sobre Y}}{\\text{Error Aleatorio}}=\\frac{\\text{Varianza de la Regresión}}{\\text{Varianza Error}}=\\frac{SCM_{regresion}}{SCM_{error}}=F\\]\nEsto se conoce en estadística como un Análisis de la Varianza,ANOVA.\nNuestro modelo sencillo que intentamos falsar es \\(H_0:\\beta_1=0\\). Para que podamos falsar dicha hipótesis la Varianza de la Regresión debe ser mayor que la Varianza del Error, cuanto más grande mejor.\nComparamos entonces el estadístico F obtenido de dividir las dos varianzas con una distribución F con los grados de libertad correspondientes, \\(1\\) en el numerador y \\(n-2\\) en el denominador. Calculando el p-valor correspondiente se podrá rechazar (o no) la hipótesis nula.\nEn Peña (2002), Apéndice 5B, se dan detalles sobre la “Deducción de las distribuciones de sumas de cuadrados”.\n\n\n1.2.9 Bondad de ajuste: Coeficiente de Determinación, \\(R^2\\)\n(Faraway 2.7 “regresión simple”,\nPeña 5.7 El coef de corr en regresión,\n5.7.1 Coeficiente de determinación y coeficiente de correlación lineal)\nPor la descomposición mencionada anteriormente, \\(SC_{total}=SC_{regresion}+SC_{residual}\\), la proporción de variabilidad explicada por la regresión, respecto al total es un indicador de la bondad de la regresión:\n\\[\\frac{SC_{regresion}}{SC_{total}} = R^2\\]\nEsto es lo que se conoce como coeficiente de determinación, \\(R^2\\). Como proporción, puede tomar valores entre \\(0\\) y \\(1\\), evitando así la dependencia de las unidades de medida, y suele expresarse en porcentaje.\nLa siguiente Figura permite ilustrar el concepto de bondad de ajuste.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nEn la gráfica de la izquierda se tiene un buen ajuste de la recta de regresión, que produce un \\(R^2\\) elevado. Mientras que en la gráfica de la izquierda el \\(R^2\\) es próximo a cero, indicando que el conocimiento de los valores de \\(X\\) aporta casi nula información sobre los valores de variable \\(Y\\). Para más énfasis en la imagen de la izquierda se muestra la variabilidad total de \\(Y\\) frente a la variabilidad que tiene \\(Y\\) para casi cualquier valor de \\(X\\).\nObtención del coeficiente\nEn el caso de la regresión lineal simple coincide con el cuadrado del coeficiente de correlación, \\(\\rho\\) (véase Peña (2002), apartado 5.7.1, donde se dan detalles de cómo llegar al coeficiente de correlación a partir del de determinación, o a partir de la varianza residual).\n\n\n1.2.10 Error estándar de la pendiente y la ordenada en el origen\n(véase Apéndice 5A Peña: Deducción de los IC para los parámetros)\nAdemás del test F (ANOVA), es importante la varianza residual, \\(SC_{residuos}/n-2\\). La varianza residual cumple un papel importante para calcular el error estándar de la pendiente, \\(\\beta_1\\):\n\\[EE_b=\\sqrt{\\frac{SC_{residuos}/n-2}{SC_x}}\\]\nCon este error estándar podemos calcular un intervalo de confianza para la pendiente.\n\\[\\beta_1\\pm t_{\\alpha/2,n-2}\\cdot EE_{\\beta_1}\\]\nTambién la varianza residual es importante para calcular el error estándar de la ordenada en el origen, \\(\\beta_0\\):\n\\[EE_{\\beta_0}=\\sqrt{\\frac{SC_{residuos}/n-2 \\cdot \\sum x_i^2}{n \\cdot SC_x}}\\]\nPudiendo de la misma manera calcular intervalos de confianza para la ordenada en el origen.\nLo más interesante de esto es poder calcular el error estándar de una predicción y poder obtener intervalos de confianza para las predicciones.\n\\[EE_{\\hat{y}}=\\sqrt{\\frac{SC_{residuos}}{n-2}(\\frac{1}{n}+\\frac{(x_i-\\bar{x})^2}{SC_x})}\\]\n\\[(\\beta_0+\\beta_1\\cdot x_i) \\pm t_{\\alpha/2,n-2}\\cdot EE_{\\hat{y}}\\]\nCon R podremos obtener intervalos de confianza para predicciones del modelo. Otras herramientas son las bandas de confianza que algunos paquetes pueden dar a nuestros gráficos de regresión. Estas son bandas de confianza para la respuesta media, \\(\\bar{Y}\\) para cada valor individual de la \\(X\\). Esto significa que tenemos una confianza, habitualmente del 95%, en que la verdadera recta de regresión cae en la región marcada.\nLos intervalos de confianza para las predicciones son más amplios que para los intervalos de confianza de la recta.\n\n\n1.2.11 Interpretación de la recta de regresión\n(véase 5.9 Peña, tb cap 15 de FDCR)\nUna vez estimada la recta de regresión, si es significativa estadísticamente hablando, se puede pasar a interpretar los coeficientes/parámetros, si tienen un sentido práctico (y el análisis de residuos que veremos más adelante no invalida los supuestos en los que se basa).\nLa interpretación generalmente más importante es la del coeficiente \\(\\beta_1\\) dado que recoge el efecto sobre la variable \\(y\\) de la variación de una unidad de la variable explicativa \\(x\\). Su interpretación debe hacerse acorde a las unidades en la que esté recogida la variable (no es lo mismo que, si es una medida de temperatura, se haya medido en ºC que en ºK, o si es de tiempo que se mida en segundos o en días). Así el impacto sobre la respuesta del cambio de \\(1\\) ºC o de \\(1\\) s será de una magnitud muy distinta al cambio de 100ºC o \\(1\\) hora. Es más, el valor del coeficiente \\(\\beta_1\\) se podría aumentar o disminuir haciendo cambios de escala en las variables \\(y\\) y \\(x\\). Por último, ¿y qué pasa si en lugar de tener sólo una variable tenemos más variables? Veremos en el apartado siguiente de regresión múltiple, el cambio que supone, en la interpretación de los coeficientes, el tener varias variables explicativas.\nLa interpretación del parámetro \\(\\beta_0\\) es el del valor medio en ausencia del valor de la \\(x\\), que en ciertas ocasiones no pertenece al rango de variación de la variable \\(x\\) o puede no tener sentido (piense por ejemplo que la variable \\(x\\) recoge la edad de los individuos, ¿tiene sentido la media a \\(0\\) años?).\nSi la recta no fuese significativa, no significa que no haya relación entre \\(x\\) e \\(y\\), quizá la relación es no lineal (como veremos en el siguiente subapartado) o quizá el rango escogido para la \\(x\\) no es el idóneo para observar su influencia sobre la \\(y\\) (quizá es demasiado estrecho). Pero, como señala Peña, para encontrar relaciones causales hay que acudir a datos experimentales (como veremos en un tema posterior), porque en un experimento se puede intentar controlar los valores de la variable \\(x\\) que se cree que influyen sobre la \\(y\\) y aleatorizar el resto de variables para “repartir” su impacto sobre la respuesta. Mientras que si los datos son observacionales sólo se puede deducir covariación, pero no causalidad, como que haya más criminalidad en las ciudades con más policías. Aquí la causa es una tercera variable, el tamaño de la ciudad. Reducir el número de policías no causaría una reducción de la criminalidad!!\n\nAdemás se puede ver en Peña (2002) el Apéndice 5C: modelo con regresor aleatorio\n\n\n\n1.2.12 Diagnosis\n(Peña Capítulo 6 Diagnosis (y predicción))\n(Sección 3.3.3 del libro “Statlearning”. )\nLa diagnosis consiste en comprobar los supuestos básicos del modelo, a saber:\n\nlinealidad\nhomocedasticidad\nnormalidad\nindependencia\n\nEn la regresión simple, los supuestos de linealidad y homocedasticidad se pueden comprobar visualmente acudiendo al diagrama de dispersión. En la regresión múltiple, no es tan directo, y se utilizarán los residuos para comprobarlo, como veremos. Los supuestos de normalidad e independencia en la regresión simple, también deben comprobarse con los residuos del modelo. Aunque el de independencia podría no tenerse por construcción, por ejemplo, si tenemos datos temporales.\nLa diagnosis de los residuos puede dar pistas para solucionar los problemas de la regresión, lo que implica que la modelización es un proceso iterativo, con cierto aire artesanal.\nUn enfoque ligeramente distinto sobre la diagnosis se encuentra en Faraway (2004), que divide los problemas potenciales de la regresión estimada en 3 categorías:\n\nerror: se asume que los errores son independientes, tienen varianza constante y son normalmente distribuidos.\nmodelo: se asume que la parte estructural del modelo es correcta, es decir, que relación entre las variables es lineal.\nobservaciones inusuales: en ocasiones unas pocas observaciones no se ajustan al modelo, o cambian/influyen demasiado en el modelo.\n\nAdemás indica que las técnicas gráficas de diagnóstico son más flexibles, pero mucho más difíciles de interpretar, que las de contrastes, que son sencillas y directas (no requieren de intuición), pero no permiten una visualización general del problema.\nCon la ayuda de R será bastante sencillo obtener los denominados gráficos de diagnóstico. Pero ¿qué se pretende observar en dichos gráficos? En líneas generales, se pretende observar aleatoriedad en los residuos, que la “nube de puntos” que generen no se aprecie ningún patrón sistemático. Salvo en el gráfico Q-Q en el que se mira la normalidad de los residuos, que se desea que se ajusten a la diagonal que aparece en el gráfico.\n\n1.2.12.1 Linealidad\nA partir del gráfico de residuos fente a valores estimados por el modelo, para cada valor observado, se deseará observar aleatoriedad, valores dispersos entorno al 0 (verticalmente), sin tendencias ni otros patrones marcados.\n\n\n\n1.2.13 POR AQUÍ (JUAN)\n\n1.2.13.1 Homocedasticidad\nLos residuos deben tener varianza constante, homocedasticidad, con respecto a los valores de la variable \\(x\\), en el caso de regresión simple. La falta de homocedasticidad invalida el uso de los estimadores MC/MV pues implica distinta precisión en las estimaciones.\nAnálisis gráfico\nEl gráfico apropiado para observar la homocedasticidad/heterocedasticidad es el de residuos fente a valores estimados por el modelo (el mismo que para la linealidad, que es también válido para el caso de regresión múltiple). De nuevo se desea observar aleatoriedad, ausencia de patrones.\nEn el siguiente gráfico se pueden ver ejemplos simulados de distintas situaciones, que se interpretan de una forma clara. Pero la realidad supera la ficción… Se necesita cierta experiencia con casos reales para no cometer equivocaciones al interpretar gráficos de residuos.\n\n\n\n\n\n\n\n\n\nLo deseable en este gráfico es encontrar una nube de puntos dispersos (y simétricos, mirando verticalmente) alrededor de 0, sin ningún patrón aparente (como en el primer gráfico). Si se observa un patrón, como una forma cónica (segundo gráfico) o una tendencia (como la no lineal del tercer gráfico), puede indicar problemas de heterocedasticidad (varianza no constante) o no linealidad en el modelo. Estas dos últimas situaciones dan pistas de las posibles acciones a tomar sobre los datos, como transformar la variable \\(x\\) o \\(y\\) para conseguir homocedasticidad, o incluir algún término no lineal en la variable \\(x\\) (manteniendo el modelo lineal en los parámetros). Por su parte, el primer gráfico permitiría validar gráficamente el supuesto de homocedasticidad.\nComo se verá en la práctica, se suele también dibujar y analizar la homocedasticidad en el gráfico del valor absoluto de los residuos frente a los valores estimados. Al tomar el valor absoluto se aumenta la resolución para detectar la falta de homocedasticidad. Ahora bien, no permite la comprobación de la nolinealidad.\nUna alternativa, no tan estandarizada en la diagnosis, es la propuesta de Faraway (2004), de dibujar los residuos frente a cada uno de los \\(x_i\\) -en caso de regresión múltiple-. Para todas la variables del conjunto de datos, tanto las incluidas en el modelo, como las no incluidos, mirando en estas últimos si existe alguna relación que indique la necesidad de incluirlo en el modelo.\nEn ocasiones los gráficos pueden ser ambiguos, pero al menos permiten verificar que no hay grandes desviaciones de los supuestos del modelo.\nContrastes También existen contrastes para detectar heterocedasticidad. Pueden parecer más precisos, pero los gráficos de residuos son más versátiles: permiten detectar problemas, revelar estructuras ocultas, que no se vislumbraban con los contrastes, como apunta Faraway (2004). Por ello, predomina un enfoque gráfico para el diagnóstico, acudiendo a los contrastes como complemento para confirmar lo observado en los gráficos.\nLos contrastes para detectar heterocedasticidad, dependen de la hipótesis alternativa especificada, la hipótesis nula es clara: varianza constante: \\[H_0: \\sigma^2 = \\text{cte} \\] Así, el contraste puede detectar bien un tipo específico de heterocedasticidad, pero no tener potencia suficiente para otros.\nDe entre los distintos contraste para comprobar la homocedasticidad destacan, el contraste de Bartlett y el de Levene, que evalúan la hipótesis nula de igualdad de varianzas entre \\(k\\) grupos. Los estadístico de contraste se pueden encontrar en https://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm y https://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm, respectivamente. El estadístico de Bartlett sigue aproximadamente una distribución \\(\\chi^2\\) con \\(k - 1\\) grados de libertad bajo \\(H_0\\). Mientras que el de Levene sigue aproximadamente una distribución \\(F\\) con \\(k-1\\) y \\(n-k\\) grados de libertad.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#por-aquí-yo---peña",
    "href": "Cap1-LM.html#por-aquí-yo---peña",
    "title": "1  Modelos lineales",
    "section": "1.3 POR AQUÍ (YO - Peña)",
    "text": "1.3 POR AQUÍ (YO - Peña)\n\n1.3.0.1 Normalidad\nLos residuos deben seguir una distribución normal, para justificar el uso de estimadores MC/MV. El gráfico apropiado para evaluarlo es el denominado Q-Q plot (gráfico Cuantil-Cuantil): un diagrama de dispersión de los cuantiles de los residuos frente a los cuantiles de la distribución normal (con la misma media y desviación típica de los residuos). La interpretación, como diagrama de dispersión, es bien sencilla, los residuos seguirán una distribución normal cuanto más se alineen los puntos del diagrama sobre una linea recta “guía” (y más diagonal sea dicha “guía”). Se incumple la normalidad generalmente por las colas, con puntos que se alejan ostensiblemente de la recta “guía”, por ejemplo, es típico observar curvatura en forma de S, indicando que los residuos tiene colas más ligeras que la distribución normal, o curvatura en forma de U indicando que siguen otra distribución.\n\n\n\n\n\n\n\n\n\nContrastes Para completar la comprobación gráfica, se acude a los contrastes de normalidad. El más difundido es del de Shapiro-Wilks, basado precisamente en el gráfico Q-Q, y considerado uno de los que más potencia poseen para contrastar normalidad. \\[\\left. \\begin{array}{ll}\nH_0: \\text{los residuos provienen de una distribución normal} \\\\\nH_1: \\hspace{1.4cm}\\text{... no provienen...}\n\\end{array} \\right\\rbrace\\] Para más información sobre el test de Shapiro-Wilk se puede consultar https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm. En este caso, el estadístico de contraste no sigue una distribución de probabilidad conocida y sus probabilidades se han calculado mediante simulaciones por el método de Monte Carlo.\n\n\n1.3.0.2 Independencia\nLa independencia es un supuesto que puede comprobarse con los gráficos de residuos, pero que puede incumplirse desde el planteamiento del problema, de la recogida de datos, etc. sin necesidad de llegar al análisis de residuos, en el que incluso podría no quedar reflejado el incumplimiento.\nPara comprobar la independencia temporal de los datos se visualizará el gráfico de residuos frente al orden en la toma de datos (si se dispone de ello), o, en su defecto, en el orden que se tengan los datos (que podrían haber sido ordenados, lo que impediría su correcto análisis).\nContrastes Como en los anteriores supuestos, se suele acudir a contrastes para completar el análisis de independencia. El más utilizado es el de Durbin-Watson, que comprueba la presencia de autocorrelación (relación temporal entre los residuos). \\[\\left. \\begin{array}{ll}\nH_0: \\text{los residuos no tienen correlación temporal} \\\\\nH_1: \\text{los residuos siguen un proceso autorregresivo de primer orden, } AR(1) \\end{array} \\right\\rbrace\\] Más información en https://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic.\n\n\n1.3.0.3 Soluciones\nComo se ha comentado, la comprobación de los supuestos mediante los gráficos de residuos, a la par que pueden conducir a rechazar uno o varios de ellos, pueden proporcionar pistas para su solución.\nLo primero que suele saltar a la vista en el análisis gráfico es la presencia de observaciones atípicas, que contribuyen a la falta de linealidad, o de homocedasticidad. Lo apropiado es analizar el impacto de dichas observaciones comparando los modelos estimados con tales observaciones o sin ellas. Para averiguar la influencia de las observaciones (sean atípicas o no) en la regresión, se han desarrollado medidas como la distancia de Cook, basada en el leverage, que vamos a ver en el siguiente apartado. El resultado de este análisis puede llevar a detectar errores de medición, o que la definición funcional (como forma lineal) del problema no es la adecuada.\nOtra posible solución para obtener linealidad y/o homocedasticidad es transformar las variables, bien \\(y\\), bien \\(x\\), o ambas. Por ejemplo, la relación entre la variable \\(y\\) y la \\(x\\) podría ser exponencial, por lo que, tomando como respuesta \\(\\log(y)\\) se tendrá linealidad. Sobre la heterocedasticidad, en ocasiones se da por la dependencia de la varianza de \\(y\\) respecto de \\(x\\). Si se dividen las observaciones por la estructura que provoque \\(x\\) en la varianza, se tendrá un modelo homocedástico. En la práctica, el gráfico de residuos frente a valores estimados puede dar pistas sobre la transformación a realizar. Se suelen intentar varias transformaciones, pues es un arte encontrar la transformación más adecuada (la intuición y la experiencia pueden ayudar). Un pequeño detalle es tener en cuenta que transformaciones como la raíz cuadrada o el logaritmo no funcionarán si la variable respuesta \\(y\\) toma valores negativos, pero se puede subsanar considerando \\(y+\\delta\\) para evitarlo (aunque se pierde la interpretación directa de los resultados).\nUn caso más complejo de intuir es la introducción de variables para conseguir linealidad, bien puede ser, términos polinomiales de la misma variable, lo que conduce a una regresión polinómica, o bien, se pueden añadir otras variables, lo que lleva a un modelo lineal de regresión múltiple. Abordaremos ambos más adelante.\nOtras opciones de resolver los problemas por incumplimento de los supuestos básicos pasan por acudir a regresión no paramétrica (que se escapa del alcance de este material) o, en el caso de la heterocedasticidad, aprovechar la estructura de varianza para utilizar el método de mínimoc cuadrados ponderados.\n\n\n1.3.1 Observaciones influyentes\nSe sabe que, cuanto más alejado de su media esté el valor de \\(x\\) observado más influencia tendrá sobre la pendiente de la recta de regresión. Sobre todo si no concuerda su pendiente respecto a la media, con la pendiente marcada por el resto de valores, y más si es un valor atípico.\nLeverage\nEl leverage (efecto palanca en español, aunque es poco utilizado) es una medida necesaria para medir dicha influencia. En el caso de regresión lineal simple se puede calcular como: \\[l_i = \\dfrac{1}{n} \\left[ 1+ \\dfrac{(x_i - \\bar x)^2 }{S_x^2} \\right] \\] donde \\(S_x^2\\) es la varianza de los datos (la dividida por \\(n\\)). El leverage toma valores entre \\(1/n\\) y \\(1\\).\nresiduos estandarizados\nUna observación con un valor de leverage próximo a \\(1\\) se considera muy influyente, pudiendo hacer que la recta de regresión pase por tal observación (además de pasar por el centro de gravedad de la nube de puntos, \\((\\bar x, \\bar y)\\)). Este hecho implica que en las observaciones con alto leverage, el residuo y su varianza serán pequeños, mientras que en las observaciones cercanas a la media, el leverage será bajo (en la media será justo \\(l_i = 1/n\\)) y su residuo y varianza serán más grandes. Por ello se suelen definir los residuos estandarizados: \\[r_i = \\dfrac{u_i}{\\hat s_R \\sqrt{1-l_i}}\\] que seguirán una distribución normal tipificada, si las hipótesis del modelo son ciertas. Generalmente, los programas de software estadístico calculan estos residuos estandarizados y construyen los gráficos de residuos con ellos.\nDistancia de Cook La distancia de Cook es la medida de influencia utilizada en la práctica. Se basa en medir el cambio en la recta de regresión al eliminar la observación \\(i\\). Viene dada por: \\[D_i = \\dfrac{(\\hat y_i - \\hat y_{-i})^2}{2\\hat s_R^2 l_i} \\] donde: - \\(\\hat y_i\\) es la estimación para la observación \\(i\\)-ésima, basada en todos los datos, - \\(\\hat y_{-i}\\) es la estimación para la observación \\(i\\)-ésima, basada en todos los datos menos el \\(i\\)-ésimo. Con esta medida, un punto es influyente si \\(D_i&gt;1\\). Los puntos con alto leverage pueden ser influyentes, pero no lo son siempre.\nUn ejemplo aclarador podría ser el del PIB vs. emisiones de CO2, ambas variables están relacionadas y, podríamos tener valores de distintos países con los que se tendría una nube de puntos que indicase que a myor PIB, mayores emisiones. Si se tiene el dato de un país con un gran PIB, si sus emisiones también son altas y acordes a la recta determinada por el resto de países, dicho país tendrá un alto leverage, pero su distancia de Cook no será mayor que 1. Por el contrario, si su valor de emisiones es muy distinto al que estimaría la recta determinada por el resto de países (sea el valor de emisiones mucho mayor o mucho menor), dicho país tendrá alto leverage y también alta distancia de Cook.\n\n\n1.3.2 Transformaciones\nComo se ha mencionado, puede ser útil transformar las variables para conseguir una relación lineal entre ellas, o conseguir homocedasticidad…\nEn la práctica, a la hora de transformar la variable respuesta, se acude a la familia de transformaciones Box-Cox, definida como: \\[\ny^{(\\lambda)} =\n\\begin{cases}\n\\frac{y^\\lambda - 1}{\\lambda}, & \\lambda \\ne 0 \\\\\n\\ln y, & \\lambda = 0\n\\end{cases}\n\\]\nY cuyos casos particulares se esquematizan en la siguiente tabla:\n\n\n\n\\(\\lambda\\)\nTransformación\nNomenclatura\n\n\n\n\n\\(1\\)\n\\(y\\)\nIdentidad\n\n\n\\(1/2\\)\n\\(\\sqrt{y}\\)\nRaíz cuadrada\n\n\n\\(0\\)\n\\(\\log y\\)\nLogaritmo natural\n\n\n\\(-1/2\\)\n\\(\\frac{1}{\\sqrt{y}}\\)\nInversa de la raíz cuadrada\n\n\n\\(-1\\)\n\\(\\frac{1}{y}\\)\nInversa",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#modelo-lineal-de-regresión-múltiple",
    "href": "Cap1-LM.html#modelo-lineal-de-regresión-múltiple",
    "title": "1  Modelos lineales",
    "section": "1.4 Modelo lineal de regresión múltiple",
    "text": "1.4 Modelo lineal de regresión múltiple\nCuando disponemos de más de una variable explicativa o independiente (continuas), entonces nuestro modelo pasa a ser un modelo con múltiples variables. ¿Cuáles? ¿En qué relación?\nLa regresión múltiple tiene los siguientes retos:\n\nLa mayor parte de los estudios son observacionales, no experimentales.\nEs muy habitual tener demasiadas variables explicativas.\nEs muy habitual tener pocas observaciones, valores de la variable respuesta.\nNo disponer de todas las combinaciones posibles de las variables explicativas.\n\nDesde el punto de vista estadístico:\n\nCuidado con las variables explicativas correlacionadas o correladas (apartan información redundante).\n¿Qué variables incluir en el modelo?\n¿Hay curvatura en la respuesta? Modelo lineal de regresión no lineal.\nLas variables explicativas ¿interaccionan entre sí?\nTodo esto hace que el número de parámetros se dispare Vs pocas observaciones.\n\nRecordemos:\n\nTodos los modelos son falsos.\nAlgunos modelos son mejores que otros.\nEl modelo perfecto y exacto no existe.\nEn un modelo, la sencillez es un acierto (principio de parsimonia).\n\nTipos de modelos:\n\nSaturado: Un parámetro para cada observación. Ajuste perfecto. Grados de libertad 0.\nMaximal: Contiene p variables y sus interacciones. Muchos de estos términos son despreciables. Grados de libertad \\(n-p-1\\).\nMinimal y Adecuado: Contiene las variables e interacciones significativas. Grados de libertad \\(n-p'-1\\).\nModelo ‘Nulo’:Único parámetro, \\(\\bar{y}\\). Grados de libertad \\(n-1\\).\n\nCorrelación\nEn la regresión múltiple, hay que tener claro que la correlación se desea entre cada variable explicativa y la respuesta, y no entre ellas. De haber correlación entre variables explicativas se presenta el problema denominado multicolinealidad. Para resolverlo se puede acudir a la selección de variables (Cap 2), teniendo en cuenta el factor de inflación de la varianza (VIF). Ahora bien, conviene recordar que correlación no implica causalidad (ejemplos: delitos vs policías, limones vs accidentes,…).\n\n1.4.1 Estimación MC\nEn 2.4 de Faraway se puede ver la obtención del estimador MC para los parámetros del modelo de regresión lineal múltiple: \\[\\hat{\\beta}=(X^\\top X)^{-1}X^\\top\\] siempre que \\(X^\\top X\\) sea invertible.\nY se habla de la matriz \\(H=X(X^\\top X)^{-1}X^\\top\\), denominada the hat-matrix, the orthogonal projection of y onto the space spanned by X.\nCon esta matriz, se pueden expresar:\n\nlos valores predichos o estimados: \\(\\hat{Y}=HY=X\\hat{\\beta}\\)\nlos residuos: \\(\\hat{\\epsilon}=Y-X\\hat{\\beta}=(I-H)Y\\)\nla suma de cuadrados residual (RSS por sus siglas en inglés): \\(\\hat{\\epsilon}^\\top\\hat{\\epsilon}=Y^\\top((I-H)Y\\)\n\n(Traído de la INTERP GEO) En Faraway (2.3) se dice que el propósito conceptual del modelo es representar, de la mejor manera posible, la complejidad de la respuesta, dada en el espacio n-dimensional, en un espacio más pequeño, el k-dimensional de las variables. Si el modelo se ajusta bien, la estructura de los datos queda capturada en esas k dimensiones, dejando la variación aleatoria en los residuos que pertenecen a un espacio de dimensión n-k.\n\n\n1.4.2 Bondad de ajuste\nEn el caso de regresión lineal múltiple la bondad de ajuste se puede medir con el coeficiente de determinación, \\(R^2\\), cuya fórmula es: \\[ R^2 = \\dfrac{\\text{Variabilidad explicada (VE)}}{\\text{Variabilidad total (VT)}=\\dfrac{\\sum (\\hat y_i - \\bar y)^2 }{\\sum (y_i - \\bar y)^2 }=\\]\nA partir de \\(R^2\\) se obtiene \\(R\\), el coeficiente de correlación múltiple.\nEl \\(R^2\\) ajustado es una corrección para suavizar el comportamiento del \\(R^2\\) que siempre aumenta al incluir en el modelo más variables explicativas. Consiste en considerar varianzas (medias de sumas de cuadrados, considerando sus respectivos grados de libertad, en lugar de sumas de cuadrados que se utilizan en \\(R^2\\)): \\[ R^2_{\\text{corregido}} = 1 - \\dfrac{\\text{Varianza residual}}{\\text{Varianza de $y$}} = R^2 - (1 - R^2) \\dfrac{k}{n+k-1}\\] Así, el \\(R^2\\) corregido de una regresión múltiple siempre será menor que el \\(R^2\\), incluso podría tomar valores negativos.\n\n\n1.4.3 Teorema de Gauss-Markov\n\n(véase 2.6 de Faraway) Faltan detalles en las fórmulas MTM, recortar!!\n\nHay varias razones para usar el estimador MC de \\(\\beta\\).\n\nes el resultado de una proyección ortogonal sobre el espacio del modelo (interpretación geométrica).\nes también el estimador de máxima verosimilitud si los errores son independientes e idénticamente distribuidos siguiendo una normal.\nes el mejor estimador lineal insesgado (BLUE) según el teorema de Gauss-Markov.\n\n\nTeorema de Gauss-Markov\nSupongamos que \\(\\mathbb{E}[\\boldsymbol{\\varepsilon}] = 0\\), \\(\\text{Var}(\\boldsymbol{\\varepsilon}) = \\sigma^2 \\mathbf{I}\\) y que la parte estructural del modelo, \\(\\mathbb{E}[\\mathbf{y}] = \\mathbf{X} \\boldsymbol{\\beta}\\), es correcta.\nSea \\(\\mathbf{c}^\\top \\boldsymbol{\\beta}\\) una función estimable, esto es, si, y solo si, existe una combinación lineal \\(\\mathbf{a}^\\top \\mathbf{y}\\) tal que $ [^] = ^,$\nentonces,\ndentro de la clase de todos los estimadores lineales insesgados de \\(\\mathbf{c}^\\top \\boldsymbol{\\beta}\\), el estimador de mínimos cuadrados tiene la varianza mínima y es único.\n\nLas funciones estimables incluyen predicciones de observaciones futuras, lo que explica por qué vale la pena considerarlas. Si la matriz \\(\\mathbf{X}\\) tiene rango completo, entonces todas las combinaciones lineales son estimables.\nLa demostración puede encontrarse en Faraway (2004), páginas … a …\nConsideraciones adicionales El teorema de Gauss-Markov recomienda usar mínimos cuadrados, salvo que haya una buena razón para no hacerlo. Como cuando los errores estén correlados y la varianza no sea constante, inclumpliendo así los supuestos del teorema. En tal caso se deben usar mínimos cuadrados generalizados.\nSi los errores son no normales pero se comportan bien, típicamente con colas pesadas, puede que estimadores robustos, generalmente no lineales, funcionen mejor. O cuando se da multicolinealidad, se pueden preferir estimadores sesgados como la regresión ridge (que veremos en un capítulo posterior)\n\n\n1.4.4 (Identificabilidad)\nLa identificabilidad es un concepto clave en modelos estadísticos, especialmente en regresión. Un modelo es identificable si los parámetros del modelo pueden ser estimados de manera única a partir de los datos observados. En otras palabras, para cada conjunto de datos, hay una única solución para los parámetros del modelo. Un modelo es no identificable si hay múltiples conjuntos de parámetros que pueden generar los mismos datos observados. Esto puede ocurrir por varias razones, como la presencia de variables redundantes o la falta de información suficiente en los datos para distinguir entre diferentes configuraciones de parámetros.\nEn el contexto de la regresión lineal, la identificabilidad se refiere a la capacidad de estimar los parámetros del modelo de manera única a partir de los datos. Si el modelo es identificable, entonces cada conjunto de datos conducirá a una única estimación de los parámetros \\(\\boldsymbol{\\beta}\\) .\nEl estimador de mínimos cuadrados es la solución de las ecuaciones normales:\n\\[\n\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}^\\top \\mathbf{y}\n\\]\ndonde \\(\\mathbf{X}\\) es una matriz de dimensión \\(n \\times p\\) . Si \\(\\mathbf{X}^\\top \\mathbf{X}\\) no tiene rango completo, es decir, cuando sus columnas son linealmente dependientes, es singular y no puede invertirse, entonces habrá infinitas soluciones para las ecuaciones normales y el modelo será, al menos en parte, no identificable.\nLos paquetes estadísticos manejan la no identificabilidad de distintas formas. En el caso de regresión anterior, algunos pueden devolver mensajes de error, y otros pueden ajustar el modelo porque los errores de redondeo eliminan la no identificabilidad exacta. En otros casos, se aplican restricciones, pero estas pueden ser diferentes de las que uno espera. Por defecto, R ajusta el modelo identificable más grande eliminando variables en orden inverso al que aparecen en la fórmula del modelo.\n\n\n1.4.5 Multicolinealidad\nLa multicolinealidad es un problema que puede aparecer en regresión múltiple, cuando una variable explicativa pueda estar altamente correlacionada con otra u otras explicativa(s). Aquellas variables que presentan multicolinealidad producen una inflación en la estimación de la varianza, y, con ello, una peor precisión para detectar significatividad.\nExisten distintas vías para detectar el grado de multicolinealidad existente. Por ejemplo, la matriz de correlaciones lineales y su determinante, el factor de inflación de la varianza, el número de condición, etc.\nNos centramos en el factor de inflación de la varianza (VIF). Su cálculo para cada parámetro \\(\\beta_i\\) se puede obtener al realizar una regresión (auxiliar) tomando como variable respuesta la variable asociada a dicho parámetro, \\(X_i\\), y como explicativas el resto de variables explicativas: \\[X_i = \\alpha_0 + \\alpha_1 X_1 + \\ldots + \\alpha_{i-1}X_{i-1} + \\alpha_{i+1}X_{i+1} + \\ldots + \\alpha_k X_k.\\] Si el coeficiente de determinación de esta regresión auxiliar, \\(R_i^2\\), es alto, dicha variable \\(X_i\\) tiene una alta relación lineal con el resto de variables, la denominada multicolinealidad. El factor de inflación de la varianza se define como: \\[VIF(\\beta_i) = \\dfrac{1}{1 - R_i^2}\\] El VIF tomará valores entre 1 e \\(\\infty\\), indicando los valores cercanos a 1 ausencia de multicolinealidad. Si, por ejemplo, \\(R_i^2 = 0.8\\) o \\(R_i^2 = 0.9\\), se tendrían VIF de 5 o 10, valores que se toman en la práctica para alertar de alta multicolinealidad.\n\nEl lector interesado puede obtener más información en https://rnoremlas.quarto.pub/un_rincon_para_r/posts/17_multicolinealidad/\n\n\n\n1.4.6 Otros casos de regresión lineal\nExisten otros modelos de regresión que pueden ajustarse con la técnica de modelos lineales mediante transformaciones. Por ejemplo si hay una relación exponencial entre las variables \\(X\\) e \\(Y\\), se pueden tomar logaritmos en ambas variables linealizando así el modelo. O si la relación es potencial, considerar una potencia de la variable \\(X\\): un modelo lineal de regresión no lineal. El tomar un modelo más complejo sólo tiene sentido si produce resultados significativos a la hora de explicar la relación.\nAhora bien, hay que tener en cuenta que estas transformaciones pueden alterar la interpretación de los coeficientes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#sec-pract-airquality",
    "href": "Cap1-LM.html#sec-pract-airquality",
    "title": "1  Modelos lineales",
    "section": "1.5 Caso práctico: airquality",
    "text": "1.5 Caso práctico: airquality\nEl primero de los casos prácticos de modelización lineal que se va a tratar en profundidad se basa en los datos airquality que contienen 153 medidas (de 6 variables) de calidad del aire en Nueva York. Entre otros, se estudia en Casero-Alonso y Durbán (2024), concretamente en: https://cdr-book.github.io/cap-lm.html#Casos. Aquí se presentarán ejemplos ligeramente distintos y con algo más de profundidad.\n\n1.5.1 Exploración de los datos\nAntes de comenzar el proceso de modelización lineal, es muy recomendable explorar los datos. El conjunto de datos airquality está disponible en la distribución base de R:\n\n?airquality #Para obtener más información sobre las variables, unidades, etc. \n\n\n#Primeras filas del data frame\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nComo se observa hay varios valores faltantes (NA) entre los datos, lo que puede afectar a los resultados de la regresión. Su impacto debe estudiarse, pero sobrepasa el nivel de este curso.\nAdemás, al observar detenidamente los valores de la variable Day se puede inferir que los datos son temporales, lo que requiere un análisis específico (de Series Temporales, que también sobrepasa el alcance de este curso). El tener datos temporales hace que se incumpla, de partida, desde el plano teórico/conceptual, como se ha mencionado en @ref(#sec-Independencia).\nAun con esta situación (valores faltantes, datos temporalmente dependientes) analizaremos diversos modelos lineales. Eso sí, no tiene sentido intentar explicar la influencia lineal de la variable Day en el resto de variables, por ejemplo, por el hecho de que los días \\(1\\), \\(2\\), etc. de meses distintos no son “homogéneos”, etc.\nProcedemos a obtener resúmenes numéricos y gráficos:\n\n## Resúmenes numéricos de las variables\nsummary(airquality)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\n## Estructura (formato) del data frame\nstr(airquality)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n## Resumen gráfico de relaciones pareadas\npairs(airquality, upper.panel = panel.smooth)\n\n\n\n\n\n\n\n\nDel resumen numérico se obtiene que hay 2 variables Ozone y Solar.R que contienen NAs, principalmente la primera con 37 valores no disponibles. Estos resúmenes, junto con la visualización de la estructura (y la consulta de las unidades en las que están medidas) permiten determinar que las variables Ozone, Solar.R, Wind y Temp se pueden considerar cuantitativas continuas (obsérvense sus rangos) a los efectos de modelización lineal, aunque sólo Wind esté definida como num (y el resto como int). Sobre Temp, viendo el rango de sus valores (de 56 a 97), no parece que estén en ºC. ¿Y cómo considerar a la variable Month? El tratamiento más adecuado es como variable cualitativa, dado que, aunque vemos valores numéricos, de 5 a 9, no puede interpretarse como una variable continua en la que tenga sentido incrementar 1 unidad. Además, otorgarle valores 5 a 9 es un convenio para tratarlas por ordenador de una manera más cómoda, pero realmente sus valores son mayo, junio… Queda así más claro que no tiene sentido aumentar 1 unidad, por ejemplo, cuando estamos en el mes 12.\nDe los diagramas de dispersión, al incluir el panel.smooth se pueden observar líneas de tendencias suavizadas de los datos. Se aprecia que casi ninguna de las relaciones entre las variables numéricas es lineal, sólo lo parece Wind frente a Temp. Los gráficos que involucran a la variable Month se aprecian distintos al resto, por los pocos valores de dicha variable, mientras que los que involucran a Day no reflejan lo mismo.\n\nAlternativas: Existen distintas funciones/paquetes más o menos sofisticados que realizan este análisis exploratorio de distintas maneras. El lector interesado puede explorar: - El paquete skimr y su función skim. - - …\n\n\nPor ejemplo, con las siguientes funciones se pueden obtener gráficos complementarios al gráfico obtenido anteriormente con pairs().\n\n\n#Es necesario tener los paquetes instalados previamente\nlibrary(corrplot) \ncorrplot(cor(airquality, use = \"pairwise\"))\nlibrary(GGally)\nggpairs(airquality)\n#Para que aparezca los diagramas de dispersión \"arriba\"\nggpairs(airquality,\n         upper = list(continuous = wrap(\"points\", alpha = 0.7)),  \n         lower = list(continuous = wrap(\"cor\", size = 4)),  \n         diag = list(continuous = wrap(\"densityDiag\")))  \n\n\n\n1.5.2 lm() simple\nPara ilustrar toda la teoría de las secciones anteriores se va a realizar una regresión lineal simple. La función lm() de R proporciona, a partir de los datos disponibles, la estimación de los parámetros del modelo que se especifique. También se pueden obtener, aplicando distintas funciones, la significación de dichas estimaciones, sus intervalos de confianza, predicciones, etc. Así como los gráficos de diagnóstico.\nModelización\nConsideramos para empezar un modelo de regresión simple, a diferencia del libro CDR. De entre los posibles modelos nos decantamos por intentar explicar la concentración de Ozono en función de la radiación solar: \\[Ozone = \\beta_0 + \\beta_1 Solar.R + \\epsilon\\]\n\nEl lector tiene aquí una buena tarea conceptual, la de ejercitarse en plantear modelizaciones lineales, que ¡tengan sentido práctico!\n¿Tiene sentido explicar/predecir Ozone en función de los valores de Solar.R observados? ¿Y al revés? ¿O explicar/predecir Day en función de Ozone? …\n\nEn R definiríamos así el modelo anterior:\n\nmodelo &lt;- Ozone ~ Solar.R\n\n\nLa sintaxis básica (regresión simple) es respuesta ~ explicativa.\nLa extensión a regresión múltiple es directa: resp ~ explica1 + explica2 indicaría un modelo con predictores explica1 y explica2, y así sucesivamente.\nEn la sección de regresión múltiple se indican algunos “trucos”.\n\n\nPregunta\n¿Cómo obtener la regresión de proporcionalidad directa con R?\n\nEstimación\nEn R pueden obtenerse de varias maneras utilizando la función lm():\n\n#Opción 1: aprovechando la definición anterior del modelo\n#lm(modelo, data = airquality) \n#Opción 2: directa -&gt; lectura más clara\nlm(Ozone ~ Solar.R, \n   data = airquality)\n\n\nCall:\nlm(formula = Ozone ~ Solar.R, data = airquality)\n\nCoefficients:\n(Intercept)      Solar.R  \n    18.5987       0.1272  \n\n\nAquí se pueden ver las estimaciones para los dos parámetros obtenidas a partir de los datos (omitiendo las observaciones con valores NA).\n\nLa función lm() aplicada a un modelo simple y ~ x (o múltiple, como veremos) requiere que los datos de las variables y y x estén en el Environment, o se especifique en data el conjunto de datos en el que están las dos variables.\n\nDiagrama de dispersión y recta estimada\nDado que se está analizando la relación entre dos variables, esta se puede visualizar fácilmente en un diagrama de dispersión, al que se puede añadir la recta estimada a partir de los datos.\n\npar(pty = \"s\") ## \"p\"lot \"ty\"pe \"s\"quare (recomendado para gráficos de dispersión)\nplot(airquality$Solar.R, airquality$Ozone)\nabline(a=18.5987, b=0.1272, col = \"red\") ## a = intercept, b = slope\n\n\n\n\n\n\n\n\nEn problemas de regresión múltiple con dos variables explicativas se puede obtener un diagrama de dispersión en 3 dimensiones y añadirle el plano de regresión, pero suele ser compleja su visualización. Con más variables es imposible obtener tales visualizaciones, diagramas de dispersión en \\(k\\) dimensiones e hiperplanos de regresión, lo que conduce a abordar el problema de regresión lineal múltiple mediante un proceso de abstracción.\nAnálisis de residuos\nValoremos la adecuación del modelo examinando los residuos. Primero guardamos el ajuste/estimación basado en los datos en un objeto, que denominamos rls, para su uso posterior:\n\nrls &lt;- lm(Ozone ~ Solar.R, data = airquality)\npar(mfrow = c(2, 2), #presenta los gráficos en formato 2x2\n    pty = \"s\",\n    mex = 0.66,\n    cex = 0.75)\nplot(rls)\n\n\n\n\n\n\n\n\n\nEl primero de los 4 gráficos de diagnóstico (Residuals vs Fitted) refleja heterocedasticidad (varianza no constante, forma de embudo), tal y como ya se podía apreciar en el diagrama de dispersión de Solar.R frente a Ozone que se obtuvo anteriormente. En este gráfico de aquí, conforme aumenta el valor de los valores estimados (fitted), la dispersión de los residuos se hace más grande, salvo en la parte final.\nEl gráfico que mejor refleja esa heterocedasticidad es el tercero (Scale-Location), donde la línea roja de tendencia dista de la horizontalidad (que reflejaría homocedasticidad) teniendo una pendiente positiva (ligera, pero apreciable).\nRespecto a la normalidad, a la vista del segundo gráfico (Q-Q Residuals), se observa una clara desviación de la linea recta punteada (que marcaría el ajuste perfecto a la distribución normal), sobre todo en la parte superior derecha. Para reforzar la impresión de este gráfico se puede acudir a otra visualización y a un contraste. Lo más apropiado es un histograma y el contraste de Shapiro-Wilk:\n\n\nhist(residuals(rls))\n\n\n\n\n\n\n\nshapiro.test(residuals(rls))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(rls)\nW = 0.91418, p-value = 2.516e-06\n\n\nEl histograma refleja una clara asimetría, que dista de la simetría de la distribución normal. Y el p-valor del contraste lleva claramente a rechazar la normalidad.\n\nEl último de los 4 gráficos de residuos (Residuals vs Leverage) señala las observaciones 117 y 62 como los valores con más influencia en los resultados de la regresión. Pero en este último gráfico no presentan un valor grande de la distancia de Cook, que sirve para medir esa influencia. De hecho, no aparecen en el gráfico ni las lineas discontinuas que marcan los límites para considerar un valor grande de la distancia y por lo tanto una posible observación influyente.\n\nLos valores de palanca o apalancamiento (Leverage) se pueden calcular usando la función hatvalues().\n\n\nEn resumen, es claro que hay dos observaciones que pueden influir en los resultados de la regresión, pero también es claro que los problemas observados (heterocedasticidad, falta de normalidad…) no provienen sólo de esos dos datos.\n\nPregunta\nRepita el análisis manteniendo Ozone como variable respuesta/dependiente, cambiando la variable explicativa Solar.R por Wind o Temp. ¿Aprecia algún cambio?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap1-LM.html#por-aquí",
    "href": "Cap1-LM.html#por-aquí",
    "title": "1  Modelos lineales",
    "section": "1.6 POR AQUÍ",
    "text": "1.6 POR AQUÍ\n\nAlternativas: Existen distintas funciones/paquetes más o menos sofisticados que realizan este análisis de residuos de distintas maneras. El lector interesado puede explorar: - El paquete ggfortify y su función autoplot. - El paquete performance y su función check_model. - …\n\n\n#Es necesario tener los paquetes instalados previamente\nlibrary(\"ggfortify\")\nautoplot(rls) +\n  theme_minimal()\nlibrary(performance) \nlibrary(see)\ncheck_model(rls)\n\nInterpretación e Inferencia\nSupongamos que fuese todo correcto. El siguiente paso sería el de averiguar la significación de los parámetros, interpretarlos y obtener alguna predicción de interés. Aplicamos la función summary() aplicada a nuestro objeto rls:\n\nsummary(rls)\n\n\nCall:\nlm(formula = Ozone ~ Solar.R, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.292 -21.361  -8.864  16.373 119.136 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 18.59873    6.74790   2.756 0.006856 ** \nSolar.R      0.12717    0.03278   3.880 0.000179 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.33 on 109 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.1213,    Adjusted R-squared:  0.1133 \nF-statistic: 15.05 on 1 and 109 DF,  p-value: 0.0001793\n\n\nCon la salida de summary() obtenemos, además de los coeficientes \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) (columna Estimate), mucha información para su interpretación. En la columna Pr(&gt;|t|) se pueden ver los p-valores de cada parámetro. El del parámetro asociado a la variable Solar.R es muy pequeño, pudiéndose concluir, en caso de que el modelo fuese válido, que la variable Solar.R influye significativamente en la variable Ozone. De la misma manera, el parámetro \\(\\beta_0\\) (Intercept, intersección con el origen) es significativo, lo que indica que el modelo no pasa por el origen. Los valores obtenidos provienen, como se ha visto en la parte teórica, de la estimación del estadístico \\(t\\) que se encuentra en la columna t value, que a su vez se basa en la mencionada estimación de cada parámetro (columna Estimate) y el error de estimación (columna Std. Error). A partir de estos dos valores y la distribución \\(t\\) correspondiente, se pueden obtener los intervalos de confianza “a mano” (paso a paso con R), pero se dispone de la función confint():\n\nconfint(rls)\n\n                 2.5 %     97.5 %\n(Intercept) 5.22460110 31.9728544\nSolar.R     0.06220373  0.1921268\n\n\nSi el modelo fuese válido se pasaría a la interpretación de estos parámetros… Primero, el signo de \\(\\hat{\\beta}_1\\), al ser positivo, indica que conforme aumenta Solar.R también aumenta Ozone. Esta conclusión es clara aunque el modelo no sea válido. Ahora bien, no sólo se obtiene numéricamente esa relación positiva, sino la magnitud de dicha relación. Así, el cambio de una unidad en Solar.R implicaría un cambio medio de aproximadamente 0.1272 unidades en Ozone, con un valor medio de 18.5987 unidades de Ozone en ausencia de radiación solar (¡Ojo! Con los datos disponibles, el mínimo valor de Solar.R es NA, según la tabla del resumen numérico, por lo que estamos hablando de una extrapolación que necesitaría del conocimiento de un experto en la materia para dilucidar su apropiada y/o oportuna interpretación).\nBondad de ajuste\nEn la salida anterior de la función summary() también se pueden observar resultados que ayudan a proporcionar la bondad de ajuste de la regresión simple planteada. El valor que resume la bondad de ajuste es el Multiple R-squared, aunque, como se ha mencionado en la parte teórica, para comparar entre modelos conviene hacerlo con el Adjusted R-squared que arroja un valor de\n\nrls.summ &lt;- summary(rls)  #guardamos el objeto generado con summary()\n#con names() listamos distintos componentes generados con summary() \nnames(rls.summ)           #también se puede consultar con ?summary.lm \n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\"  \"na.action\"    \n\nsummary(rls)$adj.r.squared\n\n[1] 0.1132809\n\n\nEste 11.3% se puede considerar “pobre”. La variable Solar.R explica “pobremente” el Ozone. No obstante, el p-valor global del modelo que se muestra en la última linea de la salida de summary() es significativo, indicando que la recta de regresión es significativa, esto es, que es mejor que proporcionar sólo la media de Ozone para cualquier valor de Solar.R que sería el modelo “básico”.\n\nPregunta\n¿Sabe decir porqué el p-valor de la recta de regresión coincide con el del parámetro asociado a Solar.R?\n\nTambién se puede observar el valor del Residual standard error y sus grados de libertad que se ha mencionado en la parte teórica.\nUn último apunte, en la salida también aparece el mensaje 42 observations deleted due to missingness que indica que no se han considerado aquellas observaciones en las que cualquier variable tiene un NA.\nPredicción\nA pesar de que el modelo no es idóneo, puede que la regresión lineal sirva para el propósito de explicar a grandes rasgos el fenómeno. Pasamos a ilustrar como se obtendrían con R predicciones a partir de la recta estimada:\n\npredict(rls, data.frame(Solar.R = c(10, 100, 300)),\n        interval = \"confidence\")\n\n       fit       lwr      upr\n1 19.87038  7.076164 32.66460\n2 31.31525 23.247138 39.38337\n3 56.74831 47.222078 66.27454\n\npredict(rls, data.frame(Solar.R = c(10, 100, 300)),\n        interval = \"prediction\")\n\n       fit        lwr       upr\n1 19.87038 -43.537910  83.27867\n2 31.31525 -31.310729  93.94124\n3 56.74831  -6.082164 119.57878\n\n\nCon el argumento interval = \"confidence\" obtenemos intervalos de confianza (para valores medios), con interval = \"prediction\" obtenemos intervalos de predicción (para valores individuales) para la predicción de Ozone dados valores de Solar.R. Concretamente para 3 valores: 10, 100 y 300, todos ellos en el rango de valores observados de la variable. La estimación media (fit) coincide en ambos casos, diferenciándose en la amplitud de los intervalos, mucho mayores para los intervalos de predicción, dado que una observación individual puede alejarse mucho más de la media, que una media de varias observaciones para el mismo valor de Solar.R.\n\n1.6.1 lm() múltiple\nModelización\nLa regresión lineal múltiple que se va a abordar aquí, ligeramente distinta a la del libro CDR, directamente en formato de R es:\n\nrlm &lt;- lm(Ozone ~ Solar.R + Wind + Temp + Month, data = airquality) \n## Equivalentemente\n#rlm &lt;- lm(Ozone ~ . - Day, data = airquality)\n\n\nPara no tener que escribir todas las variables, se puede hacer uso de .: lm(y ~ ., data) indica una regresión con y como variable respuesta, y el resto de variables de data como predictores lineales.\nEn el código de arriba además se ha usado el “truco” de quitar una variable con -.\n\nAnálisis de residuos\n\npar(mfrow = c(2, 2), #presenta los gráficos en formato 2x2\n    pty = \"s\",\n    mex = 0.66,\n    cex = 0.75)\nplot(rlm)\n\n\n\n\n\n\n\n\n\nPregunta\n¿Se ha mejorado respecto a la regresión simple?\n\nNótese que ahora, en el 4º gráfico (Residuals vs Leverage) sí que aparece la línea punteada del valor 0.5 de la distancia de Cook, que no llega a alcanzar ningundo de los datos.\nEstimación e interpretación\n\nsummary(rlm)\n\n\nCall:\nlm(formula = Ozone ~ Solar.R + Wind + Temp + Month, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.870 -13.968  -2.671   9.553  97.918 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -58.05384   22.97114  -2.527   0.0130 *  \nSolar.R       0.04960    0.02346   2.114   0.0368 *  \nWind         -3.31651    0.64579  -5.136 1.29e-06 ***\nTemp          1.87087    0.27363   6.837 5.34e-10 ***\nMonth        -2.99163    1.51592  -1.973   0.0510 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.9 on 106 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6199,    Adjusted R-squared:  0.6055 \nF-statistic: 43.21 on 4 and 106 DF,  p-value: &lt; 2.2e-16\n\n\nDe la salida de la función summary() se desprende que de las 4 variables consideradas, la más significativa es Temp, seguida de Wind (ambas con 3 asteriscos), y también es significativa Solar.R, pero a otro nivel (sólo un asterisco, por un valor inferior al 5% pero superior al 1%, cuando en la regresión simple tenía un p-valor más pequeño/significativo, 2 asteriscos). Por último, Month es significativa pero a un nivel del 10% de significación. También el término independiente es significativo.\nSobre la interpretación de los parámetros, se debe tener en cuenta las unidades en las que están recogidas cada una de las variables. Como se ha mencionado, cualquier cambio de escala influirá en dichas estimaciones, y generalmente en sus Std. Error (salvo traslaciones de las variables).\nAdemás, la interpretación de las estimaciones (magnitud) como cambios en la variable respuesta tiene ahora un cambio sustancial. El cambio en la variable respuesta de 1 unidad de una de las variables explicativas está condicionado a que el resto de variables explicativas no cambien, lo que en el contexto de la Economía se denomina ceteris paribus, y que puede que sea imposible de cumplirse. Por ejemplo, el parámetro asociado a Solar.R es, para este modelo de regresión lineal múltiple, 0.0496 menos de la mitad de magnitud que en el caso de regresión simple. Ese sería el cambio medio en las unidades de Ozone para cambios de 1 unidad en Solar.R siempre que Wind, Temp y Month tengan los mismos valores, o, dicho de otro modo, para otro día qué tuviese los mismos valores de Wind, Temp y Month, y cambiase 1 unidad Solar.R. Si es que es factible el cambio de Solar.R con los mismo valores del resto de variables.\nOtra lectura que se debe hacer sobre la interpretación de la magnitud del coeficiente es que, al ser independiente del resto de valores de las variables, es, a su vez, para cualquier combinación de dichos valores. Por ejemplo, para valores de Wind altos, medios o bajos, combinado con valores de Temp altos, medios o bajos, o para cualquier Month (entre 5 y 9, que es el rango observado).\n\nPregunta\n¿Cómo interpretaría el valor negativo del término independiente si se trata de la concentración de ozono en partes por billón?\n\nBondad de ajuste\nLa bondad del ajuste de este modelo, ha mejorado mucho comparada con la regresión simple anterior, pasando ahora a un Adjusted R-squared de 0.606.\nSi nuestro interés sólo fuese predecir valores de Ozone, este modelo podría servirnos.\nPredicción\n¿Cómo se pueden obtener predicciones con un modelo de regresión múltiple? Se deben proporcionar valores a cada una de las variables del modelo:\n\nnuevas.observ &lt;- data.frame(Solar.R = c(110, 110),\n                            Wind    = c(8, 20), \n                            Temp    = c(72, 85), \n                            Month   = c(6, 6))\npredict(rlm, newdata = nuevas.observ,\n        interval = \"confidence\" )\n\n       fit       lwr      upr\n1 37.62288 30.382618 44.86315\n2 22.14613  4.912182 39.38008\n\n\n\n1.6.1.1 Predictores cualitativos\nEl uso de predictores cualitativos debe tratarse de forma diferencial en R. Deben definirse como factor, (factor()). Así, al estimar R los parámetros del modelo donde se incluya dicha variable, generará automáticamente variables ficticias, variables dummys. La función contrasts() permite conocer la codificación que por defecto usa R para dichas variables ficticias (aunque se puede modificar). Para ilustrarlo, redefinimos la variable Month en el conjunto de datos airquality\n\nairquality$Month &lt;- factor (airquality$Month) \ncontrasts(airquality$Month)\n\n  6 7 8 9\n5 0 0 0 0\n6 1 0 0 0\n7 0 1 0 0\n8 0 0 1 0\n9 0 0 0 1\n\n\nLa salida indica que el valor 5 de Month se toma como valor de referencia (en su fila aparecen todo ceros), y se generan 4 variables dummy, una para cada uno de los meses restantes (del 6 al 9).\n\nEn R las variables definidas como factor toma como referencia la primera categoría al ordenar los valores de la variable, bien alfabéticamente (a, b, c…) o bien numéricamente de menor a mayor, si no se ha especificado otro orden.\n\nAjustamos de nuevo el modelo de regresión múltiple.\n\nrlm.cualit &lt;- lm(Ozone ~ . - Day, \n                 data = airquality)\nsummary(rlm.cualit)\n\n\nCall:\nlm(formula = Ozone ~ . - Day, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.344 -13.495  -3.165  10.399  92.689 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -74.23481   26.10184  -2.844  0.00537 ** \nSolar.R       0.05222    0.02367   2.206  0.02957 *  \nWind         -3.10872    0.66009  -4.710 7.78e-06 ***\nTemp          1.87511    0.34073   5.503 2.74e-07 ***\nMonth6      -14.75895    9.12269  -1.618  0.10876    \nMonth7       -8.74861    7.82906  -1.117  0.26640    \nMonth8       -4.19654    8.14693  -0.515  0.60758    \nMonth9      -15.96728    6.65561  -2.399  0.01823 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.72 on 103 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6369,    Adjusted R-squared:  0.6122 \nF-statistic: 25.81 on 7 and 103 DF,  p-value: &lt; 2.2e-16\n\n\nSe puede observar que se muestran parámetros para las 4 variables dummy de los 5 valores de Month. Como se ha indicado, el primer valor de la variable (el 5) lo toma como referencia (se podría cambiar si se quiere otro mes de referencia) y genera 4 variables dicotómicas que reflejan el cambio de la categoría de referencia a cada una de ellas. Matemáticamente el modelo que se estima es: \\[\\begin{align*}\nOzone = &\\beta_0 + \\beta_1 Solar.R + \\beta_2 Wind + \\beta_3 Temp + \\\\\n&\\beta_6 Month6 +  \\beta_7 Month7 +  \\beta_8 Month8 +  \\beta_9 Month9 + \\epsilon  \n\\end{align*}\\] Así, se entiende ahora mejor la visualización de contrasts() obtenida anteriormente. Cuando Month6, Month7… toman todos el valor 0 la estimación que se obtiene es del mes de mayo. Si la variable dummy Month6 toma el valor 1, y las otras 3 toman el valor 0, se obtiene la diferencia media de Ozone respecto al mes de mayo, ceteris paribus. Por lo tanto, que el parámetro asociado a Month6 sea negativo implica que, para cualquier combinación fija de valores del resto de variables, en junio disminuye la media de Ozone respecto a mayo (aunque justo este parámetro/cambio no es significativo!), etc.\n\nPregunta\n¿Como se interpreta ahora el valor del Intercept?\n¿Y cómo se explica el valor negativo de Month6 a la vista del resumen gráfico que se obtuvo con pairs()?\n\n\n\n\n1.6.2 Caso práctico: Boston\nEl segundo de los casos prácticos de modelización lineal que se va a tratar en profundidad utiliza los datos Boston, recogidos en varios paquetes, por ejemplo, en ISLR2 asociado con el libro: https://www.statlearning.com. Los datos recogen valores relacionados con viviendas para 506 distritos censales de Boston. El estudio de la modelización lineal se puede encontrar en: https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch3-linreg-lab.html y el Rscript asociado en: https://hastie.su.domains/ISLR2/Labs/R_Labs/Ch3-linreg-lab.R. De nuevo, aquí se presentarán ejemplos ligeramente distintos y con algo más de profundidad.\n\n1.6.2.1 Exploración de los datos\n\nlibrary(ISLR2)\n#?Boston #Para obtener más información sobre las variables, unidades, etc. \nhead(Boston)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio lstat medv\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3  4.98 24.0\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8  9.14 21.6\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8  4.03 34.7\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7  2.94 33.4\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7  5.33 36.2\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7  5.21 28.7\n\n\nAhora no parece haber NAs. En el resumen numérico veremos que no hay. No obstante, se puede comprobar con:\n\nsum(is.na(Boston))\n\n[1] 0\n\n\nEstructura y resúmenes\n\nstr(Boston)\n\n'data.frame':   506 obs. of  13 variables:\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...\n\nsummary(Boston)\n\n      crim                zn             indus            chas        \n Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000  \n 1st Qu.: 0.08205   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000  \n Median : 0.25651   Median :  0.00   Median : 9.69   Median :0.00000  \n Mean   : 3.61352   Mean   : 11.36   Mean   :11.14   Mean   :0.06917  \n 3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000  \n Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000  \n      nox               rm             age              dis        \n Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130  \n 1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02   1st Qu.: 2.100  \n Median :0.5380   Median :6.208   Median : 77.50   Median : 3.207  \n Mean   :0.5547   Mean   :6.285   Mean   : 68.57   Mean   : 3.795  \n 3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08   3rd Qu.: 5.188  \n Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127  \n      rad              tax           ptratio          lstat      \n Min.   : 1.000   Min.   :187.0   Min.   :12.60   Min.   : 1.73  \n 1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40   1st Qu.: 6.95  \n Median : 5.000   Median :330.0   Median :19.05   Median :11.36  \n Mean   : 9.549   Mean   :408.2   Mean   :18.46   Mean   :12.65  \n 3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20   3rd Qu.:16.95  \n Max.   :24.000   Max.   :711.0   Max.   :22.00   Max.   :37.97  \n      medv      \n Min.   : 5.00  \n 1st Qu.:17.02  \n Median :21.20  \n Mean   :22.53  \n 3rd Qu.:25.00  \n Max.   :50.00  \n\n\nA la vista de la estructura y el resumen numérico, se pueden considerar continuas casi todas las variables (internamente están definidas en R como double o int, para manejar números reales con formato de coma flotante de doble precisión que requieren más espacio en memoria o enteros, respectivamente). Llama la atención el resumen de las variables zn, chas y rad. La variable zn está definida internamente como double porque contiene algunos valores con decimales (y mirando su definición es una proporción). Ahora bien, su mediana es 0, por lo que al menos la mitad de los 506 valores son 0, de hecho el 0 aparece 372 veces. Esto, sin duda tendrá un impacto al considerarlo en los modelos. Por su parte las variables chas y rad tienen formato int, pero acudiendo a su definición, chas es dicotómica (lo que también da sentido que su mediana sea 0 y su media se 0.0692), mientras que rad es un índice, que toma valores entre 1 y 24. Dejaremos zn y rad con valores numéricos, pero, para su apropiado manejo en los modelos, es oportuno convertir chas en variable factor de R:\n\nBoston$chas &lt;- factor(Boston$chas)\ncontrasts(Boston$chas)\n\n  1\n0 0\n1 1\n\n\nComo se puede ver con la función contrasts() en los casos de variable dicotómica sólo se genera una variable dummy, quedando por defecto el valor 0 como valor de referencia. Rizando el rizo, vamos a cambiar la definición del valor de referencia para luego observar su impacto en la modelización.\n\nBoston$chas &lt;- factor(Boston$chas, \n                      levels = c(1, 0))\ncontrasts(Boston$chas)\n\n  0\n1 0\n0 1\n\n\n\npairs(Boston, upper.panel = NULL,\n      lower.panel = panel.smooth)\n\n\n\n\n\n\n\n\nA la vista del resumen gráfico, diagramas de dispersión de pares de variables, casi todas las relaciones entre pares de variables parecen no lineales. Como se puede apreciar, la dicotomía de la variable chas genera diagramas de dispersión muy distintos al resto. Y las variables rad y tax llaman la atención por presentar dos grupos de valores alejados, especialmente rad. También, la variable nox presenta dos grupos de valores pero con menor separación entre grupos.\n\n\n1.6.2.2 lm() múltiple\nModelización y estimación\nLa variable de interés a modelizar es medv (“median value”: valor mediano de las casas ocupadas por sus propietarios, en $1000s). En el libro ISLR2 utilizan las otras 12 variables explicativas como predictores. Aquí consideraremos sólo unas cuantas (ojo con lo que recogen cada una de las variables… ¿age?):\n\nrlm.Boston &lt;- lm(medv ~ lstat + rm + age + tax + chas, #chas definida como factor\n                 data = Boston)\nsummary(rlm.Boston)\n\n\nCall:\nlm(formula = medv ~ lstat + rm + age + tax + chas, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.358  -3.345  -1.124   1.930  30.073 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.323355   3.270507   1.322    0.187    \nlstat       -0.588092   0.055117 -10.670  &lt; 2e-16 ***\nrm           4.954530   0.440829  11.239  &lt; 2e-16 ***\nage          0.014624   0.011379   1.285    0.199    \ntax         -0.007009   0.001756  -3.990 7.58e-05 ***\nchas0       -3.898106   0.955381  -4.080 5.24e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.372 on 500 degrees of freedom\nMultiple R-squared:  0.6622,    Adjusted R-squared:  0.6588 \nF-statistic:   196 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\n\nSe han unificado la modelización y estimación, como se hace generalmente. Posteriormente se realizará el análisis de residuos que permitirá dar validez al modelo considerado.\nEn este caso, los parámetros asociados a las variables salen significativos, excepto uno. Este hecho, conduce a eliminar dicha variable del modelo, para obtener uno con sólo variables influyentes sobre medv.\nLa consecuencia de que algunos parámetros no sean significativos también se puede apreciar en sus intervalos de confianza… Los no significativos incluyen el 0.\n\nconfint(rlm.Boston)\n\n                   2.5 %       97.5 %\n(Intercept) -2.102274869 10.748985289\nlstat       -0.696380539 -0.479802930\nrm           4.088424014  5.820635797\nage         -0.007732991  0.036981489\ntax         -0.010459310 -0.003557925\nchas0       -5.775162051 -2.021050701\n\n\n\nPregunta\n¿Interpretación de los coeficientes?… Recuerde: ceteris paribus.\n\n\n\n1.6.2.3 Quitando predictores\nComo ilustración del proceso iterativo (manual) de modelización, pasamos a estimar un nuevo modelo en el que eliminamos la variable no significativa que se obtuvo en el modelo anterior rlm.Boston, esto es quitando age:\n\nrlm.BostonModif &lt;- lm(medv ~ lstat + rm + tax + chas, data = Boston)\n#Alternativamente, usando la función `update()`.\n#rlm.BostonModif &lt;- update(rlm.Boston, ~ . - age)\nsummary(rlm.BostonModif)\n\n\nCall:\nlm(formula = medv ~ lstat + rm + tax + chas, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.484  -3.400  -1.158   1.909  30.849 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.154797   3.270001   1.271 0.204468    \nlstat       -0.554277   0.048462 -11.437  &lt; 2e-16 ***\nrm           5.060589   0.433317  11.679  &lt; 2e-16 ***\ntax         -0.006412   0.001695  -3.783 0.000174 ***\nchas0       -4.076908   0.945811  -4.310 1.96e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.376 on 501 degrees of freedom\nMultiple R-squared:  0.6611,    Adjusted R-squared:  0.6584 \nF-statistic: 244.3 on 4 and 501 DF,  p-value: &lt; 2.2e-16\n\n\nAhora el modelo sale con todos los parámetros significativos, con un Adjusted R-squared ligeramente peor. No obstante el \\(R^2\\) ajustado es razonablemente bueno si el objetivo es predecir valores de Ozone.\n\nPregunta\n¿Cómo quitar varios predictores simultáneamente?\n¿Cómo hacer este proceso automáticamente? Véase el capítulo @(sec-Seleccion)\n\n\nPregunta\nObtenga este último modelo con la variable chas original (sin convertirla en factor) ¿Qué diferencias observa en la estimación del parámetro asociado a dicha variable?\n\nAnálisis de residuos\nValoremos la adecuación del último modelo examinando sus residuos.\n\npar(mfrow = c(2, 2), #presenta los gráficos en formato 2x2\n    pty = \"s\",\n    mex = 0.66,\n    cex = 0.75)\nplot(rlm.BostonModif)\n\n\n\n\n\n\n\n\n\nLos gráficos 1 y 3 (Residuals vs Fitted y Scale-Location) reflejan heterocedasticidad y no linealidad.\nEn el segundo gráfico (Q-Q Residuals), se observa una clara desviación de la normalidad (marcada por la linea de puntos). Veamos el histograma:\n\n\nhist(residuals(rlm.BostonModif))\n\n\n\n\n\n\n\n\nEl histograma refleja asimetría por lo que claramente rechazamos la normalidad.\n\nPor último en los 4 gráficos de residuos quedan señaladas las observaciones 369 y 373, junto con la 372 que aparece en 3 de los gráficos. Habría que considerar el modelo sin ellas para comprobar su influencia, pero, de nuevo, parece claro que los problemas comentados (heterocedasticidad, falta de normalidad…) no provienen sólo de esas observaciones.\n\n\n\n1.6.2.4 Extensiones de la regresión múltiple\nEl modelo lineal de regresión lineal múltiple permite considerar “constructos” de variables, es decir, considerar “productos” de dos variables, términos no lineales, etc. El tratamiento de esos casos con R es similar a lo visto hasta ahora, con el inconveniente de enfrentarse a su interpretación. Así, entre los “constructos” más habituales se encuentran:\n\nTérminos de interacción: La sintaxis lstat:age le dice a R que incluya un término de interacción entre lstat y age. La sintaxis lstat * age incluye como predictores simultáneamente lstat, age y la interacción lstat:age, es una abreviatura de lstat + age + lstat:age.\n\n\nsummary(lm(medv ~ lstat * age, data = Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\n\nPregunta\n¿Cómo se interpreta que la interacción salga significativa?\n\n\nTransformaciones polinómicas de los predictores: La sintaxis I(lstat^2) introduce en el modelo el predictor cuadrático de lstat.\n\n\nrlm.Boston2 &lt;- lm(medv ~ lstat + I(lstat^2), data = Boston)\nsummary(rlm.Boston2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nCargando paquete requerido: carData\n\nvif(rlm.Boston2)\n\n     lstat I(lstat^2) \n  12.93657   12.93657 \n\n\nEl p-valor cercano a cero asociado con el término cuadrático sugiere que, su inclusión, conduce a un modelo mejorado. Pero, obviamente, introduce un problema claro de colinealidad, que se puede obviar si el interés en el modelo es por su valor predictivo en lugar de explicativo.\nObviamente se pueden incluir términos cúbicos, etc.: I(variable^3)…, varios a la vez, etc. Pero para un ajuste polinómico, conviene acudir a la función poly(), que, por defecto, ortogonaliza los predictores.\n\nrlm.Boston6 &lt;- lm(medv ~ poly(lstat, 6), data = Boston)\nsummary(rlm.Boston6)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 6), data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.7317  -3.1571  -0.6941   2.0756  26.8994 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2317  97.252  &lt; 2e-16 ***\npoly(lstat, 6)1 -152.4595     5.2119 -29.252  &lt; 2e-16 ***\npoly(lstat, 6)2   64.2272     5.2119  12.323  &lt; 2e-16 ***\npoly(lstat, 6)3  -27.0511     5.2119  -5.190 3.06e-07 ***\npoly(lstat, 6)4   25.4517     5.2119   4.883 1.41e-06 ***\npoly(lstat, 6)5  -19.2524     5.2119  -3.694 0.000245 ***\npoly(lstat, 6)6    6.5088     5.2119   1.249 0.212313    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.212 on 499 degrees of freedom\nMultiple R-squared:  0.6827,    Adjusted R-squared:  0.6789 \nF-statistic: 178.9 on 6 and 499 DF,  p-value: &lt; 2.2e-16\n\n\nLa interpretación de esta salida sugiere que el modelo polinómico de orden 5 conduce al mejor ajuste del modelo con un polinomio de la variable lstat.\n\nTransformaciones logarítmicas: también se pueden aplicar otro tipo de transformaciones sobre los predictores, siempre que sean apropiadas y oportunas.\n\n\nsummary(lm(medv ~ log(rm), data = Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n1.6.2.5 Comparación de modelos mediante anova()\nCon la función anova() se pueden comparar modelos “jerárquicos”. En los casos expuestos anteriormente, se puede cuantificar hasta qué punto el ajuste cuadrático es superior al ajuste lineal, o que el ajuste de orden 6 no es superior al de orden 5.\n\nrlm.Boston1 &lt;- lm(medv ~ lstat, data = Boston)\nanova(rlm.Boston1, rlm.Boston2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nrlm.Boston5 &lt;- lm(medv ~ poly(lstat, 5), data = Boston)\nanova(rlm.Boston5, rlm.Boston6)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ poly(lstat, 5)\nModel 2: medv ~ poly(lstat, 6)\n  Res.Df   RSS Df Sum of Sq      F Pr(&gt;F)\n1    500 13597                           \n2    499 13555  1    42.364 1.5596 0.2123\n\n\nEn la primera tabla ANOVA se puede ver, en la etiquetada como línea 2, el estadístico F, que recoge el ratio de variabilidad explicada por el Model 2 frente al Model 1. Su elevado valor, junto con un p-valor prácticamente nulo, llevan a considerar el modelo cuadrático muy superior al simple. Al lector observador no le debe sorprender este resultado a la vista del diagrama de dispersión entre medv y lstat, que presenta una clara no linealidad.\nPor su parte, la segunda tabla ANOVA muestra un resultado bien distinto, con un discreto valor del estadístico F que conduce a rechazar que el polinomio de orden 6 explique mejor la respuesta que el de orden 5. Además se puede comprobar que este p-valor coincide con el p-valor asociado al término de orden 6 estimado por la regresión.\n\nPregunta ¿Qué conclusiones obtiene al comparar los modelos rlm.Boston y rlm.BostonModif?\n\n\n\n1.6.2.6 Colinealidad: vif()\nEl cálculo del factor de inflación de la varianza (VIF) que permite detectar la multicolinealidad (véase @ref(sec-multicolinealidad)) de cada parámetro se puede obtener con la función vif(), del paquete car:\n\nlibrary(car)\nvif(rlm.Boston)\n\n   lstat       rm      age      tax     chas \n2.710812 1.678750 1.795414 1.533240 1.030405 \n\n\nPara estos datos, la mayoría de los VIF son bajos o moderados.\n\nRegla de decisión práctica: No preocuparse de la multicolinealidad si vif &lt; 5, incluso vif &lt; 10.\n\n\nPregunta\n¿Cómo salen los VIF de la regresión múltiple con los datos airquality?\n\nVeamos un ejemplo claro de colinealidad:\n\nvif(rlm.Boston2)\n\n     lstat I(lstat^2) \n  12.93657   12.93657 \n\n\n\nPregunta\n¿Qué interpretación tiene que los VIF sean iguales?\n\nVamos a generar un caso con alta colinealidad:\n\nset.seed(pi)\nBoston$sum &lt;- Boston$lstat/2 + Boston$rm + rnorm(length(Boston$lstat))\nrlm.BostonColine &lt;- lm(medv ~ lstat + rm + tax + chas + sum, \n                       data = Boston)\nvif(rlm.BostonColine)\n\n    lstat        rm       tax      chas       sum \n13.189150  1.988231  1.430875  1.008564 10.209699 \n\n\n\nPregunta\n¿Qué interpretación tienen ahora los VIF?\n\n\n\nBibliografía\n\n\n\n\nCasero-Alonso, Víctor, y María Durbán. 2024. «Modelización lineal». En Fundamentos de Ciencia de Datos con R. McGraw Hill. https://cdr-book.github.io/cap-lm.html.\n\n\nFaraway, Julian J. 2004. Linear Models with R. Chapman &amp; Hall/CRC.\n\n\nFernández-Avilés, Gema, y José-María Montero. 2024. Fundamentos de Ciencia de Datos con R. McGraw Hill. https://cdr-book.github.io/index.html.\n\n\nPeña, Daniel. 2002. Regresión y diseño de experimentos. Alianza Editorial.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Modelos lineales</span>"
    ]
  },
  {
    "objectID": "Cap2-DoE.html",
    "href": "Cap2-DoE.html",
    "title": "2  Diseño de experimentos",
    "section": "",
    "text": "En preparación",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Diseño de experimentos</span>"
    ]
  },
  {
    "objectID": "Cap3-GLM.html",
    "href": "Cap3-GLM.html",
    "title": "3  Modelos lineales generalizados",
    "section": "",
    "text": "En preparación",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelos lineales generalizados</span>"
    ]
  },
  {
    "objectID": "Cap4-Superv.html",
    "href": "Cap4-Superv.html",
    "title": "4  Análisis de supervivencia o fiabilidad",
    "section": "",
    "text": "En preparación",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Análisis de supervivencia o fiabilidad</span>"
    ]
  },
  {
    "objectID": "Cap5-Seleccion.html",
    "href": "Cap5-Seleccion.html",
    "title": "5  Selección de variables",
    "section": "",
    "text": "En preparación",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selección de variables</span>"
    ]
  }
]